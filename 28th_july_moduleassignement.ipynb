{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d560b5e7-ded5-4148-9626-702d4271f06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.What is the difference between static and dynamic variables in Python?\\n\\nStatic variables are class variables that are shared among all instances of a class. They are defined within the class but outside any instance methods.\\nDynamic variables are instance variables that are unique to each instance of a class. They are usually defined within methods of a class and are prefixed with self'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"1.What is the difference between static and dynamic variables in Python?\n",
    "\n",
    "Static variables are class variables that are shared among all instances of a class. They are defined within the class but outside any instance methods.\n",
    "Dynamic variables are instance variables that are unique to each instance of a class. They are usually defined within methods of a class and are prefixed with self\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d842d068-c9a1-4525-96b8-65e63010a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "('c', 3)\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2.Explain the purpose of pop(), popitem(), clear() in a dictionary with suitable examples.\n",
    "\n",
    "pop(): Removes a specified key and returns the corresponding value.\n",
    "popitem(): Removes and returns an arbitrary (key, value) pair as a tuple.\n",
    "clear(): Removes all items from the dictionary.\n",
    "Example:\"\"\"\n",
    "\n",
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "value = my_dict.pop('b')\n",
    "print(value)# value = 2, my_dict = {'a': 1, 'c': 3'}\n",
    "\n",
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "item = my_dict.popitem() \n",
    "print(item)# item = ('c', 3), my_dict = {'a': 1, 'b': 2}\n",
    "\n",
    "\n",
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "my_dict.clear()\n",
    "print(my_dict)# my_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cc3d78-3b02-41ba-a000-71f74f419b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you mean by FrozenSet? Explain it with suitable examples.\\n\\nfrozenset is an immutable version of a set. Unlike a regular set, the elements of a frozenset cannot be modified once assigned. Example:\\n\\nmy_frozenset = frozenset([1, 2, 3, 4])\\n# my_frozenset.add(5) would raise an AttributeError because frozenset is immutable'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What do you mean by FrozenSet? Explain it with suitable examples.\n",
    "\n",
    "frozenset is an immutable version of a set. Unlike a regular set, the elements of a frozenset cannot be modified once assigned. Example:\n",
    "\n",
    "my_frozenset = frozenset([1, 2, 3, 4])\n",
    "# my_frozenset.add(5) would raise an AttributeError because frozenset is immutable\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec5e9e2-f28a-47c7-868b-34bb26ba90af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Differentiate between mutable and immutable data types in Python and give examples of mutable and immutable data types.\\n\\nMutable data types can be changed after they are created. Examples include lists, dictionaries, and sets.\\nImmutable data types cannot be changed after they are created. Examples include strings, tuples, and frozensets.\\n\\nmutable_list = [1, 2, 3]\\nmutable_list.append(4)  # mutable_list = [1, 2, 3, 4]\\n\\nimmutable_tuple = (1, 2, 3)\\n# immutable_tuple[0] = 4 would raise a TypeError\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Differentiate between mutable and immutable data types in Python and give examples of mutable and immutable data types.\n",
    "\n",
    "Mutable data types can be changed after they are created. Examples include lists, dictionaries, and sets.\n",
    "Immutable data types cannot be changed after they are created. Examples include strings, tuples, and frozensets.\n",
    "\n",
    "mutable_list = [1, 2, 3]\n",
    "mutable_list.append(4)  # mutable_list = [1, 2, 3, 4]\n",
    "\n",
    "immutable_tuple = (1, 2, 3)\n",
    "# immutable_tuple[0] = 4 would raise a TypeError\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446c0848-3633-4f4c-a0fe-ed9794e04c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is __init__? Explain with an example.\\n\\n__init__ is a special method in Python classes, called a constructor. It is automatically called when an instance of a class is created and is used to initialize the instance variables.\\nclass MyClass:\\n    def __init__(self, value):\\n        self.value = value\\n\\nobj = MyClass(10)\\nprint(obj.value)  # Output: 10\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is __init__? Explain with an example.\n",
    "\n",
    "__init__ is a special method in Python classes, called a constructor. It is automatically called when an instance of a class is created and is used to initialize the instance variables.\n",
    "class MyClass:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "obj = MyClass(10)\n",
    "print(obj.value)  # Output: 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f07feb4-3dd2-41ca-981a-9f4cc7fb3da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is docstring in Python? Explain with an example.\\n\\nA docstring is a string literal used to document a module, class, function, or method in Python. It is defined within triple quotes and provides a convenient way of associating documentation with Python code.\\ndef my_function():\\n    \"This is a docstring explaining the function.\"\\n    pass\\n\\nprint(my_function.__doc__)  # Output: This is a docstring explaining the function.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is docstring in Python? Explain with an example.\n",
    "\n",
    "A docstring is a string literal used to document a module, class, function, or method in Python. It is defined within triple quotes and provides a convenient way of associating documentation with Python code.\n",
    "def my_function():\n",
    "    \"This is a docstring explaining the function.\"\n",
    "    pass\n",
    "\n",
    "print(my_function.__doc__)  # Output: This is a docstring explaining the function.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2701f072-b564-4284-ab31-2ede1a51ecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are unit tests in Python?\\n\\nUnit tests are tests that validate the functionality of specific sections of code, usually individual functions or methods. They help ensure that the code works as expected. Python's unittest module is commonly used to create and run unit tests.\\nimport unittest\\n\\ndef add(a, b):\\n    return a + b\\n\\nclass TestAddFunction(unittest.TestCase):\\n    def test_add(self):\\n        self.assertEqual(add(1, 2), 3)\\n        self.assertEqual(add(-1, 1), 0)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are unit tests in Python?\n",
    "\n",
    "Unit tests are tests that validate the functionality of specific sections of code, usually individual functions or methods. They help ensure that the code works as expected. Python's unittest module is commonly used to create and run unit tests.\n",
    "import unittest\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class TestAddFunction(unittest.TestCase):\n",
    "    def test_add(self):\n",
    "        self.assertEqual(add(1, 2), 3)\n",
    "        self.assertEqual(add(-1, 1), 0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1339de37-2921-46c3-8820-4e33a30d572a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is break, continue and pass in Python?\\n\\nbreak: Terminates the nearest enclosing loop.\\ncontinue: Skips the rest of the code inside the current loop iteration and moves to the next iteration.\\npass: Does nothing and acts as a placeholder\\n\\nfor i in range(5):\\n    if i == 2:\\n        break  # Loop ends when i is 2\\n    print(i)\\n\\nfor i in range(5):\\n    if i == 2:\\n        continue  # Skips the rest of the code when i is 2\\n    print(i)\\n\\nfor i in range(5):\\n    if i == 2:\\n        pass  # Does nothing\\n    print(i)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is break, continue and pass in Python?\n",
    "\n",
    "break: Terminates the nearest enclosing loop.\n",
    "continue: Skips the rest of the code inside the current loop iteration and moves to the next iteration.\n",
    "pass: Does nothing and acts as a placeholder\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 2:\n",
    "        break  # Loop ends when i is 2\n",
    "    print(i)\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 2:\n",
    "        continue  # Skips the rest of the code when i is 2\n",
    "    print(i)\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 2:\n",
    "        pass  # Does nothing\n",
    "    print(i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d437a3-afc0-413f-a1d7-0178c661793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the use of self in Python?\\n\\nself is a reference to the current instance of a class. It is used to access variables and methods associated with the instance. It is the first parameter of instance methods in Python classes.\\nclass MyClass:\\n    def __init__(self, value):\\n        self.value = value\\n\\n    def display_value(self):\\n        print(self.value)\\n\\nobj = MyClass(10)\\nobj.display_value()  # Output: 10\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is the use of self in Python?\n",
    "\n",
    "self is a reference to the current instance of a class. It is used to access variables and methods associated with the instance. It is the first parameter of instance methods in Python classes.\n",
    "class MyClass:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def display_value(self):\n",
    "        print(self.value)\n",
    "\n",
    "obj = MyClass(10)\n",
    "obj.display_value()  # Output: 10\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af3c87c-796b-4e69-a6b6-203deed8a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are global, protected and private attributes in Python?\\n\\nGlobal attributes are accessible from anywhere in the code.\\nProtected attributes are indicated by a single underscore prefix (e.g., _attr) and are intended to be accessed only within the class and its subclasses.\\nPrivate attributes are indicated by a double underscore prefix (e.g., __attr) and are not accessible directly outside the class.\\n\\nclass MyClass:\\n    def __init__(self):\\n        self.public_attr = \"I am public\"\\n        self._protected_attr = \"I am protected\"\\n        self.__private_attr = \"I am private\"\\n\\nobj = MyClass()\\nprint(obj.public_attr)       # Accessible\\nprint(obj._protected_attr)   # Accessible but discouraged\\n# print(obj.__private_attr)  # Not accessible directly\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are global, protected and private attributes in Python?\n",
    "\n",
    "Global attributes are accessible from anywhere in the code.\n",
    "Protected attributes are indicated by a single underscore prefix (e.g., _attr) and are intended to be accessed only within the class and its subclasses.\n",
    "Private attributes are indicated by a double underscore prefix (e.g., __attr) and are not accessible directly outside the class.\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.public_attr = \"I am public\"\n",
    "        self._protected_attr = \"I am protected\"\n",
    "        self.__private_attr = \"I am private\"\n",
    "\n",
    "obj = MyClass()\n",
    "print(obj.public_attr)       # Accessible\n",
    "print(obj._protected_attr)   # Accessible but discouraged\n",
    "# print(obj.__private_attr)  # Not accessible directly\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f180fa5-1fe2-473e-a266-bed61cf84116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are modules and packages in Python?\\n\\nModules are files containing Python code that define functions, classes, and variables. They can be imported and used in other Python scripts.\\nPackages are collections of modules organized in directories that provide a hierarchical structure. They contain an __init__.py file to distinguish them as packages.\\n\\n# Module: my_module.py\\ndef my_function():\\n    print(\"Hello from my module\")\\n\\n# Package structure:\\n# my_package/\\n# ├── __init__.py\\n# ├── module1.py\\n# └── module2.py\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are modules and packages in Python?\n",
    "\n",
    "Modules are files containing Python code that define functions, classes, and variables. They can be imported and used in other Python scripts.\n",
    "Packages are collections of modules organized in directories that provide a hierarchical structure. They contain an __init__.py file to distinguish them as packages.\n",
    "\n",
    "# Module: my_module.py\n",
    "def my_function():\n",
    "    print(\"Hello from my module\")\n",
    "\n",
    "# Package structure:\n",
    "# my_package/\n",
    "# ├── __init__.py\n",
    "# ├── module1.py\n",
    "# └── module2.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab08ffa-cd82-496a-8131-04633572b459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are lists and tuples? What is the key difference between the two?\\n\\nLists and tuples are both sequence data types in Python.\\nThe key difference is that lists are mutable (can be modified), while tuples are immutable (cannot be modified).\\n\\nmy_list = [1, 2, 3]\\nmy_list.append(4)  # my_list = [1, 2, 3, 4]\\n\\nmy_tuple = (1, 2, 3)\\n# my_tuple.append(4) would raise an AttributeError\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are lists and tuples? What is the key difference between the two?\n",
    "\n",
    "Lists and tuples are both sequence data types in Python.\n",
    "The key difference is that lists are mutable (can be modified), while tuples are immutable (cannot be modified).\n",
    "\n",
    "my_list = [1, 2, 3]\n",
    "my_list.append(4)  # my_list = [1, 2, 3, 4]\n",
    "\n",
    "my_tuple = (1, 2, 3)\n",
    "# my_tuple.append(4) would raise an AttributeError\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158e6652-8419-40e8-a634-b7f513d1ae3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is an Interpreted language & dynamically typed language? Write 5 differences between them.\\n\\nInterpreted language: Executes code line by line using an interpreter. Example: Python.\\nDynamically typed language: Variable types are determined at runtime. Example: Python.\\nInterpreted languages execute code line by line, whereas compiled languages convert code into machine language before execution.\\nErrors in interpreted languages can be found at runtime, while errors in compiled languages are found at compile time.\\nInterpreted languages are generally slower than compiled languages due to the line-by-line execution.\\nDynamically typed languages do not require explicit type declarations, whereas statically typed languages do.\\nCode written in dynamically typed languages is usually more flexible and concise compared to statically typed languages'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is an Interpreted language & dynamically typed language? Write 5 differences between them.\n",
    "\n",
    "Interpreted language: Executes code line by line using an interpreter. Example: Python.\n",
    "Dynamically typed language: Variable types are determined at runtime. Example: Python.\n",
    "Interpreted languages execute code line by line, whereas compiled languages convert code into machine language before execution.\n",
    "Errors in interpreted languages can be found at runtime, while errors in compiled languages are found at compile time.\n",
    "Interpreted languages are generally slower than compiled languages due to the line-by-line execution.\n",
    "Dynamically typed languages do not require explicit type declarations, whereas statically typed languages do.\n",
    "Code written in dynamically typed languages is usually more flexible and concise compared to statically typed languages\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d002ed51-41ba-4335-85dd-2b2661ad93cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are Dict and List comprehensions?\\nDict comprehensions: A concise way to create dictionaries\\nsquares = {x: x*x for x in range(6)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\\nList comprehensions: A concise way to create lists\\nsquares = [x*x for x in range(6)]  # [0, 1, 4, 9, 16, 25]\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are Dict and List comprehensions?\n",
    "Dict comprehensions: A concise way to create dictionaries\n",
    "squares = {x: x*x for x in range(6)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n",
    "List comprehensions: A concise way to create lists\n",
    "squares = [x*x for x in range(6)]  # [0, 1, 4, 9, 16, 25]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580d35ec-2dd2-46ac-bc73-28e5fc1f28b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are decorators in Python? Explain it with an example. Write down its use cases.\\n\\nDecorators are functions that modify the functionality of another function. They are used to add behavior to functions or methods.\\ndef my_decorator(func):\\n    def wrapper():\\n        print(\"Something is happening before the function is called.\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are decorators in Python? Explain it with an example. Write down its use cases.\n",
    "\n",
    "Decorators are functions that modify the functionality of another function. They are used to add behavior to functions or methods.\n",
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b61d21-89c2-43b9-a567-1c2ce03a2422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow is memory managed in Python?\\n\\nPython uses a private heap to manage memory. The Python memory manager handles the allocation of heap space for Python objects.\\nIt includes an inbuilt garbage collector, which reclaims memory by tracking and collecting unused objects (those with reference count zero).\\n\\nimport gc\\ngc.collect()  # Manually trigger garbage collection\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How is memory managed in Python?\n",
    "\n",
    "Python uses a private heap to manage memory. The Python memory manager handles the allocation of heap space for Python objects.\n",
    "It includes an inbuilt garbage collector, which reclaims memory by tracking and collecting unused objects (those with reference count zero).\n",
    "\n",
    "import gc\n",
    "gc.collect()  # Manually trigger garbage collection\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1073b4a-6c07-49dc-ab36-bf3fdebdc629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is lambda in Python? Why is it used?\\n\\nLambda functions are small anonymous functions defined using the lambda keyword. They are used for creating small, throwaway functions without needing to formally define a function using def.\\nadd = lambda x, y: x + y\\nprint(add(2, 3))  # Output: 5\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is lambda in Python? Why is it used?\n",
    "\n",
    "Lambda functions are small anonymous functions defined using the lambda keyword. They are used for creating small, throwaway functions without needing to formally define a function using def.\n",
    "add = lambda x, y: x + y\n",
    "print(add(2, 3))  # Output: 5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb67704-28f7-43e3-8301-058204a91e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain split() and join() functions in Python?\\n\\nsplit(): Splits a string into a list of substrings based on a specified delimiter.\\ntext = \"Hello, World\"\\nwords = text.split(\\',\\')  # words = [\\'Hello\\', \\' World\\']\\n\\njoin(): Joins a list of strings into a single string with a specified delimiter.\\nwords = [\\'Hello\\', \\'World\\']\\ntext = \\' \\'.join(words)  # text = \"Hello World\"\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Explain split() and join() functions in Python?\n",
    "\n",
    "split(): Splits a string into a list of substrings based on a specified delimiter.\n",
    "text = \"Hello, World\"\n",
    "words = text.split(',')  # words = ['Hello', ' World']\n",
    "\n",
    "join(): Joins a list of strings into a single string with a specified delimiter.\n",
    "words = ['Hello', 'World']\n",
    "text = ' '.join(words)  # text = \"Hello World\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "220ba5fd-c34e-467b-987e-2eb1c0b4e1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are iterators, iterable & generators in Python?\\n\\nIterable: An object capable of returning its members one at a time. Examples include lists, tuples, and string\\nmy_list = [1, 2, 3]\\nfor item in my_list:\\n    print(item)\\n\\nIterator: An object representing a stream of data. It implements the __iter__() and __next__() methods.\\nmy_iterator = iter(my_list)\\nprint(next(my_iterator))  # Output: 1\\n\\nGenerator: A special type of iterator that yields values using the yield keyword\\ndef my_generator():\\n    yield 1\\n    yield 2\\n    yield 3\\n\\nfor value in my_generator():\\n    print(value)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What are iterators, iterable & generators in Python?\n",
    "\n",
    "Iterable: An object capable of returning its members one at a time. Examples include lists, tuples, and string\n",
    "my_list = [1, 2, 3]\n",
    "for item in my_list:\n",
    "    print(item)\n",
    "\n",
    "Iterator: An object representing a stream of data. It implements the __iter__() and __next__() methods.\n",
    "my_iterator = iter(my_list)\n",
    "print(next(my_iterator))  # Output: 1\n",
    "\n",
    "Generator: A special type of iterator that yields values using the yield keyword\n",
    "def my_generator():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3\n",
    "\n",
    "for value in my_generator():\n",
    "    print(value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbe9a166-aa9f-4ab3-bfb4-d739c19e817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the difference between xrange and range in Python?\\n\\nIn Python 2, range() returns a list, whereas xrange() returns an xrange object, which generates values on demand and is more memory efficient.\\nIn Python 3, xrange() is removed, and range() behaves like xrange() from Python 2, returning an iterator\\nfor i in range(5):\\n    print(i)  # In Python 3, this is memory efficient like xrange in Python 2\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is the difference between xrange and range in Python?\n",
    "\n",
    "In Python 2, range() returns a list, whereas xrange() returns an xrange object, which generates values on demand and is more memory efficient.\n",
    "In Python 3, xrange() is removed, and range() behaves like xrange() from Python 2, returning an iterator\n",
    "for i in range(5):\n",
    "    print(i)  # In Python 3, this is memory efficient like xrange in Python 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc5881a1-1271-4489-84b7-c04f8094abf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPillars of OOPs:\\n\\nThe four main pillars of Object-Oriented Programming (OOP) are:\\nEncapsulation: Bundling the data (attributes) and methods (functions) that operate on the data into a single unit or class.\\nAbstraction: Hiding the complex implementation details and showing only the necessary features of an object.\\nInheritance: Creating a new class based on an existing class to promote code reuse.\\nPolymorphism: Allowing objects of different classes to be treated as objects of a common superclass, usually through method overriding.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pillars of OOPs:\n",
    "\n",
    "The four main pillars of Object-Oriented Programming (OOP) are:\n",
    "Encapsulation: Bundling the data (attributes) and methods (functions) that operate on the data into a single unit or class.\n",
    "Abstraction: Hiding the complex implementation details and showing only the necessary features of an object.\n",
    "Inheritance: Creating a new class based on an existing class to promote code reuse.\n",
    "Polymorphism: Allowing objects of different classes to be treated as objects of a common superclass, usually through method overriding.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01ee9b3b-ff84-483f-a2f1-d98731cb0dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How will you check if a class is a child of another class?\\n\\nYou can use the issubclass() function to check if a class is a subclass of another class.\\nclass Parent:\\n    pass\\n\\nclass Child(Parent):\\n    pass\\n\\nprint(issubclass(Child, Parent))  # Output: True\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"How will you check if a class is a child of another class?\n",
    "\n",
    "You can use the issubclass() function to check if a class is a subclass of another class.\n",
    "class Parent:\n",
    "    pass\n",
    "\n",
    "class Child(Parent):\n",
    "    pass\n",
    "\n",
    "print(issubclass(Child, Parent))  # Output: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e5ef88-3e8f-408a-b459-7888241a5c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow does inheritance work in Python? Explain all types of inheritance with an example.\\n\\nSingle Inheritance: A class inherits from one parent class\\nclass Parent:\\n    pass\\n\\nclass Child(Parent):\\n    pass\\n\\nMultiple Inheritance: A class inherits from multiple parent classes\\nclass Parent1:\\n    pass\\n\\nclass Parent2:\\n    pass\\n\\nclass Child(Parent1, Parent2):\\n    pass\\n\\nMultilevel Inheritance: A class inherits from a class that is also a child class\\nclass Grandparent:\\n    pass\\n\\nclass Parent(Grandparent):\\n    pass\\n\\nclass Child(Parent):\\n    pass\\n\\n\\nHierarchical Inheritance: Multiple classes inherit from one parent class\\nclass Parent:\\n    pass\\n\\nclass Child1(Parent):\\n    pass\\n\\nclass Child2(Parent):\\n    pass\\n\\nHybrid Inheritance: A combination of two or more types of inheritance\\nclass Parent:\\n    pass\\n\\nclass Child1(Parent):\\n    pass\\n\\nclass Child2(Parent):\\n    pass\\n\\nclass Grandchild(Child1, Child2):\\n    pass\\n    '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How does inheritance work in Python? Explain all types of inheritance with an example.\n",
    "\n",
    "Single Inheritance: A class inherits from one parent class\n",
    "class Parent:\n",
    "    pass\n",
    "\n",
    "class Child(Parent):\n",
    "    pass\n",
    "\n",
    "Multiple Inheritance: A class inherits from multiple parent classes\n",
    "class Parent1:\n",
    "    pass\n",
    "\n",
    "class Parent2:\n",
    "    pass\n",
    "\n",
    "class Child(Parent1, Parent2):\n",
    "    pass\n",
    "\n",
    "Multilevel Inheritance: A class inherits from a class that is also a child class\n",
    "class Grandparent:\n",
    "    pass\n",
    "\n",
    "class Parent(Grandparent):\n",
    "    pass\n",
    "\n",
    "class Child(Parent):\n",
    "    pass\n",
    "\n",
    "\n",
    "Hierarchical Inheritance: Multiple classes inherit from one parent class\n",
    "class Parent:\n",
    "    pass\n",
    "\n",
    "class Child1(Parent):\n",
    "    pass\n",
    "\n",
    "class Child2(Parent):\n",
    "    pass\n",
    "\n",
    "Hybrid Inheritance: A combination of two or more types of inheritance\n",
    "class Parent:\n",
    "    pass\n",
    "\n",
    "class Child1(Parent):\n",
    "    pass\n",
    "\n",
    "class Child2(Parent):\n",
    "    pass\n",
    "\n",
    "class Grandchild(Child1, Child2):\n",
    "    pass\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "982bddce-1913-4b7d-975f-43ae9e642516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is encapsulation? Explain it with an example.\\n\\nEncapsulation is the wrapping of data and methods into a single unit or class. It restricts direct access to some of an object's components and can prevent the accidental modification of data.\\nclass MyClass:\\n    def __init__(self, value):\\n        self.__value = value  # Private attribute\\n\\n    def get_value(self):\\n        return self.__value\\n\\n    def set_value(self, value):\\n        self.__value = value\\n\\nobj = MyClass(10)\\nprint(obj.get_value())  # Output: 10\\nobj.set_value(20)\\nprint(obj.get_value())  # Output: 20\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is encapsulation? Explain it with an example.\n",
    "\n",
    "Encapsulation is the wrapping of data and methods into a single unit or class. It restricts direct access to some of an object's components and can prevent the accidental modification of data.\n",
    "class MyClass:\n",
    "    def __init__(self, value):\n",
    "        self.__value = value  # Private attribute\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.__value\n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.__value = value\n",
    "\n",
    "obj = MyClass(10)\n",
    "print(obj.get_value())  # Output: 10\n",
    "obj.set_value(20)\n",
    "print(obj.get_value())  # Output: 20\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab7aaaf0-6bc7-4434-abb3-4d110749060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is polymorphism? Explain it with an example.\\n\\nPolymorphism allows objects of different classes to be treated as objects of a common superclass. It is usually implemented through method overriding.\\nclass Animal:\\n    def make_sound(self):\\n        pass\\n\\nclass Dog(Animal):\\n    def make_sound(self):\\n        return \"Bark\"\\n\\nclass Cat(Animal):\\n    def make_sound(self):\\n        return \"Meow\"\\n\\ndef animal_sound(animal):\\n    print(animal.make_sound())\\n\\ndog = Dog()\\ncat = Cat()\\nanimal_sound(dog)  # Output: Bark\\nanimal_sound(cat)  # Output: Meow\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What is polymorphism? Explain it with an example.\n",
    "\n",
    "Polymorphism allows objects of different classes to be treated as objects of a common superclass. It is usually implemented through method overriding.\n",
    "class Animal:\n",
    "    def make_sound(self):\n",
    "        pass\n",
    "\n",
    "class Dog(Animal):\n",
    "    def make_sound(self):\n",
    "        return \"Bark\"\n",
    "\n",
    "class Cat(Animal):\n",
    "    def make_sound(self):\n",
    "        return \"Meow\"\n",
    "\n",
    "def animal_sound(animal):\n",
    "    print(animal.make_sound())\n",
    "\n",
    "dog = Dog()\n",
    "cat = Cat()\n",
    "animal_sound(dog)  # Output: Bark\n",
    "animal_sound(cat)  # Output: Meow\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a92f51ff-e10f-4497-929e-6796c5bb4ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe validity of identifier names is governed by the programming language\\'s rules. Typically, identifiers must start with a letter (a-z or A-Z) or an underscore (_), followed by letters, digits (0-9), or underscores. Here are the identifiers evaluated based on these rules:\\n\\na) Serial_no\\n\\nValid: Starts with a letter, followed by letters and an underscore.\\nb) 1st_Room\\n\\nInvalid: Starts with a digit.\\nc) Hundred$\\n\\nValid: Starts with a letter, followed by letters and a dollar sign ($). Some languages like Python do not allow $, but many others like JavaScript and PHP do.\\nd) Total_Marks\\n\\nValid: Starts with a letter, followed by letters and an underscore.\\ne) total-Marks\\n\\nInvalid: Hyphens (-) are not allowed in identifiers as they are interpreted as the subtraction operator.\\nf) Total Marks\\n\\nInvalid: Spaces are not allowed in identifiers.\\ng) True\\n\\nInvalid: Reserved keyword in many programming languages (e.g., Python, JavaScript).\\nh) _Percentag\\n\\nValid: Starts with an underscore, followed by letters\\n\\nExamples of Invalid Identifiers:\\n\\n1st_Room: In Python, trying to assign a value to 1st_Room will result in a syntax error.\\n\\npython\\nCopy code\\n1st_Room = \"Living Room\"\\nError: SyntaxError: invalid decimal literal\\n\\ntotal-Marks: In most languages, this will be interpreted as an operation rather than a valid identifier.\\n\\ntotal-Marks = 100\\nError: SyntaxError: can\\'t assign to operator\\n\\nTotal Marks: The space will cause an error in most languages.\\n\\n\\nTotal Marks = 100\\nError: SyntaxError: invalid syntax\\n\\nTrue: Attempting to assign to a keyword.\\n\\nTrue = 1\\nError: SyntaxError: cannot assign to True\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "The validity of identifier names is governed by the programming language's rules. Typically, identifiers must start with a letter (a-z or A-Z) or an underscore (_), followed by letters, digits (0-9), or underscores. Here are the identifiers evaluated based on these rules:\n",
    "\n",
    "a) Serial_no\n",
    "\n",
    "Valid: Starts with a letter, followed by letters and an underscore.\n",
    "b) 1st_Room\n",
    "\n",
    "Invalid: Starts with a digit.\n",
    "c) Hundred$\n",
    "\n",
    "Valid: Starts with a letter, followed by letters and a dollar sign ($). Some languages like Python do not allow $, but many others like JavaScript and PHP do.\n",
    "d) Total_Marks\n",
    "\n",
    "Valid: Starts with a letter, followed by letters and an underscore.\n",
    "e) total-Marks\n",
    "\n",
    "Invalid: Hyphens (-) are not allowed in identifiers as they are interpreted as the subtraction operator.\n",
    "f) Total Marks\n",
    "\n",
    "Invalid: Spaces are not allowed in identifiers.\n",
    "g) True\n",
    "\n",
    "Invalid: Reserved keyword in many programming languages (e.g., Python, JavaScript).\n",
    "h) _Percentag\n",
    "\n",
    "Valid: Starts with an underscore, followed by letters\n",
    "\n",
    "Examples of Invalid Identifiers:\n",
    "\n",
    "1st_Room: In Python, trying to assign a value to 1st_Room will result in a syntax error.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "1st_Room = \"Living Room\"\n",
    "Error: SyntaxError: invalid decimal literal\n",
    "\n",
    "total-Marks: In most languages, this will be interpreted as an operation rather than a valid identifier.\n",
    "\n",
    "total-Marks = 100\n",
    "Error: SyntaxError: can't assign to operator\n",
    "\n",
    "Total Marks: The space will cause an error in most languages.\n",
    "\n",
    "\n",
    "Total Marks = 100\n",
    "Error: SyntaxError: invalid syntax\n",
    "\n",
    "True: Attempting to assign to a keyword.\n",
    "\n",
    "True = 1\n",
    "Error: SyntaxError: cannot assign to True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c35fbd8a-78ea-4b5b-acd1-f9d4bc3be5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA)To add an element \"freedom_fighter\" at the 0th index of the list name, you can use the insert method in Python. Here\\'s how you can do it:\\n\\n\\n# Original list\\nname = [\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapu\"]\\n\\n# Adding \"freedom_fighter\" at the 0th index\\nname.insert(0, \"freedom_fighter\")\\n\\n# Printing the updated list\\nprint(name)\\nAfter running the above code, the updated list will be:\\n\\n[\\'freedom_fighter\\', \\'Mohan\\', \\'dash\\', \\'karam\\', \\'chandra\\', \\'gandhi\\', \\'Bapu\\']\\nTo find the output of the given code, we need to break down and understand the list slicing and the len function used in the code.\\n\\nHere is the given code:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\\n\\nlength1 = len((name[-len(name)+1:-1:2]))\\n\\nlength2 = len((name[-len(name)+1:-1]))\\n\\nprint(length1 + length2)\\nStep-by-Step Explanation\\nUnderstanding the List:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\\nThere is a small typo here: \"MOhan\" \"dash\" should be \"MOhan\", \"dash\". However, Python will concatenate these two strings due to the lack of a comma, making it:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\nSo, the corrected list is:\\n\\n[\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\nCalculating length1:\\n\\nlength1 = len((name[-len(name)+1:-1:2]))\\n-len(name) evaluates to -6 (since the length of name is 6).\\n-len(name) + 1 evaluates to -5.\\nname[-5:-1:2] means starting from the second element (index -5) to the second last element (index -1) with a step of 2.\\nSlicing:\\n\\nname[-5:-1:2] = [\"Bapuji\", \"karam\"]\\nThe length of this slice is 2.\\nCalculating length2:\\n\\nlength2 = len((name[-len(name)+1:-1]))\\nThis slice is name[-5:-1], which includes elements from index -5 to index -2.\\nSlicing:\\n\\nname[-5:-1] = [\"Bapuji\", \"MOhandash\", \"karam\", \"chandra\"]\\nThe length of this slice is 4.\\nSumming the Lengths:\\n\\nprint(length1 + length2)\\nlength1 is 2.\\nlength2 is 4.\\nThe sum is 2 + 4 = 6.\\nOutput\\n6\\n\\nTo add two more elements [\"NetaJi\", \"Bose\"] at the end of the list, you can use the extend method in Python. Here\\'s how you can do it:\\n\\n# Original list\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\n\\n# Adding [\"NetaJi\", \"Bose\"] at the end of the list\\nname.extend([\"NetaJi\", \"Bose\"])\\n\\n# Printing the updated list\\nprint(name)\\nAfter running the above code, the updated list will be:\\n\\n[\\'freedomFighter\\', \\'Bapuji\\', \\'MOhandash\\', \\'karam\\', \\'chandra\\', \\'gandhi\\', \\'NetaJi\\', \\'Bose\\']\\n\\nc) add two more elements in the name [\"NetaJi\",\"Bose\"] at the end of the list.\\n\\n\\nd) what will be the value of temp:\\n\\nname = [\"Bapuji\", \"dash\", \"karam\", \"chandra\",\"gandi\",\"Mohan\"]\\n\\ntemp=name[-1]\\n\\nname[-1]=name[0]\\n\\nname[0]=temp\\n\\nprint(name)\\n\\nTo find the output of the given code, we need to break down and understand the list slicing and the len function used in the code.\\n\\nHere is the given code:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\\n\\nlength1 = len((name[-len(name)+1:-1:2]))\\n\\nlength2 = len((name[-len(name)+1:-1]))\\n\\nprint(length1 + length2)\\nStep-by-Step Explanation\\nUnderstanding the List:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\\nThere is a small typo here: \"MOhan\" \"dash\" should be \"MOhan\", \"dash\". However, Python will concatenate these two strings due to the lack of a comma, making it:\\n\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\nSo, the corrected list is:\\n\\n[\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\nCalculating length1:\\n\\nlength1 = len((name[-len(name)+1:-1:2]))\\n-len(name) evaluates to -6 (since the length of name is 6).\\n-len(name) + 1 evaluates to -5.\\nname[-5:-1:2] means starting from the second element (index -5) to the second last element (index -1) with a step of 2.\\nSlicing:\\n\\nname[-5:-1:2] = [\"Bapuji\", \"karam\"]\\nThe length of this slice is 2.\\nCalculating length2:\\n\\nlength2 = len((name[-len(name)+1:-1]))\\nThis slice is name[-5:-1], which includes elements from index -5 to index -2.\\nSlicing:\\n\\nname[-5:-1] = [\"Bapuji\", \"MOhandash\", \"karam\", \"chandra\"]\\nThe length of this slice is 4.\\nSumming the Lengths:\\n\\nprint(length1 + length2)\\nlength1 is 2.\\nlength2 is 4.\\nThe sum is 2 + 4 = 6.\\nOutput\\n6\\n\\n\\nc) Add two more elements to the list\\nTo add the elements [\"NetaJi\", \"Bose\"] at the end of the list, you can use the extend method:\\n\\n# Original list\\nname = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\\n\\n# Adding [\"NetaJi\", \"Bose\"] at the end of the list\\nname.extend([\"NetaJi\", \"Bose\"])\\n\\n# Printing the updated list\\nprint(name)\\nAfter running the above code, the updated list will be:\\n\\n[\\'freedomFighter\\', \\'Bapuji\\', \\'MOhandash\\', \\'karam\\', \\'chandra\\', \\'gandhi\\', \\'NetaJi\\', \\'Bose\\']\\nd) Value of temp and final list\\nThe given code swaps the first and last elements of the list name. Here’s the code provided:\\n\\nname = [\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Mohan\"]\\n\\ntemp = name[-1]  # temp will be the last element of the list\\nname[-1] = name[0]  # last element is replaced with the first element\\nname[0] = temp  # first element is replaced with the value stored in temp\\n\\nprint(name)\\nLet\\'s break it down:\\n\\nInitial list:\\n\\n[\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Mohan\"]\\nAssign the last element to temp:\\n\\ntemp = name[-1]\\n# temp = \"Mohan\"\\nReplace the last element with the first element:\\n\\nname[-1] = name[0]\\n# name becomes [\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\\nReplace the first element with temp:\\n\\nname[0] = temp\\n# name becomes [\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\\nSo, the value of temp is \"Mohan\", and the final list name will be:\\n\\n[\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A)To add an element \"freedom_fighter\" at the 0th index of the list name, you can use the insert method in Python. Here's how you can do it:\n",
    "\n",
    "\n",
    "# Original list\n",
    "name = [\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapu\"]\n",
    "\n",
    "# Adding \"freedom_fighter\" at the 0th index\n",
    "name.insert(0, \"freedom_fighter\")\n",
    "\n",
    "# Printing the updated list\n",
    "print(name)\n",
    "After running the above code, the updated list will be:\n",
    "\n",
    "['freedom_fighter', 'Mohan', 'dash', 'karam', 'chandra', 'gandhi', 'Bapu']\n",
    "To find the output of the given code, we need to break down and understand the list slicing and the len function used in the code.\n",
    "\n",
    "Here is the given code:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "\n",
    "length1 = len((name[-len(name)+1:-1:2]))\n",
    "\n",
    "length2 = len((name[-len(name)+1:-1]))\n",
    "\n",
    "print(length1 + length2)\n",
    "Step-by-Step Explanation\n",
    "Understanding the List:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "There is a small typo here: \"MOhan\" \"dash\" should be \"MOhan\", \"dash\". However, Python will concatenate these two strings due to the lack of a comma, making it:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "So, the corrected list is:\n",
    "\n",
    "[\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "Calculating length1:\n",
    "\n",
    "length1 = len((name[-len(name)+1:-1:2]))\n",
    "-len(name) evaluates to -6 (since the length of name is 6).\n",
    "-len(name) + 1 evaluates to -5.\n",
    "name[-5:-1:2] means starting from the second element (index -5) to the second last element (index -1) with a step of 2.\n",
    "Slicing:\n",
    "\n",
    "name[-5:-1:2] = [\"Bapuji\", \"karam\"]\n",
    "The length of this slice is 2.\n",
    "Calculating length2:\n",
    "\n",
    "length2 = len((name[-len(name)+1:-1]))\n",
    "This slice is name[-5:-1], which includes elements from index -5 to index -2.\n",
    "Slicing:\n",
    "\n",
    "name[-5:-1] = [\"Bapuji\", \"MOhandash\", \"karam\", \"chandra\"]\n",
    "The length of this slice is 4.\n",
    "Summing the Lengths:\n",
    "\n",
    "print(length1 + length2)\n",
    "length1 is 2.\n",
    "length2 is 4.\n",
    "The sum is 2 + 4 = 6.\n",
    "Output\n",
    "6\n",
    "\n",
    "To add two more elements [\"NetaJi\", \"Bose\"] at the end of the list, you can use the extend method in Python. Here's how you can do it:\n",
    "\n",
    "# Original list\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "\n",
    "# Adding [\"NetaJi\", \"Bose\"] at the end of the list\n",
    "name.extend([\"NetaJi\", \"Bose\"])\n",
    "\n",
    "# Printing the updated list\n",
    "print(name)\n",
    "After running the above code, the updated list will be:\n",
    "\n",
    "['freedomFighter', 'Bapuji', 'MOhandash', 'karam', 'chandra', 'gandhi', 'NetaJi', 'Bose']\n",
    "\n",
    "c) add two more elements in the name [\"NetaJi\",\"Bose\"] at the end of the list.\n",
    "\n",
    "\n",
    "d) what will be the value of temp:\n",
    "\n",
    "name = [\"Bapuji\", \"dash\", \"karam\", \"chandra\",\"gandi\",\"Mohan\"]\n",
    "\n",
    "temp=name[-1]\n",
    "\n",
    "name[-1]=name[0]\n",
    "\n",
    "name[0]=temp\n",
    "\n",
    "print(name)\n",
    "\n",
    "To find the output of the given code, we need to break down and understand the list slicing and the len function used in the code.\n",
    "\n",
    "Here is the given code:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "\n",
    "length1 = len((name[-len(name)+1:-1:2]))\n",
    "\n",
    "length2 = len((name[-len(name)+1:-1]))\n",
    "\n",
    "print(length1 + length2)\n",
    "Step-by-Step Explanation\n",
    "Understanding the List:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "There is a small typo here: \"MOhan\" \"dash\" should be \"MOhan\", \"dash\". However, Python will concatenate these two strings due to the lack of a comma, making it:\n",
    "\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "So, the corrected list is:\n",
    "\n",
    "[\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "Calculating length1:\n",
    "\n",
    "length1 = len((name[-len(name)+1:-1:2]))\n",
    "-len(name) evaluates to -6 (since the length of name is 6).\n",
    "-len(name) + 1 evaluates to -5.\n",
    "name[-5:-1:2] means starting from the second element (index -5) to the second last element (index -1) with a step of 2.\n",
    "Slicing:\n",
    "\n",
    "name[-5:-1:2] = [\"Bapuji\", \"karam\"]\n",
    "The length of this slice is 2.\n",
    "Calculating length2:\n",
    "\n",
    "length2 = len((name[-len(name)+1:-1]))\n",
    "This slice is name[-5:-1], which includes elements from index -5 to index -2.\n",
    "Slicing:\n",
    "\n",
    "name[-5:-1] = [\"Bapuji\", \"MOhandash\", \"karam\", \"chandra\"]\n",
    "The length of this slice is 4.\n",
    "Summing the Lengths:\n",
    "\n",
    "print(length1 + length2)\n",
    "length1 is 2.\n",
    "length2 is 4.\n",
    "The sum is 2 + 4 = 6.\n",
    "Output\n",
    "6\n",
    "\n",
    "\n",
    "c) Add two more elements to the list\n",
    "To add the elements [\"NetaJi\", \"Bose\"] at the end of the list, you can use the extend method:\n",
    "\n",
    "# Original list\n",
    "name = [\"freedomFighter\", \"Bapuji\", \"MOhandash\", \"karam\", \"chandra\", \"gandhi\"]\n",
    "\n",
    "# Adding [\"NetaJi\", \"Bose\"] at the end of the list\n",
    "name.extend([\"NetaJi\", \"Bose\"])\n",
    "\n",
    "# Printing the updated list\n",
    "print(name)\n",
    "After running the above code, the updated list will be:\n",
    "\n",
    "['freedomFighter', 'Bapuji', 'MOhandash', 'karam', 'chandra', 'gandhi', 'NetaJi', 'Bose']\n",
    "d) Value of temp and final list\n",
    "The given code swaps the first and last elements of the list name. Here’s the code provided:\n",
    "\n",
    "name = [\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Mohan\"]\n",
    "\n",
    "temp = name[-1]  # temp will be the last element of the list\n",
    "name[-1] = name[0]  # last element is replaced with the first element\n",
    "name[0] = temp  # first element is replaced with the value stored in temp\n",
    "\n",
    "print(name)\n",
    "Let's break it down:\n",
    "\n",
    "Initial list:\n",
    "\n",
    "[\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Mohan\"]\n",
    "Assign the last element to temp:\n",
    "\n",
    "temp = name[-1]\n",
    "# temp = \"Mohan\"\n",
    "Replace the last element with the first element:\n",
    "\n",
    "name[-1] = name[0]\n",
    "# name becomes [\"Bapuji\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\n",
    "Replace the first element with temp:\n",
    "\n",
    "name[0] = temp\n",
    "# name becomes [\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\n",
    "So, the value of temp is \"Mohan\", and the final list name will be:\n",
    "\n",
    "[\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\", \"Bapuji\"]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f65299d-dc9d-4e84-b57e-085bd955c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "7\n",
      "2\n",
      "4\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Given List:\n",
    "animal = ['Human', 'cat', 'mat', 'cat', 'rat', 'Human', 'Lion']\n",
    "#Operations:\n",
    "#Count the occurrences of 'Human':\n",
    "\n",
    "print(animal.count('Human'))\n",
    "#The count method returns the number of times the specified element appears in the list.\n",
    "#In the list animal, 'Human' appears twice.Output: 2\n",
    "#Find the index of 'rat':\n",
    "\n",
    "print(animal.index('rat'))\n",
    "#The index method returns the index of the first occurrence of the specified element.\n",
    "#In the list animal, 'rat' appears at index 4.\n",
    "#Output: 4\n",
    "#Find the length of the list:\n",
    "\n",
    "print(len(animal))\n",
    "#The len function returns the number of elements in the list.\n",
    "#In the list animal, there are 7 elements.\n",
    "#Output: 7\n",
    "#Complete Code with Outputs:\n",
    "\n",
    "animal = ['Human', 'cat', 'mat', 'cat', 'rat', 'Human', 'Lion']\n",
    "\n",
    "print(animal.count('Human'))  # Output: 2\n",
    "print(animal.index('rat'))    # Output: 4\n",
    "print(len(animal))            # Output: 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "988c1fd4-e42f-492f-8370-f773d742cac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven Tuple:\\ntuple1 = (10, 20, \"Apple\", 3.4, \\'a\\', [\"master\", \"ji\"], (\"sita\", \"geeta\", 22), [{\"roll_no\": \"N1\"}, {\"name\": \"Navneet\"}])\\na) Print the length of tuple1:\\nThe len function returns the number of elements in the tuple.\\n\\nprint(len(tuple1))  # Output: 8\\nb) Print the value of \"name\" in the last element of tuple1:\\nTo access the value of \"name\" in the last dictionary of the tuple:\\n\\nprint(tuple1[-1][-1][\"name\"])  # Output: \"Navneet\"\\nc) Fetch the value of roll_no from this tuple:\\nTo access the value of roll_no in the first dictionary of the last element:\\n\\nroll_no_value = tuple1[-1][0][\"roll_no\"]\\nprint(roll_no_value)  # Output: \"N1\"\\nd) Print the second element of the second-to-last tuple element:\\nTo access the second element (\"geeta\") from the tuple element that is third from the end:\\nprint(tuple1[-3][1])  # Output: \"geeta\"\\ne) Fetch the element 22 from this tuple:\\nTo access the element 22 in the tuple within tuple1:\\nelement_22 = tuple1[-3][2]\\nprint(element_22)  # Output: 22\\nComplete Code with Outputs:\\ntuple1 = (10, 20, \"Apple\", 3.4, \\'a\\', [\"master\", \"ji\"], (\"sita\", \"geeta\", 22), [{\"roll_no\": \"N1\"}, {\"name\": \"Navneet\"}])\\n\\n# a) Print the length of tuple1\\nprint(len(tuple1))  # Output: 8\\n\\n# b) Print the value of \"name\" in the last element of tuple1\\nprint(tuple1[-1][-1][\"name\"])  # Output: \"Navneet\"\\n\\n# c) Fetch the value of roll_no from this tuple\\nroll_no_value = tuple1[-1][0][\"roll_no\"]\\nprint(roll_no_value)  # Output: \"N1\"\\n\\n# d) Print the second element of the second-to-last tuple element\\nprint(tuple1[-3][1])  # Output: \"geeta\"\\n\\n# e) Fetch the element 22 from this tuple\\nelement_22 = tuple1[-3][2]\\nprint(element_22)  # Output: 22\\nThis code correctly performs each of the specified operations and prints the desired values.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given Tuple:\n",
    "tuple1 = (10, 20, \"Apple\", 3.4, 'a', [\"master\", \"ji\"], (\"sita\", \"geeta\", 22), [{\"roll_no\": \"N1\"}, {\"name\": \"Navneet\"}])\n",
    "a) Print the length of tuple1:\n",
    "The len function returns the number of elements in the tuple.\n",
    "\n",
    "print(len(tuple1))  # Output: 8\n",
    "b) Print the value of \"name\" in the last element of tuple1:\n",
    "To access the value of \"name\" in the last dictionary of the tuple:\n",
    "\n",
    "print(tuple1[-1][-1][\"name\"])  # Output: \"Navneet\"\n",
    "c) Fetch the value of roll_no from this tuple:\n",
    "To access the value of roll_no in the first dictionary of the last element:\n",
    "\n",
    "roll_no_value = tuple1[-1][0][\"roll_no\"]\n",
    "print(roll_no_value)  # Output: \"N1\"\n",
    "d) Print the second element of the second-to-last tuple element:\n",
    "To access the second element (\"geeta\") from the tuple element that is third from the end:\n",
    "print(tuple1[-3][1])  # Output: \"geeta\"\n",
    "e) Fetch the element 22 from this tuple:\n",
    "To access the element 22 in the tuple within tuple1:\n",
    "element_22 = tuple1[-3][2]\n",
    "print(element_22)  # Output: 22\n",
    "Complete Code with Outputs:\n",
    "tuple1 = (10, 20, \"Apple\", 3.4, 'a', [\"master\", \"ji\"], (\"sita\", \"geeta\", 22), [{\"roll_no\": \"N1\"}, {\"name\": \"Navneet\"}])\n",
    "\n",
    "# a) Print the length of tuple1\n",
    "print(len(tuple1))  # Output: 8\n",
    "\n",
    "# b) Print the value of \"name\" in the last element of tuple1\n",
    "print(tuple1[-1][-1][\"name\"])  # Output: \"Navneet\"\n",
    "\n",
    "# c) Fetch the value of roll_no from this tuple\n",
    "roll_no_value = tuple1[-1][0][\"roll_no\"]\n",
    "print(roll_no_value)  # Output: \"N1\"\n",
    "\n",
    "# d) Print the second element of the second-to-last tuple element\n",
    "print(tuple1[-3][1])  # Output: \"geeta\"\n",
    "\n",
    "# e) Fetch the element 22 from this tuple\n",
    "element_22 = tuple1[-3][2]\n",
    "print(element_22)  # Output: 22\n",
    "This code correctly performs each of the specified operations and prints the desired values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3431eba5-fc79-4560-ae8a-efaff36dd6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Function to display the message based on the signal color\\ndef signal_message(color):\\n    if color.lower() == \"red\":\\n        return \"Stop\"\\n    elif color.lower() == \"yellow\":\\n        return \"Stay\"\\n    elif color.lower() == \"green\":\\n        return \"Go\"\\n    else:\\n        return \"Invalid color\"\\n\\n# Test the function\\ncolor = input(\"Enter the color of the signal (Red/Yellow/Green): \").strip()\\nmessage = signal_message(color)\\nprint(message)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Function to display the message based on the signal color\n",
    "def signal_message(color):\n",
    "    if color.lower() == \"red\":\n",
    "        return \"Stop\"\n",
    "    elif color.lower() == \"yellow\":\n",
    "        return \"Stay\"\n",
    "    elif color.lower() == \"green\":\n",
    "        return \"Go\"\n",
    "    else:\n",
    "        return \"Invalid color\"\n",
    "\n",
    "# Test the function\n",
    "color = input(\"Enter the color of the signal (Red/Yellow/Green): \").strip()\n",
    "message = signal_message(color)\n",
    "print(message)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05f99e3f-26f6-47ba-91cb-358e97870886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Calculator\n",
      "Choose an operation: +, -, *, /\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter operator:  +\n",
      "Enter first number:  12\n",
      "Enter second number:  23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is: 35.0\n"
     ]
    }
   ],
   "source": [
    "# Function to perform the calculations\n",
    "def calculator(num1, num2, operator):\n",
    "    if operator == '+':\n",
    "        return num1 + num2\n",
    "    elif operator == '-':\n",
    "        return num1 - num2\n",
    "    elif operator == '*':\n",
    "        return num1 * num2\n",
    "    elif operator == '/':\n",
    "        if num2 != 0:\n",
    "            return num1 / num2\n",
    "        else:\n",
    "            return \"Error: Division by zero\"\n",
    "    else:\n",
    "        return \"Invalid operator\"\n",
    "\n",
    "# Main program\n",
    "print(\"Simple Calculator\")\n",
    "print(\"Choose an operation: +, -, *, /\")\n",
    "operator = input(\"Enter operator: \").strip()\n",
    "\n",
    "try:\n",
    "    num1 = float(input(\"Enter first number: \"))\n",
    "    num2 = float(input(\"Enter second number: \"))\n",
    "    result = calculator(num1, num2, operator)\n",
    "    print(f\"The result is: {result}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input: Please enter numeric values for the numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "991f2ce6-ea34-4030-9ca7-56ecdda79f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest number among 10, 20, and 15 is: 20\n"
     ]
    }
   ],
   "source": [
    "# Pre-specified numbers\n",
    "num1 = 10\n",
    "num2 = 20\n",
    "num3 = 15\n",
    "\n",
    "# Finding the largest using nested ternary operators\n",
    "largest = num1 if (num1 > num2 and num1 > num3) else (num2 if num2 > num3 else num3)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The largest number among {num1}, {num2}, and {num3} is: {largest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3877dd27-272d-45bc-9959-32c549acfe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a whole number:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors of 5 are: [1, 5]\n"
     ]
    }
   ],
   "source": [
    "# Function to find and print factors of a number\n",
    "def find_factors(number):\n",
    "    if number <= 0:\n",
    "        print(\"Please enter a positive whole number.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the counter\n",
    "    factor = 1\n",
    "    \n",
    "    # List to store factors\n",
    "    factors = []\n",
    "    \n",
    "    # While loop to find factors\n",
    "    while factor <= number:\n",
    "        if number % factor == 0:\n",
    "            factors.append(factor)\n",
    "        factor += 1\n",
    "    \n",
    "    # Print the factors\n",
    "    print(f\"Factors of {number} are: {factors}\")\n",
    "\n",
    "# Main program\n",
    "try:\n",
    "    num = int(input(\"Enter a whole number: \"))\n",
    "    find_factors(num)\n",
    "except ValueError:\n",
    "    print(\"Invalid input: Please enter a valid whole number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33c590e1-d304-45ef-a384-fdda1c663110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a positive number (or a negative number to stop):  5\n",
      "Enter a positive number (or a negative number to stop):  51\n",
      "Enter a positive number (or a negative number to stop):  10\n",
      "Enter a positive number (or a negative number to stop):  20\n",
      "Enter a positive number (or a negative number to stop):  30\n",
      "Enter a positive number (or a negative number to stop):  40\n",
      "Enter a positive number (or a negative number to stop):  50\n",
      "Enter a positive number (or a negative number to stop):  -5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of all positive numbers entered is: 206.0\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the sum of positive numbers\n",
    "def sum_positive_numbers():\n",
    "    total_sum = 0  # Initialize the sum to 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Take user input\n",
    "            number = float(input(\"Enter a positive number (or a negative number to stop): \"))\n",
    "            \n",
    "            # Check if the entered number is negative\n",
    "            if number < 0:\n",
    "                break  # Exit the loop if a negative number is entered\n",
    "            \n",
    "            # Add the positive number to the total sum\n",
    "            total_sum += number\n",
    "        \n",
    "        except ValueError:\n",
    "            print(\"Invalid input: Please enter a valid number.\")\n",
    "    \n",
    "    # Print the sum of positive numbers\n",
    "    print(f\"The sum of all positive numbers entered is: {total_sum}\")\n",
    "\n",
    "# Main program\n",
    "sum_positive_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "892dbec2-2311-482b-97f6-0d37afa843b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime numbers between 2 and 100 are:\n",
      "2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 "
     ]
    }
   ],
   "source": [
    "# Function to find and print prime numbers between 2 and 100\n",
    "def find_primes(start, end):\n",
    "    print(f\"Prime numbers between {start} and {end} are:\")\n",
    "    \n",
    "    for num in range(start, end + 1):\n",
    "        if num > 1:  # Numbers less than or equal to 1 are not prime\n",
    "            is_prime = True\n",
    "            for i in range(2, int(num**0.5) + 1):  # Check divisibility from 2 up to sqrt(num)\n",
    "                if num % i == 0:\n",
    "                    is_prime = False\n",
    "                    break\n",
    "            if is_prime:\n",
    "                print(num, end=' ')  # Print the prime number\n",
    "\n",
    "# Main program\n",
    "find_primes(2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71a6c0de-1dd6-428e-8915-ffc9af80425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the marks for subject 1:  5\n",
      "Enter the marks for subject 2:  6\n",
      "Enter the marks for subject 3:  5\n",
      "Enter the marks for subject 4:  20\n",
      "Enter the marks for subject 5:  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marks in five major subjects: [5, 6, 5, 20, 60]\n",
      "Percentage: 19.20%\n",
      "Grade: Reappear\n"
     ]
    }
   ],
   "source": [
    "# Accepting the marks for five major subjects\n",
    "marks = []\n",
    "for i in range(5):\n",
    "    mark = int(input(f\"Enter the marks for subject {i+1}: \"))\n",
    "    marks.append(mark)\n",
    "\n",
    "# Displaying the marks\n",
    "print(\"Marks in five major subjects:\", marks)\n",
    "\n",
    "# Calculating the total marks\n",
    "total_marks = sum(marks)\n",
    "\n",
    "# Calculating the percentage\n",
    "percentage = total_marks / 5\n",
    "print(f\"Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# Determining the grade based on the percentage\n",
    "def determine_grade(percentage):\n",
    "    match percentage:\n",
    "        case percentage if percentage > 85:\n",
    "            return 'A'\n",
    "        case percentage if 85 >= percentage >= 75:\n",
    "            return 'B'\n",
    "        case percentage if 75 > percentage >= 50:\n",
    "            return 'C'\n",
    "        case percentage if 50 > percentage >= 30:\n",
    "            return 'D'\n",
    "        case _:\n",
    "            return 'Reappear'\n",
    "\n",
    "grade = determine_grade(percentage)\n",
    "print(f\"Grade: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28ba3497-6a76-4640-a71c-8daf785ff602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the wavelength (in nm):  720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color corresponding to the wavelength 720.0 nm is: Red\n"
     ]
    }
   ],
   "source": [
    "# Function to determine the color based on wavelength\n",
    "def determine_color(wavelength):\n",
    "    if 400.0 <= wavelength < 440.0:\n",
    "        return 'Violet'\n",
    "    elif 440.0 <= wavelength < 460.0:\n",
    "        return 'Indigo'\n",
    "    elif 460.0 <= wavelength < 500.0:\n",
    "        return 'Blue'\n",
    "    elif 500.0 <= wavelength < 570.0:\n",
    "        return 'Green'\n",
    "    elif 570.0 <= wavelength < 590.0:\n",
    "        return 'Yellow'\n",
    "    elif 590.0 <= wavelength < 620.0:\n",
    "        return 'Orange'\n",
    "    elif 620.0 <= wavelength <= 720.0:\n",
    "        return 'Red'\n",
    "    else:\n",
    "        return 'Wavelength out of VIBGYOR range'\n",
    "\n",
    "# Accepting the wavelength as input from the user\n",
    "wavelength = float(input(\"Enter the wavelength (in nm): \"))\n",
    "\n",
    "# Determining the color based on the wavelength\n",
    "color = determine_color(wavelength)\n",
    "\n",
    "# Displaying the result\n",
    "print(f\"The color corresponding to the wavelength {wavelength} nm is: {color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95710082-19fc-4431-b328-2dfdafe20b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravitational force between Earth and Sun: 3.54e+22 N\n",
      "Gravitational force between Moon and Earth: 1.98e+20 N\n",
      "The stronger gravitational force is between the Earth and Sun.\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "G = 6.67430e-11  # Gravitational constant in m^3 kg^-1 s^-2\n",
    "mass_earth = 5.972e24  # Mass of Earth in kg\n",
    "mass_moon = 7.34767309e22  # Mass of Moon in kg\n",
    "mass_sun = 1.989e30  # Mass of Sun in kg\n",
    "distance_earth_sun = 1.496e11  # Average distance between Earth and Sun in meters\n",
    "distance_moon_earth = 3.844e8  # Average distance between Moon and Earth in meters\n",
    "\n",
    "# Calculate gravitational force between Earth and Sun\n",
    "F_earth_sun = (G * mass_earth * mass_sun) / (distance_earth_sun ** 2)\n",
    "\n",
    "# Calculate gravitational force between Moon and Earth\n",
    "F_moon_earth = (G * mass_moon * mass_earth) / (distance_moon_earth ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Gravitational force between Earth and Sun: {F_earth_sun:.2e} N\")\n",
    "print(f\"Gravitational force between Moon and Earth: {F_moon_earth:.2e} N\")\n",
    "\n",
    "# Compare the forces\n",
    "if F_earth_sun > F_moon_earth:\n",
    "    stronger_force = \"Earth and Sun\"\n",
    "else:\n",
    "    stronger_force = \"Moon and Earth\"\n",
    "\n",
    "print(f\"The stronger gravitational force is between the {stronger_force}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82ac2cfb-51b9-4e52-b8a8-1b515a733a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Information:\n",
      "Name: Alice\n",
      "Age: 20\n",
      "Roll Number: A123\n",
      "Name: Bob\n",
      "Age: 21\n",
      "Roll Number: B456\n",
      "\n",
      "Updated Information:\n",
      "Name: Alicia\n",
      "Age: 21\n",
      "Roll Number: A123\n",
      "Name: Bob\n",
      "Age: 21\n",
      "Roll Number: B789\n",
      "\n",
      "Testing Getter and Setter Methods:\n",
      "Student 1 Name: Alicia\n",
      "Student 1 Updated Name: Alice Updated\n"
     ]
    }
   ],
   "source": [
    "class Student:\n",
    "    def __init__(self, name, age, roll_number):  # Corrected constructor method\n",
    "        self.__name = name\n",
    "        self.__age = age\n",
    "        self.__roll_number = roll_number\n",
    "\n",
    "    # Getter methods\n",
    "    def get_name(self):\n",
    "        return self.__name\n",
    "\n",
    "    def get_age(self):\n",
    "        return self.__age\n",
    "\n",
    "    def get_roll_number(self):\n",
    "        return self.__roll_number\n",
    "\n",
    "    # Setter methods\n",
    "    def set_name(self, name):\n",
    "        self.__name = name\n",
    "\n",
    "    def set_age(self, age):\n",
    "        self.__age = age\n",
    "\n",
    "    def set_roll_number(self, roll_number):\n",
    "        self.__roll_number = roll_number\n",
    "\n",
    "    # Method to display student information\n",
    "    def display_info(self):\n",
    "        print(f\"Name: {self.__name}\")\n",
    "        print(f\"Age: {self.__age}\")\n",
    "        print(f\"Roll Number: {self.__roll_number}\")\n",
    "\n",
    "    # Method to update student details\n",
    "    def update_details(self, name=None, age=None, roll_number=None):\n",
    "        if name is not None:\n",
    "            self.__name = name\n",
    "        if age is not None:\n",
    "            self.__age = age\n",
    "        if roll_number is not None:\n",
    "            self.__roll_number = roll_number\n",
    "\n",
    "# Create instances of the Student class\n",
    "student1 = Student(\"Alice\", 20, \"A123\")\n",
    "student2 = Student(\"Bob\", 21, \"B456\")\n",
    "\n",
    "# Display initial student information\n",
    "print(\"Initial Information:\")\n",
    "student1.display_info()\n",
    "student2.display_info()\n",
    "\n",
    "# Update student details\n",
    "student1.update_details(name=\"Alicia\", age=21)\n",
    "student2.update_details(roll_number=\"B789\")\n",
    "\n",
    "# Display updated student information\n",
    "print(\"\\nUpdated Information:\")\n",
    "student1.display_info()\n",
    "student2.display_info()\n",
    "\n",
    "# Testing getter and setter methods\n",
    "print(\"\\nTesting Getter and Setter Methods:\")\n",
    "print(f\"Student 1 Name: {student1.get_name()}\")\n",
    "student1.set_name(\"Alice Updated\")\n",
    "print(f\"Student 1 Updated Name: {student1.get_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9572813-cdf3-47dc-a532-faef061bd80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Information:\n",
      "Book Name: 1984\n",
      "Author: George Orwell\n",
      "Availability: Available\n",
      "Book Name: To Kill a Mockingbird\n",
      "Author: Harper Lee\n",
      "Availability: Available\n",
      "\n",
      "Borrowing and Returning Books:\n",
      "The book '1984' has been borrowed.\n",
      "Book Name: 1984\n",
      "Author: George Orwell\n",
      "Availability: Not Available\n",
      "The book '1984' has been returned.\n",
      "Book Name: 1984\n",
      "Author: George Orwell\n",
      "Availability: Available\n",
      "The book 'To Kill a Mockingbird' has been borrowed.\n",
      "Book Name: To Kill a Mockingbird\n",
      "Author: Harper Lee\n",
      "Availability: Not Available\n",
      "Sorry, the book 'To Kill a Mockingbird' is currently unavailable.\n",
      "The book 'To Kill a Mockingbird' has been returned.\n",
      "Book Name: To Kill a Mockingbird\n",
      "Author: Harper Lee\n",
      "Availability: Available\n"
     ]
    }
   ],
   "source": [
    "class LibraryBook:\n",
    "    def __init__(self, book_name, author):  # Corrected constructor method\n",
    "        self.__book_name = book_name\n",
    "        self.__author = author\n",
    "        self.__is_available = True\n",
    "\n",
    "    # Getter methods\n",
    "    def get_book_name(self):\n",
    "        return self.__book_name\n",
    "\n",
    "    def get_author(self):\n",
    "        return self.__author\n",
    "\n",
    "    def is_available(self):\n",
    "        return self.__is_available\n",
    "\n",
    "    # Method to borrow the book\n",
    "    def borrow_book(self):\n",
    "        if self.__is_available:\n",
    "            self.__is_available = False\n",
    "            print(f\"The book '{self.__book_name}' has been borrowed.\")\n",
    "        else:\n",
    "            print(f\"Sorry, the book '{self.__book_name}' is currently unavailable.\")\n",
    "\n",
    "    # Method to return the book\n",
    "    def return_book(self):\n",
    "        if not self.__is_available:\n",
    "            self.__is_available = True\n",
    "            print(f\"The book '{self.__book_name}' has been returned.\")\n",
    "        else:\n",
    "            print(f\"The book '{self.__book_name}' was not borrowed.\")\n",
    "\n",
    "    # Method to display book information\n",
    "    def display_info(self):\n",
    "        availability_status = \"Available\" if self.__is_available else \"Not Available\"\n",
    "        print(f\"Book Name: {self.__book_name}\")\n",
    "        print(f\"Author: {self.__author}\")\n",
    "        print(f\"Availability: {availability_status}\")\n",
    "\n",
    "# Test the LibraryBook class with sample data\n",
    "book1 = LibraryBook(\"1984\", \"George Orwell\")\n",
    "book2 = LibraryBook(\"To Kill a Mockingbird\", \"Harper Lee\")\n",
    "\n",
    "# Display initial book information\n",
    "print(\"Initial Information:\")\n",
    "book1.display_info()\n",
    "book2.display_info()\n",
    "\n",
    "# Borrow and return books\n",
    "print(\"\\nBorrowing and Returning Books:\")\n",
    "book1.borrow_book()\n",
    "book1.display_info()\n",
    "book1.return_book()\n",
    "book1.display_info()\n",
    "\n",
    "book2.borrow_book()\n",
    "book2.display_info()\n",
    "book2.borrow_book()  # Attempt to borrow again\n",
    "book2.return_book()\n",
    "book2.display_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8abcfc7f-dc78-4f3a-9604-28732c2c6ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Account Information:\n",
      "Account Number: S123\n",
      "Account Holder: Alice\n",
      "Balance: 0.00\n",
      "Account Number: C456\n",
      "Account Holder: Bob\n",
      "Balance: 0.00\n",
      "\n",
      "Performing Transactions:\n",
      "Deposited 1000.00. New balance is 1000.00\n",
      "Deposited 25.00. New balance is 1025.00\n",
      "Interest of 25.00 applied. New balance is 1025.00\n",
      "Withdrew 200.00. New balance is 825.00\n",
      "Account Number: S123\n",
      "Account Holder: Alice\n",
      "Balance: 825.00\n",
      "Deposited 500.00. New balance is 500.00\n",
      "Withdrew 800.00. New balance is -300.00\n",
      "Invalid withdrawal amount or insufficient funds\n",
      "Account Number: C456\n",
      "Account Holder: Bob\n",
      "Balance: -300.00\n"
     ]
    }
   ],
   "source": [
    "class BankAccount:\n",
    "    def __init__(self, account_number, account_holder):  # Corrected constructor method\n",
    "        self.__account_number = account_number\n",
    "        self.__account_holder = account_holder\n",
    "        self.__balance = 0.0\n",
    "\n",
    "    # Getter methods\n",
    "    def get_account_number(self):\n",
    "        return self.__account_number\n",
    "\n",
    "    def get_account_holder(self):\n",
    "        return self.__account_holder\n",
    "\n",
    "    def get_balance(self):\n",
    "        return self.__balance\n",
    "\n",
    "    # Method to deposit money\n",
    "    def deposit(self, amount):\n",
    "        if amount > 0:\n",
    "            self.__balance += amount\n",
    "            print(f\"Deposited {amount:.2f}. New balance is {self.__balance:.2f}\")\n",
    "        else:\n",
    "            print(\"Invalid deposit amount\")\n",
    "\n",
    "    # Method to withdraw money\n",
    "    def withdraw(self, amount):\n",
    "        if 0 < amount <= self.__balance:\n",
    "            self.__balance -= amount\n",
    "            print(f\"Withdrew {amount:.2f}. New balance is {self.__balance:.2f}\")\n",
    "        else:\n",
    "            print(\"Invalid withdrawal amount or insufficient funds\")\n",
    "\n",
    "    # Method to display account information\n",
    "    def display_info(self):\n",
    "        print(f\"Account Number: {self.__account_number}\")\n",
    "        print(f\"Account Holder: {self.__account_holder}\")\n",
    "        print(f\"Balance: {self.__balance:.2f}\")\n",
    "\n",
    "class SavingsAccount(BankAccount):\n",
    "    def __init__(self, account_number, account_holder, interest_rate):  # Corrected constructor method\n",
    "        super().__init__(account_number, account_holder)\n",
    "        self.__interest_rate = interest_rate\n",
    "\n",
    "    def get_interest_rate(self):\n",
    "        return self.__interest_rate\n",
    "\n",
    "    def apply_interest(self):\n",
    "        interest = self.get_balance() * self.__interest_rate / 100\n",
    "        self.deposit(interest)\n",
    "        print(f\"Interest of {interest:.2f} applied. New balance is {self.get_balance():.2f}\")\n",
    "\n",
    "class CheckingAccount(BankAccount):\n",
    "    def __init__(self, account_number, account_holder, overdraft_limit):  # Corrected constructor method\n",
    "        super().__init__(account_number, account_holder)\n",
    "        self.__overdraft_limit = overdraft_limit\n",
    "\n",
    "    def get_overdraft_limit(self):\n",
    "        return self.__overdraft_limit\n",
    "\n",
    "    def withdraw(self, amount):\n",
    "        if 0 < amount <= self.get_balance() + self.__overdraft_limit:\n",
    "            self._BankAccount__balance -= amount  # Corrected reference to the balance\n",
    "            print(f\"Withdrew {amount:.2f}. New balance is {self.get_balance():.2f}\")\n",
    "        else:\n",
    "            print(\"Invalid withdrawal amount or insufficient funds\")\n",
    "\n",
    "# Test the banking system\n",
    "savings = SavingsAccount(\"S123\", \"Alice\", 2.5)\n",
    "checking = CheckingAccount(\"C456\", \"Bob\", 500.0)\n",
    "\n",
    "# Display initial account information\n",
    "print(\"Initial Account Information:\")\n",
    "savings.display_info()\n",
    "checking.display_info()\n",
    "\n",
    "# Perform transactions\n",
    "print(\"\\nPerforming Transactions:\")\n",
    "savings.deposit(1000)\n",
    "savings.apply_interest()\n",
    "savings.withdraw(200)\n",
    "savings.display_info()\n",
    "\n",
    "checking.deposit(500)\n",
    "checking.withdraw(800)\n",
    "checking.withdraw(300)\n",
    "checking.display_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41cbfdc6-8c7f-4805-8ab1-68d1375a67c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog sound: Woof!\n"
     ]
    }
   ],
   "source": [
    "class Animal:\n",
    "    def make_sound(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class Dog(Animal):\n",
    "    def make_sound(self):\n",
    "        return \"Woof!\"\n",
    "\n",
    "class Cat(Animal):\n",
    "    def make_sound(self):\n",
    "        return \"Meow!\"\n",
    "\n",
    "# Test the program by creating instances of Dog and Cat\n",
    "dog = Dog()\n",
    "cat = Cat()\n",
    "\n",
    "# Calling the make_sound() method and printing the results\n",
    "print(f\"Dog sound: {dog.make_sound()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fee6417a-e908-40fa-bb32-5c9bff1a75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MenuItem:\n",
    "    def __init__(self, name, description, price, category, item_id):\n",
    "        self.__item_id = item_id\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.price = price\n",
    "        self.category = category\n",
    "\n",
    "    def get_item_id(self):\n",
    "        return self.__item_id\n",
    "\n",
    "    def update_info(self, name=None, description=None, price=None, category=None):\n",
    "        if name:\n",
    "            self.name = name\n",
    "        if description:\n",
    "            self.description = description\n",
    "        if price:\n",
    "            self.price = price\n",
    "        if category:\n",
    "            self.category = category\n",
    "\n",
    "class FoodItem(MenuItem):\n",
    "    def __init__(self, name, description, price, category, item_id, is_vegetarian):\n",
    "        super().__init__(name, description, price, category, item_id)\n",
    "        self.is_vegetarian = is_vegetarian\n",
    "\n",
    "class BeverageItem(MenuItem):\n",
    "    def __init__(self, name, description, price, category, item_id, is_alcoholic):\n",
    "        super().__init__(name, description, price, category, item_id)\n",
    "        self.is_alcoholic = is_alcoholic\n",
    "\n",
    "class Restaurant:\n",
    "    def __init__(self):\n",
    "        self.menu = []\n",
    "\n",
    "    def add_menu_item(self, item):\n",
    "        self.menu.append(item)\n",
    "\n",
    "    def remove_menu_item(self, item_id):\n",
    "        self.menu = [item for item in self.menu if item.get_item_id() != item_id]\n",
    "\n",
    "    def update_menu_item(self, item_id, name=None, description=None, price=None, category=None):\n",
    "        for item in self.menu:\n",
    "            if item.get_item_id() == item_id:\n",
    "                item.update_info(name, description, price, category)\n",
    "\n",
    "# Example usage\n",
    "restaurant = Restaurant()\n",
    "food = FoodItem(\"Burger\", \"Beef burger\", 5.99, \"Main Course\", 1, False)\n",
    "beverage = BeverageItem(\"Coke\", \"Coca-Cola\", 1.99, \"Drink\", 2, False)\n",
    "restaurant.add_menu_item(food)\n",
    "restaurant.add_menu_item(beverage)\n",
    "restaurant.update_menu_item(1, price=6.49)\n",
    "restaurant.remove_menu_item(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e08b9ab-e46f-4f81-b7c1-79d29db8f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Room:\n",
    "    def __init__(self, room_number, room_type, rate, room_id):\n",
    "        self.__room_id = room_id\n",
    "        self.room_number = room_number\n",
    "        self.room_type = room_type\n",
    "        self.rate = rate\n",
    "        self.__availability = True\n",
    "\n",
    "    def get_room_id(self):\n",
    "        return self.__room_id\n",
    "\n",
    "    def book_room(self):\n",
    "        if self.__availability:\n",
    "            self.__availability = False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_in(self):\n",
    "        if not self.__availability:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_out(self):\n",
    "        self.__availability = True\n",
    "\n",
    "class SuiteRoom(Room):\n",
    "    def __init__(self, room_number, rate, room_id, has_jacuzzi):\n",
    "        super().__init__(room_number, \"Suite\", rate, room_id)\n",
    "        self.has_jacuzzi = has_jacuzzi\n",
    "\n",
    "class StandardRoom(Room):\n",
    "    def __init__(self, room_number, rate, room_id):\n",
    "        super().__init__(room_number, \"Standard\", rate, room_id)\n",
    "\n",
    "class Hotel:\n",
    "    def __init__(self):\n",
    "        self.rooms = []\n",
    "\n",
    "    def add_room(self, room):\n",
    "        self.rooms.append(room)\n",
    "\n",
    "    def book_room(self, room_id):\n",
    "        for room in self.rooms:\n",
    "            if room.get_room_id() == room_id:\n",
    "                return room.book_room()\n",
    "\n",
    "    def check_in(self, room_id):\n",
    "        for room in self.rooms:\n",
    "            if room.get_room_id() == room_id:\n",
    "                return room.check_in()\n",
    "\n",
    "    def check_out(self, room_id):\n",
    "        for room in self.rooms:\n",
    "            if room.get_room_id() == room_id:\n",
    "                room.check_out()\n",
    "\n",
    "# Example usage\n",
    "hotel = Hotel()\n",
    "suite = SuiteRoom(101, 200, 1, True)\n",
    "standard = StandardRoom(102, 100, 2)\n",
    "hotel.add_room(suite)\n",
    "hotel.add_room(standard)\n",
    "hotel.book_room(1)\n",
    "hotel.check_in(1)\n",
    "hotel.check_out(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0362f721-e2e1-4184-8f74-69f2045631b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member:\n",
    "    def __init__(self, name, age, membership_type, member_id):\n",
    "        self.__member_id = member_id\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.membership_type = membership_type\n",
    "        self.__membership_status = True\n",
    "\n",
    "    def get_member_id(self):\n",
    "        return self.__member_id\n",
    "\n",
    "    def register_member(self):\n",
    "        self.__membership_status = True\n",
    "\n",
    "    def renew_membership(self):\n",
    "        self.__membership_status = True\n",
    "\n",
    "    def cancel_membership(self):\n",
    "        self.__membership_status = False\n",
    "\n",
    "class FamilyMember(Member):\n",
    "    def __init__(self, name, age, membership_type, member_id, family_members):\n",
    "        super().__init__(name, age, membership_type, member_id)\n",
    "        self.family_members = family_members\n",
    "\n",
    "class IndividualMember(Member):\n",
    "    def __init__(self, name, age, membership_type, member_id):\n",
    "        super().__init__(name, age, membership_type, member_id)\n",
    "\n",
    "class FitnessClub:\n",
    "    def __init__(self):\n",
    "        self.members = []\n",
    "\n",
    "    def add_member(self, member):\n",
    "        self.members.append(member)\n",
    "\n",
    "    def renew_membership(self, member_id):\n",
    "        for member in self.members:\n",
    "            if member.get_member_id() == member_id:\n",
    "                member.renew_membership()\n",
    "\n",
    "    def cancel_membership(self, member_id):\n",
    "        for member in self.members:\n",
    "            if member.get_member_id() == member_id:\n",
    "                member.cancel_membership()\n",
    "\n",
    "# Example usage\n",
    "club = FitnessClub()\n",
    "family_member = FamilyMember(\"Alice\", 40, \"Family\", 1, [\"Bob\", \"Charlie\"])\n",
    "individual_member = IndividualMember(\"Dave\", 30, \"Individual\", 2)\n",
    "club.add_member(family_member)\n",
    "club.add_member(individual_member)\n",
    "club.renew_membership(1)\n",
    "club.cancel_membership(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b87ad32-fd3e-402c-a1f7-cad73aa73ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attendees: 1\n"
     ]
    }
   ],
   "source": [
    "class Event:\n",
    "    def __init__(self, name, date, time, location, event_id):\n",
    "        self.__event_id = event_id\n",
    "        self.name = name\n",
    "        self.date = date\n",
    "        self.time = time\n",
    "        self.location = location\n",
    "        self.__attendees = []\n",
    "\n",
    "    def get_event_id(self):\n",
    "        return self.__event_id\n",
    "\n",
    "    def add_attendee(self, attendee):\n",
    "        self.__attendees.append(attendee)\n",
    "\n",
    "    def remove_attendee(self, attendee):\n",
    "        if attendee in self.__attendees:\n",
    "            self.__attendees.remove(attendee)\n",
    "\n",
    "    def get_total_attendees(self):\n",
    "        return len(self.__attendees)\n",
    "\n",
    "class PrivateEvent(Event):\n",
    "    def __init__(self, name, date, time, location, event_id, invited_guests):\n",
    "        super().__init__(name, date, time, location, event_id)\n",
    "        self.invited_guests = invited_guests\n",
    "\n",
    "class PublicEvent(Event):\n",
    "    def __init__(self, name, date, time, location, event_id, publicity_channel):\n",
    "        super().__init__(name, date, time, location, event_id)\n",
    "        self.publicity_channel = publicity_channel\n",
    "\n",
    "# Example usage\n",
    "event = PublicEvent(\"Concert\", \"2024-12-31\", \"20:00\", \"Stadium\", 1, \"Social Media\")\n",
    "event.add_attendee(\"Alice\")\n",
    "event.add_attendee(\"Bob\")\n",
    "event.remove_attendee(\"Bob\")\n",
    "print(f\"Total attendees: {event.get_total_attendees()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb7f7888-881b-4be4-a9a3-494514bc29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining seats: 99\n"
     ]
    }
   ],
   "source": [
    "class Flight:\n",
    "    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, flight_id):\n",
    "        self.__flight_id = flight_id\n",
    "        self.flight_number = flight_number\n",
    "        self.departure_airport = departure_airport\n",
    "        self.arrival_airport = arrival_airport\n",
    "        self.departure_time = departure_time\n",
    "        self.arrival_time = arrival_time\n",
    "        self.__available_seats = 100\n",
    "\n",
    "    def get_flight_id(self):\n",
    "        return self.__flight_id\n",
    "\n",
    "    def book_seat(self):\n",
    "        if self.__available_seats > 0:\n",
    "            self.__available_seats -= 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def cancel_reservation(self):\n",
    "        self.__available_seats += 1\n",
    "\n",
    "    def get_remaining_seats(self):\n",
    "        return self.__available_seats\n",
    "\n",
    "class DomesticFlight(Flight):\n",
    "    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, flight_id, domestic_service):\n",
    "        super().__init__(flight_number, departure_airport, arrival_airport, departure_time, arrival_time, flight_id)\n",
    "        self.domestic_service = domestic_service\n",
    "\n",
    "class InternationalFlight(Flight):\n",
    "    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, flight_id, international_service):\n",
    "        super().__init__(flight_number, departure_airport, arrival_airport, departure_time, arrival_time, flight_id)\n",
    "        self.international_service = international_service\n",
    "\n",
    "# Example usage\n",
    "flight = InternationalFlight(\"AI202\", \"JFK\", \"LHR\", \"10:00\", \"20:00\", 1, \"Premium Service\")\n",
    "flight.book_seat()\n",
    "flight.book_seat()\n",
    "flight.cancel_reservation()\n",
    "print(f\"Remaining seats: {flight.get_remaining_seats()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04470027-9efe-41b3-b9e5-e8c53a7ee482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants.py\n",
    "\n",
    "PI = 3.141592653589793\n",
    "SPEED_OF_LIGHT = 299792458  # in meters per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d262dfc-2172-4d1c-a7a0-4353e839a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculator.py\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "def divide(a, b):\n",
    "    if b != 0:\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(\"Cannot divide by zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9504b5c5-048a-49a0-9846-8c76ac067c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecommerce/product_management.py\n",
    "\n",
    "class Product:\n",
    "    def _init_(self, product_id, name, price):\n",
    "        self.product_id = product_id\n",
    "        self.name = name\n",
    "        self.price = price\n",
    "\n",
    "    def _str_(self):\n",
    "        return f\"Product({self.product_id}, {self.name}, {self.price})\"\n",
    "\n",
    "    def apply_discount(self, discount):\n",
    "        self.price -= self.price * discount\n",
    "\n",
    "def add_product(products, product):\n",
    "    products.append(product)\n",
    "\n",
    "def remove_product(products, product_id):\n",
    "    products[:] = [p for p in products if p.product_id != product_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85a3c5ef-0ca4-4d38-b954-9966f4b8f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecommerce/order_processing.py\n",
    "\n",
    "class Order:\n",
    "    def _init_(self, order_id, products):\n",
    "        self.order_id = order_id\n",
    "        self.products = products\n",
    "        self.status = \"Pending\"\n",
    "\n",
    "    def _str_(self):\n",
    "        return f\"Order({self.order_id}, {self.products}, {self.status})\"\n",
    "\n",
    "    def calculate_total(self):\n",
    "        return sum(product.price for product in self.products)\n",
    "\n",
    "    def process_order(self):\n",
    "        self.status = \"Processed\"\n",
    "\n",
    "def create_order(order_id, products):\n",
    "    return Order(order_id, products)\n",
    "\n",
    "def cancel_order(order):\n",
    "    order.status = \"Cancelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c983f0f-8804-4144-a66a-ee63e5bb0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_utils.py\n",
    "\n",
    "def reverse_string(s):\n",
    "    return s[::-1]\n",
    "\n",
    "def capitalize_string(s):\n",
    "    return s.capitalize()\n",
    "\n",
    "def uppercase_string(s):\n",
    "    return s.upper()\n",
    "\n",
    "def lowercase_string(s):\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bd3050c-14a4-41cf-a41a-0bcb2faff487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_operations.py\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def write_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(data)\n",
    "\n",
    "def append_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "740f425c-c97c-4fac-a877-15a0d43a1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and write to 'employees.txt'\n",
    "with open('employees.txt', 'w') as file:\n",
    "    # Example employee details\n",
    "    employees = [\n",
    "        {\"name\": \"John Doe\", \"age\": 30, \"salary\": 50000},\n",
    "        {\"name\": \"Jane Smith\", \"age\": 25, \"salary\": 55000},\n",
    "        {\"name\": \"Mike Johnson\", \"age\": 45, \"salary\": 60000}\n",
    "    ]\n",
    "    \n",
    "    for emp in employees:\n",
    "        file.write(f\"Name: {emp['name']}, Age: {emp['age']}, Salary: {emp['salary']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a9a52ee-c59b-4c66-a4b5-536ea9bbe52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'inventory.txt' was not found.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('inventory.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            print(line.strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'inventory.txt' was not found.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: You do not have permission to read the file 'inventory.txt'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "edfbb8ec-30c3-4fdb-9efe-da9efd9a44d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'expenses.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read 'expenses.txt' and calculate total amount spent\u001b[39;00m\n\u001b[0;32m      2\u001b[0m total_expenses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpenses.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;66;03m# Assume each line in 'expenses.txt' is a number\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'expenses.txt'"
     ]
    }
   ],
   "source": [
    "# Read 'expenses.txt' and calculate total amount spent\n",
    "total_expenses = 0\n",
    "\n",
    "with open('expenses.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Assume each line in 'expenses.txt' is a number\n",
    "            total_expenses += float(line.strip())\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not convert line to float: {line.strip()}\")\n",
    "\n",
    "print(f\"Total Expenses: ${total_expenses:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac3502d9-b494-46e7-aa83-2a58ae05847d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'paragraph.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read 'paragraph.txt' and count word occurrences\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparagraph.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Read the entire file and convert to lowercase\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use regex to find all words (considering word boundaries)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'paragraph.txt'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Read 'paragraph.txt' and count word occurrences\n",
    "with open('paragraph.txt', 'r') as file:\n",
    "    text = file.read().lower()  # Read the entire file and convert to lowercase\n",
    "\n",
    "# Use regex to find all words (considering word boundaries)\n",
    "words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Count occurrences\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Display results in alphabetical order\n",
    "for word in sorted(word_count):\n",
    "    print(f\"{word}: {word_count[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43b3f64b-8f5a-4c47-984c-2e8a45dee223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\1476729124.py:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"\"\"Here's the corrected version of the text, with minor formatting adjustments for clarity:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here's the corrected version of the text, with minor formatting adjustments for clarity:\\n\\n### Measures of Central Tendency and Measures of Dispersion\\n\\n**Measures of Central Tendency**  \\nThese measures provide a central value around which the data points in a dataset are distributed. They help in understanding the typical value of a dataset. Common measures include:\\n\\n**Mean:**\\n- **Definition:** The average of all the values in a dataset.\\n- **Calculation:** Sum of all values divided by the number of values.\\n\\\\[ \\text{Mean} = \\x0crac{\\\\sum x_i}{N} \\\\]\\nwhere \\\\( x_i \\\\) represents each individual value and \\\\( N \\\\) is the number of values.\\n\\n**Median:**\\n- **Definition:** The middle value when the dataset is ordered from smallest to largest. If the number of values is even, it's the average of the two middle values.\\n- **Calculation:**\\n  1. Order the data.\\n  2. If \\\\( N \\\\) is odd, the median is the middle value.\\n  3. If \\\\( N \\\\) is even, the median is the average of the two middle values.\\n\\n**Mode:**\\n- **Definition:** The value that occurs most frequently in the dataset.\\n- **Calculation:** Identify the value(s) with the highest frequency. A dataset can have one mode, more than one mode, or no mode at all.\\n\\n**Measures of Dispersion**  \\nThese measures describe the spread or variability within a dataset, indicating how much the data points differ from the central value. Common measures include:\\n\\n**Range:**\\n- **Definition:** The difference between the maximum and minimum values in the dataset.\\n- **Calculation:** \\\\[ \\text{Range} = \\text{Max} - \\text{Min} \\\\]\\n\\n**Variance:**\\n- **Definition:** The average of the squared differences between each data point and the mean. It quantifies the degree of spread in the data.\\n- **Calculation:** \\\\[ \\text{Variance} = \\x0crac{\\\\sum (x_i - \\text{Mean})^2}{N} \\\\]\\nwhere \\\\( x_i \\\\) represents each value, Mean is the average, and \\\\( N \\\\) is the number of values.\\n\\n**Standard Deviation:**\\n- **Definition:** The square root of the variance, representing the average distance of each data point from the mean.\\n- **Calculation:** \\\\[ \\text{Standard Deviation} = \\\\sqrt{\\text{Variance}} \\\\]\\n\\n**Example:**  \\nConsider the dataset: [4, 8, 6, 5, 9].\\n\\n**Mean:**\\n\\\\[ \\text{Mean} = \\x0crac{4 + 8 + 6 + 5 + 9}{5} = \\x0crac{32}{5} = 6.4 \\\\]\\n\\n**Median:**  \\nOrder the data [4, 5, 6, 8, 9]. The median is 6.\\n\\n**Mode:**  \\nThere is no mode since all values occur only once.\\n\\n**Range:**\\n\\\\[ \\text{Range} = 9 - 4 = 5 \\\\]\\n\\n**Variance:**\\n\\\\[ \\text{Variance} = \\x0crac{(4 - 6.4)^2 + (8 - 6.4)^2 + (6 - 6.4)^2 + (5 - 6.4)^2 + (9 - 6.4)^2}{5} \\\\]\\n\\\\[ = \\x0crac{5.76 + 2.56 + 0.16 + 1.96 + 6.76}{5} = \\x0crac{17.2}{5} = 3.44 \\\\]\\n\\n**Standard Deviation:**\\n\\\\[ \\text{Standard Deviation} = \\\\sqrt{3.44} \\x07pprox 1.85 \\\\]\\n\\nThese calculations provide a comprehensive view of the data's distribution and help in making informed decisions.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Here's the corrected version of the text, with minor formatting adjustments for clarity:\n",
    "\n",
    "### Measures of Central Tendency and Measures of Dispersion\n",
    "\n",
    "**Measures of Central Tendency**  \n",
    "These measures provide a central value around which the data points in a dataset are distributed. They help in understanding the typical value of a dataset. Common measures include:\n",
    "\n",
    "**Mean:**\n",
    "- **Definition:** The average of all the values in a dataset.\n",
    "- **Calculation:** Sum of all values divided by the number of values.\n",
    "\\[ \\text{Mean} = \\frac{\\sum x_i}{N} \\]\n",
    "where \\( x_i \\) represents each individual value and \\( N \\) is the number of values.\n",
    "\n",
    "**Median:**\n",
    "- **Definition:** The middle value when the dataset is ordered from smallest to largest. If the number of values is even, it's the average of the two middle values.\n",
    "- **Calculation:**\n",
    "  1. Order the data.\n",
    "  2. If \\( N \\) is odd, the median is the middle value.\n",
    "  3. If \\( N \\) is even, the median is the average of the two middle values.\n",
    "\n",
    "**Mode:**\n",
    "- **Definition:** The value that occurs most frequently in the dataset.\n",
    "- **Calculation:** Identify the value(s) with the highest frequency. A dataset can have one mode, more than one mode, or no mode at all.\n",
    "\n",
    "**Measures of Dispersion**  \n",
    "These measures describe the spread or variability within a dataset, indicating how much the data points differ from the central value. Common measures include:\n",
    "\n",
    "**Range:**\n",
    "- **Definition:** The difference between the maximum and minimum values in the dataset.\n",
    "- **Calculation:** \\[ \\text{Range} = \\text{Max} - \\text{Min} \\]\n",
    "\n",
    "**Variance:**\n",
    "- **Definition:** The average of the squared differences between each data point and the mean. It quantifies the degree of spread in the data.\n",
    "- **Calculation:** \\[ \\text{Variance} = \\frac{\\sum (x_i - \\text{Mean})^2}{N} \\]\n",
    "where \\( x_i \\) represents each value, Mean is the average, and \\( N \\) is the number of values.\n",
    "\n",
    "**Standard Deviation:**\n",
    "- **Definition:** The square root of the variance, representing the average distance of each data point from the mean.\n",
    "- **Calculation:** \\[ \\text{Standard Deviation} = \\sqrt{\\text{Variance}} \\]\n",
    "\n",
    "**Example:**  \n",
    "Consider the dataset: [4, 8, 6, 5, 9].\n",
    "\n",
    "**Mean:**\n",
    "\\[ \\text{Mean} = \\frac{4 + 8 + 6 + 5 + 9}{5} = \\frac{32}{5} = 6.4 \\]\n",
    "\n",
    "**Median:**  \n",
    "Order the data [4, 5, 6, 8, 9]. The median is 6.\n",
    "\n",
    "**Mode:**  \n",
    "There is no mode since all values occur only once.\n",
    "\n",
    "**Range:**\n",
    "\\[ \\text{Range} = 9 - 4 = 5 \\]\n",
    "\n",
    "**Variance:**\n",
    "\\[ \\text{Variance} = \\frac{(4 - 6.4)^2 + (8 - 6.4)^2 + (6 - 6.4)^2 + (5 - 6.4)^2 + (9 - 6.4)^2}{5} \\]\n",
    "\\[ = \\frac{5.76 + 2.56 + 0.16 + 1.96 + 6.76}{5} = \\frac{17.2}{5} = 3.44 \\]\n",
    "\n",
    "**Standard Deviation:**\n",
    "\\[ \\text{Standard Deviation} = \\sqrt{3.44} \\approx 1.85 \\]\n",
    "\n",
    "These calculations provide a comprehensive view of the data's distribution and help in making informed decisions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2540c7cc-133a-496f-be4f-b80d4067051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"types of Skewness\\nPositive Skewness (Right Skewness):\\n\\nDescription: The right tail (larger values) of the distribution is longer or fatter than the left tail (smaller values). The bulk of the data is concentrated on the left side, and the mean is greater than the median.\\nGraph: The distribution is skewed to the right, with the peak of the distribution leaning towards the left.\\nNegative Skewness (Left Skewness):\\n\\nDescription: The left tail (smaller values) of the distribution is longer or fatter than the right tail (larger values). The bulk of the data is concentrated on the right side, and the mean is less than the median.\\nGraph: The distribution is skewed to the left, with the peak of the distribution leaning towards the right.\\nZero Skewness:\\n\\nDescription: The distribution is perfectly symmetrical around its mean, similar to the normal distribution. The mean and median are equal.\\nGraph: The distribution is bell-shaped and symmetrical.import matplotlib.pyplot as plt\\nimport numpy as np\\nimport seaborn as sns\\n\\n# Generating data\\nnp.random.seed(0)\\ndata_positive_skew = np.random.exponential(scale=2, size=1000)\\ndata_negative_skew = -np.random.exponential(scale=2, size=1000)\\ndata_normal = np.random.normal(loc=0, scale=1, size=1000)\\n\\n# Plotting\\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\\n\\n# Positive Skew\\nsns.histplot(data_positive_skew, kde=True, ax=axs[0])\\naxs[0].set_title('Positive Skewness (Right Skew)')\\n\\n# Negative Skew\\nsns.histplot(data_negative_skew, kde=True, ax=axs[1])\\naxs[1].set_title('Negative Skewness (Left Skew)')\\n\\n# Zero Skewness\\nsns.histplot(data_normal, kde=True, ax=axs[2])\\naxs[2].set_title('Zero Skewness (Symmetric)')\\n\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"types of Skewness\n",
    "Positive Skewness (Right Skewness):\n",
    "\n",
    "Description: The right tail (larger values) of the distribution is longer or fatter than the left tail (smaller values). The bulk of the data is concentrated on the left side, and the mean is greater than the median.\n",
    "Graph: The distribution is skewed to the right, with the peak of the distribution leaning towards the left.\n",
    "Negative Skewness (Left Skewness):\n",
    "\n",
    "Description: The left tail (smaller values) of the distribution is longer or fatter than the right tail (larger values). The bulk of the data is concentrated on the right side, and the mean is less than the median.\n",
    "Graph: The distribution is skewed to the left, with the peak of the distribution leaning towards the right.\n",
    "Zero Skewness:\n",
    "\n",
    "Description: The distribution is perfectly symmetrical around its mean, similar to the normal distribution. The mean and median are equal.\n",
    "Graph: The distribution is bell-shaped and symmetrical.import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Generating data\n",
    "np.random.seed(0)\n",
    "data_positive_skew = np.random.exponential(scale=2, size=1000)\n",
    "data_negative_skew = -np.random.exponential(scale=2, size=1000)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Positive Skew\n",
    "sns.histplot(data_positive_skew, kde=True, ax=axs[0])\n",
    "axs[0].set_title('Positive Skewness (Right Skew)')\n",
    "\n",
    "# Negative Skew\n",
    "sns.histplot(data_negative_skew, kde=True, ax=axs[1])\n",
    "axs[1].set_title('Negative Skewness (Left Skew)')\n",
    "\n",
    "# Zero Skewness\n",
    "sns.histplot(data_normal, kde=True, ax=axs[2])\n",
    "axs[2].set_title('Zero Skewness (Symmetric)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d525e39-156d-4288-adc7-5c0f10cf49f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probability Mass Function (PMF) and Probability Density Function (PDF) are fundamental concepts in probability theory and statistics that describe the distribution of random variables. Here\\'s an explanation of each and their differences:\\n\\nProbability Mass Function (PMF)\\nDefinition: The PMF is used for discrete random variables. It provides the probability that a discrete random variable is exactly equal to a specific value.\\nNotation: If \\n𝑋\\nX is a discrete random variable, the PMF is denoted as \\n𝑃\\n(\\n𝑋\\n=\\n𝑥\\n)\\nP(X=x) or \\n𝑝\\n(\\n𝑥\\n)\\np(x).\\nProperties:\\nThe sum of the probabilities for all possible values of the random variable must equal 1.\\n∑\\n𝑥\\n𝑝\\n(\\n𝑥\\n)\\n=\\n1\\nx\\n∑\\n\\u200b\\n p(x)=1\\nEach value of \\n𝑝\\n(\\n𝑥\\n)\\np(x) is between 0 and 1.\\nExample: Consider rolling a fair six-sided die. The…\\n[22:48, 27/07/2024] Atharv Cr: 22\\n[22:50, 27/07/2024] Atharv Cr: from flask import Flask\\n\\napp = Flask(_name_)\\n\\n@app.route(\\'/\\')\\ndef home():\\n    return \"Hello, World!\"\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)\\nExplanation:\\nFlask(_name_): Creates a new Flask application instance.\\n@app.route(\\'/\\'): Defines a route for the homepage.\\nhome(): The view function that returns \"Hello, World!\".\\napp.run(debug=True): Starts the Flask development server in debug mode.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Probability Mass Function (PMF) and Probability Density Function (PDF) are fundamental concepts in probability theory and statistics that describe the distribution of random variables. Here's an explanation of each and their differences:\n",
    "\n",
    "Probability Mass Function (PMF)\n",
    "Definition: The PMF is used for discrete random variables. It provides the probability that a discrete random variable is exactly equal to a specific value.\n",
    "Notation: If \n",
    "𝑋\n",
    "X is a discrete random variable, the PMF is denoted as \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "=\n",
    "𝑥\n",
    ")\n",
    "P(X=x) or \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "p(x).\n",
    "Properties:\n",
    "The sum of the probabilities for all possible values of the random variable must equal 1.\n",
    "∑\n",
    "𝑥\n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "1\n",
    "x\n",
    "∑\n",
    "​\n",
    " p(x)=1\n",
    "Each value of \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "p(x) is between 0 and 1.\n",
    "Example: Consider rolling a fair six-sided die. The…\n",
    "[22:48, 27/07/2024] Atharv Cr: 22\n",
    "[22:50, 27/07/2024] Atharv Cr: from flask import Flask\n",
    "\n",
    "app = Flask(_name_)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\n",
    "Explanation:\n",
    "Flask(_name_): Creates a new Flask application instance.\n",
    "@app.route('/'): Defines a route for the homepage.\n",
    "home(): The view function that returns \"Hello, World!\".\n",
    "app.run(debug=True): Starts the Flask development server in debug mode.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ce6da877-0983-4212-9ba5-e0e3f74a6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\393537223.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCorrelation is a statistical measure that describes the degree and direction of the relationship between two or more variables. It indicates how changes in one variable are associated with changes in another. Correlation is widely used in various fields, such as economics, finance, psychology, and social sciences, to determine relationships and predict outcomes.\\nTypes of Correlation\\n1.\\tPositive Correlation:\\no\\tWhen both variables move in the same direction. As one variable increases, the other variable also increases, and vice versa.\\no\\tExample: Height and weight of individuals. Generally, as height increases, weight also increases.\\n2.\\tNegative Correlation:\\no\\tWhen the variables move in opposite directions. As one variable increases, the other variable decreases, and vice versa.\\no\\tExample: The amount of rainfall and the need for irrigation. As rainfall increases, the need for irrigation decreases.\\n3.\\tNo Correlation:\\no\\tWhen there is no discernible pattern or relationship between the variables. Changes in one variable do not predict changes in the other.\\no\\tExample: The number of books read and the amount of ice cream consumed. There is no significant relationship between these variables.\\nMethods of Determining Correlation\\n1.\\tPearson’s Correlation Coefficient (r):\\no\\tMeasures the linear relationship between two continuous variables.\\no\\tValues range from -1 to 1.\\n\\uf0a7\\t+1 indicates a perfect positive correlation.\\n\\uf0a7\\t-1 indicates a perfect negative correlation.\\n\\uf0a7\\t0 indicates no correlation.\\no\\tFormula: r=n(∑xy)−(∑x)(∑y)[n∑x2−(∑x)2][n∑y2−(∑y)2]r = \\x0crac{n(\\\\sum xy) - (\\\\sum x)(\\\\sum y)}{\\\\sqrt{[n\\\\sum x^2 - (\\\\sum x)^2][n\\\\sum y^2 - (\\\\sum y)^2]}}r=[n∑x2−(∑x)2][n∑y2−(∑y)2]n(∑xy)−(∑x)(∑y)\\no\\tSuitable for normally distributed data.\\n2.\\tSpearman’s Rank Correlation Coefficient (ρ or rs):\\no\\tMeasures the strength and direction of association between two ranked variables.\\no\\tNon-parametric test and does not assume a normal distribution.\\no\\tValues range from -1 to 1, similar to Pearson’s coefficient.\\no\\tSuitable for ordinal data or when the assumptions of Pearson’s correlation are not met.\\no\\tFormula: rs=1−6∑di2n(n2−1)r_s = 1 - \\x0crac{6\\\\sum d_i^2}{n(n^2 - 1)}rs=1−n(n2−1)6∑di2 where did_idi is the difference between the ranks of corresponding values of the two variables.\\n3.\\tKendall’s Tau (τ):\\no\\tMeasures the association between two ranked variables.\\no\\tNon-parametric and less sensitive to errors in data compared to Spearman’s.\\no\\tSuitable for small sample sizes and ordinal data.\\no\\tFormula: τ=(C−D)12n(n−1)\\tau = \\x0crac{(C - D)}{\\x0crac{1}{2}n(n-1)}τ=21n(n−1)(C−D) where CCC is the number of concordant pairs and DDD is the number of discordant pairs.\\n4.\\tPoint-Biserial Correlation:\\no\\tMeasures the relationship between a binary variable and a continuous variable.\\no\\tSpecial case of Pearson’s correlation coefficient.\\no\\tSuitable when one variable is dichotomous and the other is continuous.\\no\\tFormula: rpb=M1−M0sn1n0n2r_{pb} = \\x0crac{M_1 - M_0}{s} \\\\sqrt{\\x0crac{n_1 n_0}{n^2}}rpb=sM1−M0n2n1n0 where M1M_1M1 and M0M_0M0 are the means of the continuous variable for the binary categories, sss is the standard deviation of the continuous variable, and n1n_1n1 and n0n_0n0 are the sample sizes of the binary categories.\\n5.\\tPhi Coefficient (φ):\\no\\tMeasures the association between two binary variables.\\no\\tSpecial case of Pearson’s correlation for binary data.\\no\\tValues range from -1 to 1.\\no\\tFormula: ϕ=AD−BC(A+B)(C+D)(A+C)(B+D)\\\\phi = \\x0crac{AD - BC}{\\\\sqrt{(A+B)(C+D)(A+C)(B+D)}}ϕ=(A+B)(C+D)(A+C)(B+D)AD−BC where A,B,C,DA, B, C, DA,B,C,D are the frequencies in a 2x2 contingency table.\\nThese methods provide different ways to measure and interpret the relationship between variables, depending on the nature of the data and the assumptions that can be made about their distribution and scale.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Correlation is a statistical measure that describes the degree and direction of the relationship between two or more variables. It indicates how changes in one variable are associated with changes in another. Correlation is widely used in various fields, such as economics, finance, psychology, and social sciences, to determine relationships and predict outcomes.\n",
    "Types of Correlation\n",
    "1.\tPositive Correlation:\n",
    "o\tWhen both variables move in the same direction. As one variable increases, the other variable also increases, and vice versa.\n",
    "o\tExample: Height and weight of individuals. Generally, as height increases, weight also increases.\n",
    "2.\tNegative Correlation:\n",
    "o\tWhen the variables move in opposite directions. As one variable increases, the other variable decreases, and vice versa.\n",
    "o\tExample: The amount of rainfall and the need for irrigation. As rainfall increases, the need for irrigation decreases.\n",
    "3.\tNo Correlation:\n",
    "o\tWhen there is no discernible pattern or relationship between the variables. Changes in one variable do not predict changes in the other.\n",
    "o\tExample: The number of books read and the amount of ice cream consumed. There is no significant relationship between these variables.\n",
    "Methods of Determining Correlation\n",
    "1.\tPearson’s Correlation Coefficient (r):\n",
    "o\tMeasures the linear relationship between two continuous variables.\n",
    "o\tValues range from -1 to 1.\n",
    "\t+1 indicates a perfect positive correlation.\n",
    "\t-1 indicates a perfect negative correlation.\n",
    "\t0 indicates no correlation.\n",
    "o\tFormula: r=n(∑xy)−(∑x)(∑y)[n∑x2−(∑x)2][n∑y2−(∑y)2]r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n\\sum x^2 - (\\sum x)^2][n\\sum y^2 - (\\sum y)^2]}}r=[n∑x2−(∑x)2][n∑y2−(∑y)2]n(∑xy)−(∑x)(∑y)\n",
    "o\tSuitable for normally distributed data.\n",
    "2.\tSpearman’s Rank Correlation Coefficient (ρ or rs):\n",
    "o\tMeasures the strength and direction of association between two ranked variables.\n",
    "o\tNon-parametric test and does not assume a normal distribution.\n",
    "o\tValues range from -1 to 1, similar to Pearson’s coefficient.\n",
    "o\tSuitable for ordinal data or when the assumptions of Pearson’s correlation are not met.\n",
    "o\tFormula: rs=1−6∑di2n(n2−1)r_s = 1 - \\frac{6\\sum d_i^2}{n(n^2 - 1)}rs=1−n(n2−1)6∑di2 where did_idi is the difference between the ranks of corresponding values of the two variables.\n",
    "3.\tKendall’s Tau (τ):\n",
    "o\tMeasures the association between two ranked variables.\n",
    "o\tNon-parametric and less sensitive to errors in data compared to Spearman’s.\n",
    "o\tSuitable for small sample sizes and ordinal data.\n",
    "o\tFormula: τ=(C−D)12n(n−1)\\tau = \\frac{(C - D)}{\\frac{1}{2}n(n-1)}τ=21n(n−1)(C−D) where CCC is the number of concordant pairs and DDD is the number of discordant pairs.\n",
    "4.\tPoint-Biserial Correlation:\n",
    "o\tMeasures the relationship between a binary variable and a continuous variable.\n",
    "o\tSpecial case of Pearson’s correlation coefficient.\n",
    "o\tSuitable when one variable is dichotomous and the other is continuous.\n",
    "o\tFormula: rpb=M1−M0sn1n0n2r_{pb} = \\frac{M_1 - M_0}{s} \\sqrt{\\frac{n_1 n_0}{n^2}}rpb=sM1−M0n2n1n0 where M1M_1M1 and M0M_0M0 are the means of the continuous variable for the binary categories, sss is the standard deviation of the continuous variable, and n1n_1n1 and n0n_0n0 are the sample sizes of the binary categories.\n",
    "5.\tPhi Coefficient (φ):\n",
    "o\tMeasures the association between two binary variables.\n",
    "o\tSpecial case of Pearson’s correlation for binary data.\n",
    "o\tValues range from -1 to 1.\n",
    "o\tFormula: ϕ=AD−BC(A+B)(C+D)(A+C)(B+D)\\phi = \\frac{AD - BC}{\\sqrt{(A+B)(C+D)(A+C)(B+D)}}ϕ=(A+B)(C+D)(A+C)(B+D)AD−BC where A,B,C,DA, B, C, DA,B,C,D are the frequencies in a 2x2 contingency table.\n",
    "These methods provide different ways to measure and interpret the relationship between variables, depending on the nature of the data and the assumptions that can be made about their distribution and scale.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb2347be-e30e-45a6-82f5-46fb4cd0ccd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Pearson\\'s correlation coefficient between the marks obtained by the 10 students in Accountancy and Statistics is approximately \\n0.903\\n0.903. This indicates a strong positive correlation between the two sets of marks, meaning that as the marks in Accountancy increase, the marks in Statistics also tend to increase.\\n[12:17, 28/07/2024] Atharv Cr: import pandas as pd\\nimport numpy as np\\n\\n# Data from the image\\ndata = {\\n    \"Student\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\n    \"Accountancy\": [45, 70, 65, 30, 90, 40, 50, 75, 85, 60],\\n    \"Statistics\": [35, 90, 70, 40, 95, 40, 60, 80, 80, 50]\\n}\\n\\n# Creating a DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Calculating Pearson\\'s correlation coefficient\\ncorrelation = df[\"Accountancy\"].corr(df[\"Statistics\"], method=\"pearson\")\\ncorrelation\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The Pearson's correlation coefficient between the marks obtained by the 10 students in Accountancy and Statistics is approximately \n",
    "0.903\n",
    "0.903. This indicates a strong positive correlation between the two sets of marks, meaning that as the marks in Accountancy increase, the marks in Statistics also tend to increase.\n",
    "[12:17, 28/07/2024] Atharv Cr: import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data from the image\n",
    "data = {\n",
    "    \"Student\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Accountancy\": [45, 70, 65, 30, 90, 40, 50, 75, 85, 60],\n",
    "    \"Statistics\": [35, 90, 70, 40, 95, 40, 60, 80, 80, 50]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculating Pearson's correlation coefficient\n",
    "correlation = df[\"Accountancy\"].corr(df[\"Statistics\"], method=\"pearson\")\n",
    "correlation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c02a84d8-c0e2-46ba-95e0-68749fed6989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDifferences Between Correlation and Regression\\nPurpose:\\n\\nCorrelation: Measures the strength and direction of the linear relationship between two variables. It quantifies how much two variables change together, without establishing a cause-and-effect relationship.\\nRegression: Establishes a cause-and-effect relationship between variables. It predicts the value of a dependent variable based on the value of one or more independent variables.\\nDirection of Relationship:\\n\\nCorrelation: Symmetrical. The correlation between \\n𝑋\\nX and \\n𝑌\\nY is the same as between \\n𝑌\\nY and \\n𝑋\\nX.\\nRegression: Asymmetrical. Regression of \\n𝑌\\nY on \\n𝑋\\nX is generally different from the regression of \\n𝑋\\nX on \\n𝑌\\nY. It specifically predicts \\n𝑌\\nY from \\n𝑋\\nX.\\nNature of Analysis:\\n\\nCorrelation: Descriptive. It provides a single summary value (correlation coefficient) that describes the degree and direction of relationship.\\nRegression: Analytical. It provides an equation that models the relationship, which can be used for prediction.\\nRepresentation:\\n\\nCorrelation: Represented by a single value, the correlation coefficient (e.g., Pearson's r).\\nRegression: Represented by a regression equation (e.g., \\n𝑌\\n=\\n𝑎\\n+\\n𝑏\\n𝑋\\nY=a+bX), where \\n𝑎\\na is the intercept and \\n𝑏\\nb is the slope.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Differences Between Correlation and Regression\n",
    "Purpose:\n",
    "\n",
    "Correlation: Measures the strength and direction of the linear relationship between two variables. It quantifies how much two variables change together, without establishing a cause-and-effect relationship.\n",
    "Regression: Establishes a cause-and-effect relationship between variables. It predicts the value of a dependent variable based on the value of one or more independent variables.\n",
    "Direction of Relationship:\n",
    "\n",
    "Correlation: Symmetrical. The correlation between \n",
    "𝑋\n",
    "X and \n",
    "𝑌\n",
    "Y is the same as between \n",
    "𝑌\n",
    "Y and \n",
    "𝑋\n",
    "X.\n",
    "Regression: Asymmetrical. Regression of \n",
    "𝑌\n",
    "Y on \n",
    "𝑋\n",
    "X is generally different from the regression of \n",
    "𝑋\n",
    "X on \n",
    "𝑌\n",
    "Y. It specifically predicts \n",
    "𝑌\n",
    "Y from \n",
    "𝑋\n",
    "X.\n",
    "Nature of Analysis:\n",
    "\n",
    "Correlation: Descriptive. It provides a single summary value (correlation coefficient) that describes the degree and direction of relationship.\n",
    "Regression: Analytical. It provides an equation that models the relationship, which can be used for prediction.\n",
    "Representation:\n",
    "\n",
    "Correlation: Represented by a single value, the correlation coefficient (e.g., Pearson's r).\n",
    "Regression: Represented by a regression equation (e.g., \n",
    "𝑌\n",
    "=\n",
    "𝑎\n",
    "+\n",
    "𝑏\n",
    "𝑋\n",
    "Y=a+bX), where \n",
    "𝑎\n",
    "a is the intercept and \n",
    "𝑏\n",
    "b is the slope.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0705e22d-2e30-4792-91b0-0c417db49ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\3183450126.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFinding the Most Likely Price at Delhi\\nTo find the most likely price at Delhi corresponding to the price of Rs. 70 at Agra using the given correlation coefficient (r=+0.8r = +0.8r=+0.8), we can use the regression equation. However, we need additional information such as the means and standard deviations of the prices at both locations.\\nAssuming we have the means and standard deviations:\\n•\\tMean price at Agra (Xˉ\\x08ar{X}Xˉ) = μX\\\\mu_XμX\\n•\\tMean price at Delhi (Yˉ\\x08ar{Y}Yˉ) = μY\\\\mu_YμY\\n•\\tStandard deviation of prices at Agra (σX\\\\sigma_XσX) = sXs_XsX\\n•\\tStandard deviation of prices at Delhi (σY\\\\sigma_YσY) = sYs_YsY\\nThe regression equation of YYY on XXX is given by: Y=a+bXY = a + bXY=a+bX\\nWhere: b=r×sYsXb = r \\times \\x0crac{s_Y}{s_X}b=r×sXsY a=μY−bμXa = \\\\mu_Y - b \\\\mu_Xa=μY−bμX\\nGiven the price at Agra (X=70X = 70X=70), the predicted price at Delhi (YYY) is: Y=a+b×70Y = a + b \\times 70Y=a+b×70\\nLet's assume some hypothetical values for means and standard deviations for calculation:\\n•\\tXˉ=60\\x08ar{X} = 60Xˉ=60 (mean price at Agra)\\n•\\tYˉ=50\\x08ar{Y} = 50Yˉ=50 (mean price at Delhi)\\n•\\tσX=15\\\\sigma_X = 15σX=15 (standard deviation of prices at Agra)\\n•\\tσY=10\\\\sigma_Y = 10σY=10 (standard deviation of prices at Delhi)\\nNow, calculate bbb: b=0.8×1015=0.8×0.6667=0.5333b = 0.8 \\times \\x0crac{10}{15} = 0.8 \\times 0.6667 = 0.5333b=0.8×1510=0.8×0.6667=0.5333\\nCalculate aaa: a=50−0.5333×60=50−32=18a = 50 - 0.5333 \\times 60 = 50 - 32 = 18a=50−0.5333×60=50−32=18\\nNow, predict YYY for X=70X = 70X=70: Y=18+0.5333×70=18+37.331=55.331Y = 18 + 0.5333 \\times 70 = 18 + 37.331 = 55.331Y=18+0.5333×70=18+37.331=55.331\\nSo, the most likely price at Delhi corresponding to the price of Rs. 70 at Agra is approximately Rs. 55.33.\\nIf you can provide the actual means and standard deviations, we can perform a more accurate calculation.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finding the Most Likely Price at Delhi\n",
    "To find the most likely price at Delhi corresponding to the price of Rs. 70 at Agra using the given correlation coefficient (r=+0.8r = +0.8r=+0.8), we can use the regression equation. However, we need additional information such as the means and standard deviations of the prices at both locations.\n",
    "Assuming we have the means and standard deviations:\n",
    "•\tMean price at Agra (Xˉ\\bar{X}Xˉ) = μX\\mu_XμX\n",
    "•\tMean price at Delhi (Yˉ\\bar{Y}Yˉ) = μY\\mu_YμY\n",
    "•\tStandard deviation of prices at Agra (σX\\sigma_XσX) = sXs_XsX\n",
    "•\tStandard deviation of prices at Delhi (σY\\sigma_YσY) = sYs_YsY\n",
    "The regression equation of YYY on XXX is given by: Y=a+bXY = a + bXY=a+bX\n",
    "Where: b=r×sYsXb = r \\times \\frac{s_Y}{s_X}b=r×sXsY a=μY−bμXa = \\mu_Y - b \\mu_Xa=μY−bμX\n",
    "Given the price at Agra (X=70X = 70X=70), the predicted price at Delhi (YYY) is: Y=a+b×70Y = a + b \\times 70Y=a+b×70\n",
    "Let's assume some hypothetical values for means and standard deviations for calculation:\n",
    "•\tXˉ=60\\bar{X} = 60Xˉ=60 (mean price at Agra)\n",
    "•\tYˉ=50\\bar{Y} = 50Yˉ=50 (mean price at Delhi)\n",
    "•\tσX=15\\sigma_X = 15σX=15 (standard deviation of prices at Agra)\n",
    "•\tσY=10\\sigma_Y = 10σY=10 (standard deviation of prices at Delhi)\n",
    "Now, calculate bbb: b=0.8×1015=0.8×0.6667=0.5333b = 0.8 \\times \\frac{10}{15} = 0.8 \\times 0.6667 = 0.5333b=0.8×1510=0.8×0.6667=0.5333\n",
    "Calculate aaa: a=50−0.5333×60=50−32=18a = 50 - 0.5333 \\times 60 = 50 - 32 = 18a=50−0.5333×60=50−32=18\n",
    "Now, predict YYY for X=70X = 70X=70: Y=18+0.5333×70=18+37.331=55.331Y = 18 + 0.5333 \\times 70 = 18 + 37.331 = 55.331Y=18+0.5333×70=18+37.331=55.331\n",
    "So, the most likely price at Delhi corresponding to the price of Rs. 70 at Agra is approximately Rs. 55.33.\n",
    "If you can provide the actual means and standard deviations, we can perform a more accurate calculation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f7e3f42-047d-43c1-aed3-d403198aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\1384811185.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\no solve the given problem, we need to extract information from the given regression equations and the variance of xxx.\\nGiven Data:\\n1.\\tVariance of xxx (σx2\\\\sigma_x^2σx2) = 9\\n2.\\tRegression equations:\\no\\t8x−10y=−668x - 10y = -668x−10y=−66\\no\\t40x−18y=21440x - 18y = 21440x−18y=214\\nStep-by-Step Solution\\n1.\\tTransform the regression equations into the standard form:\\no\\tFrom the first equation: 8x−10y=−668x - 10y = -668x−10y=−66\\n\\uf0a7\\tDivide by 10: 0.8x−y=−6.60.8x - y = -6.60.8x−y=−6.6\\n\\uf0a7\\ty=0.8x+6.6y = 0.8x + 6.6y=0.8x+6.6\\no\\tFrom the second equation: 40x−18y=21440x - 18y = 21440x−18y=214\\n\\uf0a7\\tDivide by 18: 2.22x−y=11.892.22x - y = 11.892.22x−y=11.89\\n\\uf0a7\\ty=2.22x−11.89y = 2.22x - 11.89y=2.22x−11.89\\n(a) Mean values of xxx and yyy:\\nThe equations of regression lines can be used to find the means of xxx and yyy. At the point of means (xˉ\\x08ar{x}xˉ, yˉ\\x08ar{y}yˉ), the regression lines intersect.\\nFrom: y=0.8x+6.6y = 0.8x + 6.6y=0.8x+6.6 y=2.22x−11.89y = 2.22x - 11.89y=2.22x−11.89\\nSet the equations equal to each other to find xˉ\\x08ar{x}xˉ: 0.8xˉ+6.6=2.22xˉ−11.890.8\\x08ar{x} + 6.6 = 2.22\\x08ar{x} - 11.890.8xˉ+6.6=2.22xˉ−11.89 6.6+11.89=2.22xˉ−0.8xˉ6.6 + 11.89 = 2.22\\x08ar{x} - 0.8\\x08ar{x}6.6+11.89=2.22xˉ−0.8xˉ 18.49=1.42xˉ18.49 = 1.42\\x08ar{x}18.49=1.42xˉ xˉ=18.491.42\\x08ar{x} = \\x0crac{18.49}{1.42}xˉ=1.4218.49 xˉ=13.02\\x08ar{x} = 13.02xˉ=13.02\\nNow, substitute xˉ\\x08ar{x}xˉ back into one of the regression equations to find yˉ\\x08ar{y}yˉ: yˉ=0.8(13.02)+6.6\\x08ar{y} = 0.8(13.02) + 6.6yˉ=0.8(13.02)+6.6 yˉ=10.416+6.6\\x08ar{y} = 10.416 + 6.6yˉ=10.416+6.6 yˉ=17.02\\x08ar{y} = 17.02yˉ=17.02\\n(b) Coefficient of correlation (rrr):\\nThe regression coefficients (byxb_{yx}byx and bxyb_{xy}bxy) can be found from the given equations: byx=0.8b_{yx} = 0.8byx=0.8 bxy=12.22=0.450b_{xy} = \\x0crac{1}{2.22} = 0.450bxy=2.221=0.450\\nThe relationship between the regression coefficients and the correlation coefficient rrr is: r=byx⋅bxyr = \\\\sqrt{b_{yx} \\\\cdot b_{xy}}r=byx⋅bxy r=0.8⋅0.450r = \\\\sqrt{0.8 \\\\cdot 0.450}r=0.8⋅0.450 r=0.36r = \\\\sqrt{0.36}r=0.36 r=0.6r = 0.6r=0.6\\n(c) Standard deviation of yyy (σy\\\\sigma_yσy):\\nWe know the variance of xxx (σx2\\\\sigma_x^2σx2) is 9, so the standard deviation of xxx (σx\\\\sigma_xσx) is: σx=9\\\\sigma_x = \\\\sqrt{9}σx=9 σx=3\\\\sigma_x = 3σx=3\\nThe relationship between the standard deviations, regression coefficients, and correlation coefficient is: byx=r⋅σyσxb_{yx} = r \\\\cdot \\x0crac{\\\\sigma_y}{\\\\sigma_x}byx=r⋅σxσy 0.8=0.6⋅σy30.8 = 0.6 \\\\cdot \\x0crac{\\\\sigma_y}{3}0.8=0.6⋅3σy 0.8=0.6σy30.8 = \\x0crac{0.6\\\\sigma_y}{3}0.8=30.6σy σy=0.8⋅30.6\\\\sigma_y = \\x0crac{0.8 \\\\cdot 3}{0.6}σy=0.60.8⋅3 σy=4\\\\sigma_y = 4σy=4\\nSummary:\\n(a) The mean values are:\\n•\\txˉ=13.02\\x08ar{x} = 13.02xˉ=13.02\\n•\\tyˉ=17.02\\x08ar{y} = 17.02yˉ=17.02\\n(b) The coefficient of correlation rrr is 0.6.\\n(c) The standard deviation of yyy (σy\\\\sigma_yσy) is 4.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "o solve the given problem, we need to extract information from the given regression equations and the variance of xxx.\n",
    "Given Data:\n",
    "1.\tVariance of xxx (σx2\\sigma_x^2σx2) = 9\n",
    "2.\tRegression equations:\n",
    "o\t8x−10y=−668x - 10y = -668x−10y=−66\n",
    "o\t40x−18y=21440x - 18y = 21440x−18y=214\n",
    "Step-by-Step Solution\n",
    "1.\tTransform the regression equations into the standard form:\n",
    "o\tFrom the first equation: 8x−10y=−668x - 10y = -668x−10y=−66\n",
    "\tDivide by 10: 0.8x−y=−6.60.8x - y = -6.60.8x−y=−6.6\n",
    "\ty=0.8x+6.6y = 0.8x + 6.6y=0.8x+6.6\n",
    "o\tFrom the second equation: 40x−18y=21440x - 18y = 21440x−18y=214\n",
    "\tDivide by 18: 2.22x−y=11.892.22x - y = 11.892.22x−y=11.89\n",
    "\ty=2.22x−11.89y = 2.22x - 11.89y=2.22x−11.89\n",
    "(a) Mean values of xxx and yyy:\n",
    "The equations of regression lines can be used to find the means of xxx and yyy. At the point of means (xˉ\\bar{x}xˉ, yˉ\\bar{y}yˉ), the regression lines intersect.\n",
    "From: y=0.8x+6.6y = 0.8x + 6.6y=0.8x+6.6 y=2.22x−11.89y = 2.22x - 11.89y=2.22x−11.89\n",
    "Set the equations equal to each other to find xˉ\\bar{x}xˉ: 0.8xˉ+6.6=2.22xˉ−11.890.8\\bar{x} + 6.6 = 2.22\\bar{x} - 11.890.8xˉ+6.6=2.22xˉ−11.89 6.6+11.89=2.22xˉ−0.8xˉ6.6 + 11.89 = 2.22\\bar{x} - 0.8\\bar{x}6.6+11.89=2.22xˉ−0.8xˉ 18.49=1.42xˉ18.49 = 1.42\\bar{x}18.49=1.42xˉ xˉ=18.491.42\\bar{x} = \\frac{18.49}{1.42}xˉ=1.4218.49 xˉ=13.02\\bar{x} = 13.02xˉ=13.02\n",
    "Now, substitute xˉ\\bar{x}xˉ back into one of the regression equations to find yˉ\\bar{y}yˉ: yˉ=0.8(13.02)+6.6\\bar{y} = 0.8(13.02) + 6.6yˉ=0.8(13.02)+6.6 yˉ=10.416+6.6\\bar{y} = 10.416 + 6.6yˉ=10.416+6.6 yˉ=17.02\\bar{y} = 17.02yˉ=17.02\n",
    "(b) Coefficient of correlation (rrr):\n",
    "The regression coefficients (byxb_{yx}byx and bxyb_{xy}bxy) can be found from the given equations: byx=0.8b_{yx} = 0.8byx=0.8 bxy=12.22=0.450b_{xy} = \\frac{1}{2.22} = 0.450bxy=2.221=0.450\n",
    "The relationship between the regression coefficients and the correlation coefficient rrr is: r=byx⋅bxyr = \\sqrt{b_{yx} \\cdot b_{xy}}r=byx⋅bxy r=0.8⋅0.450r = \\sqrt{0.8 \\cdot 0.450}r=0.8⋅0.450 r=0.36r = \\sqrt{0.36}r=0.36 r=0.6r = 0.6r=0.6\n",
    "(c) Standard deviation of yyy (σy\\sigma_yσy):\n",
    "We know the variance of xxx (σx2\\sigma_x^2σx2) is 9, so the standard deviation of xxx (σx\\sigma_xσx) is: σx=9\\sigma_x = \\sqrt{9}σx=9 σx=3\\sigma_x = 3σx=3\n",
    "The relationship between the standard deviations, regression coefficients, and correlation coefficient is: byx=r⋅σyσxb_{yx} = r \\cdot \\frac{\\sigma_y}{\\sigma_x}byx=r⋅σxσy 0.8=0.6⋅σy30.8 = 0.6 \\cdot \\frac{\\sigma_y}{3}0.8=0.6⋅3σy 0.8=0.6σy30.8 = \\frac{0.6\\sigma_y}{3}0.8=30.6σy σy=0.8⋅30.6\\sigma_y = \\frac{0.8 \\cdot 3}{0.6}σy=0.60.8⋅3 σy=4\\sigma_y = 4σy=4\n",
    "Summary:\n",
    "(a) The mean values are:\n",
    "•\txˉ=13.02\\bar{x} = 13.02xˉ=13.02\n",
    "•\tyˉ=17.02\\bar{y} = 17.02yˉ=17.02\n",
    "(b) The coefficient of correlation rrr is 0.6.\n",
    "(c) The standard deviation of yyy (σy\\sigma_yσy) is 4.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "304fd0f9-b9e1-42f5-ac9d-90c3d536765b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normal Distribution\\nThe normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. It is characterized by its bell-shaped curve, where the mean, median, and mode of the distribution are all equal. The normal distribution is fundamental in statistics and is used to represent real-valued random variables with unknown distributions.\\n\\nFour Assumptions of Normal Distribution\\nIndependence:\\n\\nThe observations are independent of each other. This means that the occurrence of one event does not influence the occurrence of another.\\nRandom Sampling:\\n\\nThe data is collected through random sampling. This ensures that every possible sample has an equal chance of being selected, which helps in generalizing the results to the entire population.\\nConstant Variance (Homoscedasticity):\\n\\nThe variance within each subgroup of the data is the same. This assumption is essential for making valid inferences about the data. In other words, the spread or variability of the data points is consistent across the range of values.\\nNormally Distributed Errors:\\n\\nThe errors (residuals) of the model are normally distributed. This assumption ensures that the distribution of errors follows a normal distribution, which is crucial for many statistical tests and procedures'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Normal Distribution\n",
    "The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. It is characterized by its bell-shaped curve, where the mean, median, and mode of the distribution are all equal. The normal distribution is fundamental in statistics and is used to represent real-valued random variables with unknown distributions.\n",
    "\n",
    "Four Assumptions of Normal Distribution\n",
    "Independence:\n",
    "\n",
    "The observations are independent of each other. This means that the occurrence of one event does not influence the occurrence of another.\n",
    "Random Sampling:\n",
    "\n",
    "The data is collected through random sampling. This ensures that every possible sample has an equal chance of being selected, which helps in generalizing the results to the entire population.\n",
    "Constant Variance (Homoscedasticity):\n",
    "\n",
    "The variance within each subgroup of the data is the same. This assumption is essential for making valid inferences about the data. In other words, the spread or variability of the data points is consistent across the range of values.\n",
    "Normally Distributed Errors:\n",
    "\n",
    "The errors (residuals) of the model are normally distributed. This assumption ensures that the distribution of errors follows a normal distribution, which is crucial for many statistical tests and procedures\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2045f8bd-deb9-4c5d-8b80-9149f0924dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Characteristics or Properties of the Normal Distribution Curve\\nBell-Shaped Curve:\\n\\nThe normal distribution curve is bell-shaped and symmetric about the mean. This means that the left and right halves of the curve are mirror images of each other.\\nMean, Median, and Mode:\\n\\nIn a normal distribution, the mean, median, and mode are all located at the center of the distribution and are equal. This central peak is the highest point on the curve.\\nSymmetry:\\n\\nThe curve is symmetric about the mean. This implies that 50% of the data lies to the left of the mean and 50% to the right.\\nAsymptotic Nature:\\n\\nThe tails of the normal distribution curve approach but never touch the horizontal axis. This means that the probability of extreme values (far from the mean) is never zero.\\nEmpirical Rule (68-95-99.7 Rule):\\n\\nApproximately 68% of the data falls within one standard deviation of the mean.\\nAbout 95% of the data falls within two standard deviations of the mean.\\nNearly 99.7% of the data falls within three standard deviations of the mean.\\nTotal Area Under the Curve:\\n\\nThe total area under the normal distribution curve is equal to 1. This area represents the total probability of all possible outcomes.\\nUnimodal:\\n\\nThe normal distribution has a single peak, or mode, indicating that it is unimodal.\\nInflection Points:\\n\\nThe points where the curve changes concavity (from convex to concave or vice versa) are located at one standard deviation away from the mean.\\nNo Skewness:\\n\\nThe skewness of a normal distribution is zero, indicating no asymmetry in the distribution of data.\\nNo Kurtosis:\\n\\nThe kurtosis of a normal distribution is zero (mesokurtic), indicating that the tails of the distribution are neither heavy nor light compared to a normal distribution.\\nVisual Representation\\nHere is a visual representation of the properties of a normal distribution curve:\\n\\nThe highest point of the curve is at the mean (µ).\\nThe width of the curve is determined by the standard deviation (σ).\\nThe curve is symmetric about the mean, with the left and right halves being mirror images.\\nThe tails of the curve extend indefinitely and approach, but never touch, the horizontal axis.\\nThese properties and assumptions make the normal distribution a powerful tool in statistics, widely used in hypothesis testing, regression analysis, and various other statistical methods.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Characteristics or Properties of the Normal Distribution Curve\n",
    "Bell-Shaped Curve:\n",
    "\n",
    "The normal distribution curve is bell-shaped and symmetric about the mean. This means that the left and right halves of the curve are mirror images of each other.\n",
    "Mean, Median, and Mode:\n",
    "\n",
    "In a normal distribution, the mean, median, and mode are all located at the center of the distribution and are equal. This central peak is the highest point on the curve.\n",
    "Symmetry:\n",
    "\n",
    "The curve is symmetric about the mean. This implies that 50% of the data lies to the left of the mean and 50% to the right.\n",
    "Asymptotic Nature:\n",
    "\n",
    "The tails of the normal distribution curve approach but never touch the horizontal axis. This means that the probability of extreme values (far from the mean) is never zero.\n",
    "Empirical Rule (68-95-99.7 Rule):\n",
    "\n",
    "Approximately 68% of the data falls within one standard deviation of the mean.\n",
    "About 95% of the data falls within two standard deviations of the mean.\n",
    "Nearly 99.7% of the data falls within three standard deviations of the mean.\n",
    "Total Area Under the Curve:\n",
    "\n",
    "The total area under the normal distribution curve is equal to 1. This area represents the total probability of all possible outcomes.\n",
    "Unimodal:\n",
    "\n",
    "The normal distribution has a single peak, or mode, indicating that it is unimodal.\n",
    "Inflection Points:\n",
    "\n",
    "The points where the curve changes concavity (from convex to concave or vice versa) are located at one standard deviation away from the mean.\n",
    "No Skewness:\n",
    "\n",
    "The skewness of a normal distribution is zero, indicating no asymmetry in the distribution of data.\n",
    "No Kurtosis:\n",
    "\n",
    "The kurtosis of a normal distribution is zero (mesokurtic), indicating that the tails of the distribution are neither heavy nor light compared to a normal distribution.\n",
    "Visual Representation\n",
    "Here is a visual representation of the properties of a normal distribution curve:\n",
    "\n",
    "The highest point of the curve is at the mean (µ).\n",
    "The width of the curve is determined by the standard deviation (σ).\n",
    "The curve is symmetric about the mean, with the left and right halves being mirror images.\n",
    "The tails of the curve extend indefinitely and approach, but never touch, the horizontal axis.\n",
    "These properties and assumptions make the normal distribution a powerful tool in statistics, widely used in hypothesis testing, regression analysis, and various other statistical methods.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f7166ab-4607-45b3-ba62-6f89f0cf3e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLet's evaluate each statement based on the properties of the normal distribution:\\n(a) Within a range 0.6745 of σ on both sides the middle 50% of the observations occur i.e., mean ±0.6745σ covers 50% area, 25% on each side.\\n•\\tThis statement is correct. The value 0.6745σ corresponds to the z-score that covers the middle 50% of the observations in a normal distribution.\\n(b) Mean ±1S.D. (i.e., µ ± 1σ) covers 68.268% area, 34.134 % area lies on either side of the mean.\\n•\\tThis statement is correct. According to the empirical rule, about 68.268% of the data falls within one standard deviation from the mean, and therefore 34.134% lies on each side of the mean.\\n(c) Mean ±2S.D. (i.e., µ ± 2σ) covers 95.45% area, 47.725% area lies on either side of the mean.\\n•\\tThis statement is correct. Approximately 95.45% of the data falls within two standard deviations from the mean, and thus 47.725% lies on each side of the mean.\\n(d) Mean ±3 S.D. (i.e., µ ±3σ) covers 99.73% area, 49.856% area lies on either side of the mean.\\n•\\tThis statement is mostly correct, but there is a slight error in the percentage on each side of the mean. Mean ±3σ covers 99.73% area, and approximately 49.865% (not 49.856%) lies on either side of the mean.\\n(e) Only 0.27% area is outside the range µ ±3σ.\\n•\\tThis statement is correct. Since µ ± 3σ covers 99.73% of the data, the remaining 0.27% of the data lies outside this range.\\nSummary:\\nAll the statements are correct with minor precision issues in statement (d) regarding the exact percentage of area on each side of the mean. The values given are very close to the true values.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's evaluate each statement based on the properties of the normal distribution:\n",
    "(a) Within a range 0.6745 of σ on both sides the middle 50% of the observations occur i.e., mean ±0.6745σ covers 50% area, 25% on each side.\n",
    "•\tThis statement is correct. The value 0.6745σ corresponds to the z-score that covers the middle 50% of the observations in a normal distribution.\n",
    "(b) Mean ±1S.D. (i.e., µ ± 1σ) covers 68.268% area, 34.134 % area lies on either side of the mean.\n",
    "•\tThis statement is correct. According to the empirical rule, about 68.268% of the data falls within one standard deviation from the mean, and therefore 34.134% lies on each side of the mean.\n",
    "(c) Mean ±2S.D. (i.e., µ ± 2σ) covers 95.45% area, 47.725% area lies on either side of the mean.\n",
    "•\tThis statement is correct. Approximately 95.45% of the data falls within two standard deviations from the mean, and thus 47.725% lies on each side of the mean.\n",
    "(d) Mean ±3 S.D. (i.e., µ ±3σ) covers 99.73% area, 49.856% area lies on either side of the mean.\n",
    "•\tThis statement is mostly correct, but there is a slight error in the percentage on each side of the mean. Mean ±3σ covers 99.73% area, and approximately 49.865% (not 49.856%) lies on either side of the mean.\n",
    "(e) Only 0.27% area is outside the range µ ±3σ.\n",
    "•\tThis statement is correct. Since µ ± 3σ covers 99.73% of the data, the remaining 0.27% of the data lies outside this range.\n",
    "Summary:\n",
    "All the statements are correct with minor precision issues in statement (d) regarding the exact percentage of area on each side of the mean. The values given are very close to the true values.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52de697b-5803-4f98-a365-9efdd5d8d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\1277614881.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGiven a normal distribution with a mean (μ\\\\muμ) of 60 and a standard deviation (σ\\\\sigmaσ) of 10, we can find the percentage of items in specified ranges using the properties of the normal distribution and the z-score table.\\nFinding Z-Scores\\nThe z-score is calculated using the formula: z=x−μσz = \\x0crac{x - \\\\mu}{\\\\sigma}z=σx−μ\\n(i) Percentage of items between 60 and 72\\n1.\\tCalculate the z-scores: z60=60−6010=0z_{60} = \\x0crac{60 - 60}{10} = 0z60=1060−60=0 z72=72−6010=1.2z_{72} = \\x0crac{72 - 60}{10} = 1.2z72=1072−60=1.2\\n2.\\tFind the area under the normal curve between these z-scores:\\no\\tFrom the z-table, the area to the left of z=0z = 0z=0 is 0.5.\\no\\tFrom the z-table, the area to the left of z=1.2z = 1.2z=1.2 is approximately 0.8849.\\n3.\\tCalculate the percentage: Percentage=(0.8849−0.5)×100≈38.49%\\text{Percentage} = (0.8849 - 0.5) \\times 100 \\x07pprox 38.49\\\\%Percentage=(0.8849−0.5)×100≈38.49%\\n(ii) Percentage of items between 50 and 60\\n1.\\tCalculate the z-scores: z50=50−6010=−1z_{50} = \\x0crac{50 - 60}{10} = -1z50=1050−60=−1 z60=60−6010=0z_{60} = \\x0crac{60 - 60}{10} = 0z60=1060−60=0\\n2.\\tFind the area under the normal curve between these z-scores:\\no\\tFrom the z-table, the area to the left of z=−1z = -1z=−1 is approximately 0.1587.\\no\\tFrom the z-table, the area to the left of z=0z = 0z=0 is 0.5.\\n3.\\tCalculate the percentage: Percentage=(0.5−0.1587)×100≈34.13%\\text{Percentage} = (0.5 - 0.1587) \\times 100 \\x07pprox 34.13\\\\%Percentage=(0.5−0.1587)×100≈34.13%\\n(iii) Percentage of items beyond 72\\n1.\\tCalculate the z-score: z72=72−6010=1.2z_{72} = \\x0crac{72 - 60}{10} = 1.2z72=1072−60=1.2\\n2.\\tFind the area under the normal curve to the right of this z-score:\\no\\tFrom the z-table, the area to the left of z=1.2z = 1.2z=1.2 is approximately 0.8849.\\n3.\\tCalculate the percentage: Percentage=(1−0.8849)×100≈11.51%\\text{Percentage} = (1 - 0.8849) \\times 100 \\x07pprox 11.51\\\\%Percentage=(1−0.8849)×100≈11.51%\\n(iv) Percentage of items between 70 and 80\\n1.\\tCalculate the z-scores: z70=70−6010=1z_{70} = \\x0crac{70 - 60}{10} = 1z70=1070−60=1 z80=80−6010=2z_{80} = \\x0crac{80 - 60}{10} = 2z80=1080−60=2\\n2.\\tFind the area under the normal curve between these z-scores:\\no\\tFrom the z-table, the area to the left of z=1z = 1z=1 is approximately 0.8413.\\no\\tFrom the z-table, the area to the left of z=2z = 2z=2 is approximately 0.9772.\\n3.\\tCalculate the percentage: Percentage=(0.9772−0.8413)×100≈13.59%\\text{Percentage} = (0.9772 - 0.8413) \\times 100 \\x07pprox 13.59\\\\%Percentage=(0.9772−0.8413)×100≈13.59%\\nSummary\\n(i) Percentage of items between 60 and 72: 38.49%\\n(ii) Percentage of items between 50 and 60: 34.13%\\n(iii) Percentage of items beyond 72: 11.51%\\n(iv) Percentage of items between 70 and 80: 13.59%'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given a normal distribution with a mean (μ\\muμ) of 60 and a standard deviation (σ\\sigmaσ) of 10, we can find the percentage of items in specified ranges using the properties of the normal distribution and the z-score table.\n",
    "Finding Z-Scores\n",
    "The z-score is calculated using the formula: z=x−μσz = \\frac{x - \\mu}{\\sigma}z=σx−μ\n",
    "(i) Percentage of items between 60 and 72\n",
    "1.\tCalculate the z-scores: z60=60−6010=0z_{60} = \\frac{60 - 60}{10} = 0z60=1060−60=0 z72=72−6010=1.2z_{72} = \\frac{72 - 60}{10} = 1.2z72=1072−60=1.2\n",
    "2.\tFind the area under the normal curve between these z-scores:\n",
    "o\tFrom the z-table, the area to the left of z=0z = 0z=0 is 0.5.\n",
    "o\tFrom the z-table, the area to the left of z=1.2z = 1.2z=1.2 is approximately 0.8849.\n",
    "3.\tCalculate the percentage: Percentage=(0.8849−0.5)×100≈38.49%\\text{Percentage} = (0.8849 - 0.5) \\times 100 \\approx 38.49\\%Percentage=(0.8849−0.5)×100≈38.49%\n",
    "(ii) Percentage of items between 50 and 60\n",
    "1.\tCalculate the z-scores: z50=50−6010=−1z_{50} = \\frac{50 - 60}{10} = -1z50=1050−60=−1 z60=60−6010=0z_{60} = \\frac{60 - 60}{10} = 0z60=1060−60=0\n",
    "2.\tFind the area under the normal curve between these z-scores:\n",
    "o\tFrom the z-table, the area to the left of z=−1z = -1z=−1 is approximately 0.1587.\n",
    "o\tFrom the z-table, the area to the left of z=0z = 0z=0 is 0.5.\n",
    "3.\tCalculate the percentage: Percentage=(0.5−0.1587)×100≈34.13%\\text{Percentage} = (0.5 - 0.1587) \\times 100 \\approx 34.13\\%Percentage=(0.5−0.1587)×100≈34.13%\n",
    "(iii) Percentage of items beyond 72\n",
    "1.\tCalculate the z-score: z72=72−6010=1.2z_{72} = \\frac{72 - 60}{10} = 1.2z72=1072−60=1.2\n",
    "2.\tFind the area under the normal curve to the right of this z-score:\n",
    "o\tFrom the z-table, the area to the left of z=1.2z = 1.2z=1.2 is approximately 0.8849.\n",
    "3.\tCalculate the percentage: Percentage=(1−0.8849)×100≈11.51%\\text{Percentage} = (1 - 0.8849) \\times 100 \\approx 11.51\\%Percentage=(1−0.8849)×100≈11.51%\n",
    "(iv) Percentage of items between 70 and 80\n",
    "1.\tCalculate the z-scores: z70=70−6010=1z_{70} = \\frac{70 - 60}{10} = 1z70=1070−60=1 z80=80−6010=2z_{80} = \\frac{80 - 60}{10} = 2z80=1080−60=2\n",
    "2.\tFind the area under the normal curve between these z-scores:\n",
    "o\tFrom the z-table, the area to the left of z=1z = 1z=1 is approximately 0.8413.\n",
    "o\tFrom the z-table, the area to the left of z=2z = 2z=2 is approximately 0.9772.\n",
    "3.\tCalculate the percentage: Percentage=(0.9772−0.8413)×100≈13.59%\\text{Percentage} = (0.9772 - 0.8413) \\times 100 \\approx 13.59\\%Percentage=(0.9772−0.8413)×100≈13.59%\n",
    "Summary\n",
    "(i) Percentage of items between 60 and 72: 38.49%\n",
    "(ii) Percentage of items between 50 and 60: 34.13%\n",
    "(iii) Percentage of items beyond 72: 11.51%\n",
    "(iv) Percentage of items between 70 and 80: 13.59%\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c3db9bb0-2b31-4022-a4d9-668ab1e451a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\2606715326.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGiven a normal distribution with a mean (μ\\\\muμ) of 49 and a standard deviation (σ\\\\sigmaσ) of 6, we can determine the proportion of students who scored above certain marks using z-scores and the standard normal distribution table.\\n(a) Proportion of students who scored more than 55 marks\\n1.\\tCalculate the z-score for 55 marks: z=55−496=66=1z = \\x0crac{55 - 49}{6} = \\x0crac{6}{6} = 1z=655−49=66=1\\n2.\\tFind the area to the left of z=1z = 1z=1 using the z-table:\\no\\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\\n3.\\tCalculate the area to the right (proportion of students who scored more than 55 marks): Proportion=1−0.8413=0.1587\\text{Proportion} = 1 - 0.8413 = 0.1587Proportion=1−0.8413=0.1587\\nSo, approximately 15.87% of students scored more than 55 marks.\\nTo find the number of students: Number of students=15000×0.1587=2380.5\\text{Number of students} = 15000 \\times 0.1587 = 2380.5Number of students=15000×0.1587=2380.5\\nApproximately 2381 students scored more than 55 marks.\\n(b) Proportion of students who scored more than 70 marks\\n1.\\tCalculate the z-score for 70 marks: z=70−496=216=3.5z = \\x0crac{70 - 49}{6} = \\x0crac{21}{6} = 3.5z=670−49=621=3.5\\n2.\\tFind the area to the left of z=3.5z = 3.5z=3.5 using the z-table:\\no\\tThe area to the left of z=3.5z = 3.5z=3.5 is approximately 0.9997.\\n3.\\tCalculate the area to the right (proportion of students who scored more than 70 marks): Proportion=1−0.9997=0.0003\\text{Proportion} = 1 - 0.9997 = 0.0003Proportion=1−0.9997=0.0003\\nSo, approximately 0.03% of students scored more than 70 marks.\\nTo find the number of students: Number of students=15000×0.0003=4.5\\text{Number of students} = 15000 \\times 0.0003 = 4.5Number of students=15000×0.0003=4.5\\nApproximately 5 students scored more than 70 marks.\\nSummary\\n(a) 15.87% of students (approximately 2381 students) scored more than 55 marks.\\n(b) 0.03% of students (approximately 5 students) scored more than 70 marks.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given a normal distribution with a mean (μ\\muμ) of 49 and a standard deviation (σ\\sigmaσ) of 6, we can determine the proportion of students who scored above certain marks using z-scores and the standard normal distribution table.\n",
    "(a) Proportion of students who scored more than 55 marks\n",
    "1.\tCalculate the z-score for 55 marks: z=55−496=66=1z = \\frac{55 - 49}{6} = \\frac{6}{6} = 1z=655−49=66=1\n",
    "2.\tFind the area to the left of z=1z = 1z=1 using the z-table:\n",
    "o\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\n",
    "3.\tCalculate the area to the right (proportion of students who scored more than 55 marks): Proportion=1−0.8413=0.1587\\text{Proportion} = 1 - 0.8413 = 0.1587Proportion=1−0.8413=0.1587\n",
    "So, approximately 15.87% of students scored more than 55 marks.\n",
    "To find the number of students: Number of students=15000×0.1587=2380.5\\text{Number of students} = 15000 \\times 0.1587 = 2380.5Number of students=15000×0.1587=2380.5\n",
    "Approximately 2381 students scored more than 55 marks.\n",
    "(b) Proportion of students who scored more than 70 marks\n",
    "1.\tCalculate the z-score for 70 marks: z=70−496=216=3.5z = \\frac{70 - 49}{6} = \\frac{21}{6} = 3.5z=670−49=621=3.5\n",
    "2.\tFind the area to the left of z=3.5z = 3.5z=3.5 using the z-table:\n",
    "o\tThe area to the left of z=3.5z = 3.5z=3.5 is approximately 0.9997.\n",
    "3.\tCalculate the area to the right (proportion of students who scored more than 70 marks): Proportion=1−0.9997=0.0003\\text{Proportion} = 1 - 0.9997 = 0.0003Proportion=1−0.9997=0.0003\n",
    "So, approximately 0.03% of students scored more than 70 marks.\n",
    "To find the number of students: Number of students=15000×0.0003=4.5\\text{Number of students} = 15000 \\times 0.0003 = 4.5Number of students=15000×0.0003=4.5\n",
    "Approximately 5 students scored more than 70 marks.\n",
    "Summary\n",
    "(a) 15.87% of students (approximately 2381 students) scored more than 55 marks.\n",
    "(b) 0.03% of students (approximately 5 students) scored more than 70 marks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6d9ff9d9-991a-4412-be78-1259d555c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\1482582530.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHeight of 500 Students\\nGiven a normal distribution of heights for 500 students with a mean (μ\\\\muμ) of 65 inches and a standard deviation (σ\\\\sigmaσ) of 5 inches, we can find the number of students with heights in specified ranges using z-scores and the standard normal distribution table.\\n(a) Students with height greater than 70 inches\\n1.\\tCalculate the z-score for 70 inches: z=70−655=1z = \\x0crac{70 - 65}{5} = 1z=570−65=1\\n2.\\tFind the area to the left of z=1z = 1z=1 using the z-table:\\no\\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\\n3.\\tCalculate the area to the right (proportion of students with height greater than 70 inches): Proportion=1−0.8413=0.1587\\text{Proportion} = 1 - 0.8413 = 0.1587Proportion=1−0.8413=0.1587\\n4.\\tCalculate the number of students: Number of students=500×0.1587=79.35\\text{Number of students} = 500 \\times 0.1587 = 79.35Number of students=500×0.1587=79.35\\nApproximately 79 students have a height greater than 70 inches.\\n(b) Students with height between 60 and 70 inches\\n1.\\tCalculate the z-scores: z60=60−655=−1z_{60} = \\x0crac{60 - 65}{5} = -1z60=560−65=−1 z70=70−655=1z_{70} = \\x0crac{70 - 65}{5} = 1z70=570−65=1\\n2.\\tFind the area to the left of these z-scores using the z-table:\\no\\tThe area to the left of z=−1z = -1z=−1 is approximately 0.1587.\\no\\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\\n3.\\tCalculate the area between the z-scores: Proportion=0.8413−0.1587=0.6826\\text{Proportion} = 0.8413 - 0.1587 = 0.6826Proportion=0.8413−0.1587=0.6826\\n4.\\tCalculate the number of students: Number of students=500×0.6826=341.3\\text{Number of students} = 500 \\times 0.6826 = 341.3Number of students=500×0.6826=341.3\\nApproximately 341 students have a height between 60 and 70 inches.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Height of 500 Students\n",
    "Given a normal distribution of heights for 500 students with a mean (μ\\muμ) of 65 inches and a standard deviation (σ\\sigmaσ) of 5 inches, we can find the number of students with heights in specified ranges using z-scores and the standard normal distribution table.\n",
    "(a) Students with height greater than 70 inches\n",
    "1.\tCalculate the z-score for 70 inches: z=70−655=1z = \\frac{70 - 65}{5} = 1z=570−65=1\n",
    "2.\tFind the area to the left of z=1z = 1z=1 using the z-table:\n",
    "o\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\n",
    "3.\tCalculate the area to the right (proportion of students with height greater than 70 inches): Proportion=1−0.8413=0.1587\\text{Proportion} = 1 - 0.8413 = 0.1587Proportion=1−0.8413=0.1587\n",
    "4.\tCalculate the number of students: Number of students=500×0.1587=79.35\\text{Number of students} = 500 \\times 0.1587 = 79.35Number of students=500×0.1587=79.35\n",
    "Approximately 79 students have a height greater than 70 inches.\n",
    "(b) Students with height between 60 and 70 inches\n",
    "1.\tCalculate the z-scores: z60=60−655=−1z_{60} = \\frac{60 - 65}{5} = -1z60=560−65=−1 z70=70−655=1z_{70} = \\frac{70 - 65}{5} = 1z70=570−65=1\n",
    "2.\tFind the area to the left of these z-scores using the z-table:\n",
    "o\tThe area to the left of z=−1z = -1z=−1 is approximately 0.1587.\n",
    "o\tThe area to the left of z=1z = 1z=1 is approximately 0.8413.\n",
    "3.\tCalculate the area between the z-scores: Proportion=0.8413−0.1587=0.6826\\text{Proportion} = 0.8413 - 0.1587 = 0.6826Proportion=0.8413−0.1587=0.6826\n",
    "4.\tCalculate the number of students: Number of students=500×0.6826=341.3\\text{Number of students} = 500 \\times 0.6826 = 341.3Number of students=500×0.6826=341.3\n",
    "Approximately 341 students have a height between 60 and 70 inches.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "481d3a8f-8675-4274-972f-7b1b085f5156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStatistical Hypothesis\\nDefinition\\nA statistical hypothesis is a statement or assumption about a population parameter (such as the mean or variance) that can be tested using statistical methods. There are two types of hypotheses:\\n\\nNull Hypothesis (H₀): A statement of no effect or no difference, which serves as the starting point for hypothesis testing.\\nAlternative Hypothesis (H₁ or Ha): A statement that contradicts the null hypothesis, indicating the presence of an effect or difference.\\nErrors in Hypothesis Testing\\nType I Error (\\n𝛼\\nα): The error of rejecting the null hypothesis when it is actually true. This is also known as a \"false positive\" and the probability of making this error is denoted by the significance level \\n𝛼\\nα.\\nType II Error (\\n𝛽\\nβ): The error of failing to reject the null hypothesis when it is actually false. This is also known as a \"false negative\" and the probability of making this error is denoted by \\n𝛽\\nβ.\\nPower of a Test: The probability of correctly rejecting the null hypothesis when it is false, calculated as \\n1\\n−\\n𝛽\\n1−β.\\nSample\\nA sample is a subset of individuals or observations selected from a larger population. The purpose of sampling is to make inferences about the population without examining every member.\\n\\nLarge Samples & Small Samples\\nLarge Samples: Typically, a sample is considered large if it has 30 or more observations. Large samples tend to provide more reliable and stable estimates of population parameters due to the central limit theorem, which states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.\\nSmall Samples: A sample is considered small if it has fewer than 30 observations. Statistical methods for small samples often require the assumption that the population from which the sample is drawn is normally distributed. Small samples are more susceptible to sampling variability and may not provide as reliable estimates as large samples.\\nIn summary, a statistical hypothesis is a testable statement about a population parameter, and hypothesis testing involves making decisions based on sample data, with potential for Type I and Type II errors. The concept of samples is fundamental in statistics, with distinctions between large and small samples influencing the choice of statistical methods.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Statistical Hypothesis\n",
    "Definition\n",
    "A statistical hypothesis is a statement or assumption about a population parameter (such as the mean or variance) that can be tested using statistical methods. There are two types of hypotheses:\n",
    "\n",
    "Null Hypothesis (H₀): A statement of no effect or no difference, which serves as the starting point for hypothesis testing.\n",
    "Alternative Hypothesis (H₁ or Ha): A statement that contradicts the null hypothesis, indicating the presence of an effect or difference.\n",
    "Errors in Hypothesis Testing\n",
    "Type I Error (\n",
    "𝛼\n",
    "α): The error of rejecting the null hypothesis when it is actually true. This is also known as a \"false positive\" and the probability of making this error is denoted by the significance level \n",
    "𝛼\n",
    "α.\n",
    "Type II Error (\n",
    "𝛽\n",
    "β): The error of failing to reject the null hypothesis when it is actually false. This is also known as a \"false negative\" and the probability of making this error is denoted by \n",
    "𝛽\n",
    "β.\n",
    "Power of a Test: The probability of correctly rejecting the null hypothesis when it is false, calculated as \n",
    "1\n",
    "−\n",
    "𝛽\n",
    "1−β.\n",
    "Sample\n",
    "A sample is a subset of individuals or observations selected from a larger population. The purpose of sampling is to make inferences about the population without examining every member.\n",
    "\n",
    "Large Samples & Small Samples\n",
    "Large Samples: Typically, a sample is considered large if it has 30 or more observations. Large samples tend to provide more reliable and stable estimates of population parameters due to the central limit theorem, which states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.\n",
    "Small Samples: A sample is considered small if it has fewer than 30 observations. Statistical methods for small samples often require the assumption that the population from which the sample is drawn is normally distributed. Small samples are more susceptible to sampling variability and may not provide as reliable estimates as large samples.\n",
    "In summary, a statistical hypothesis is a testable statement about a population parameter, and hypothesis testing involves making decisions based on sample data, with potential for Type I and Type II errors. The concept of samples is fundamental in statistics, with distinctions between large and small samples influencing the choice of statistical methods.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6682b323-4641-49e9-abe5-b95bfcc65611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\2997388779.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo test the hypothesis that the population standard deviation is 10.5 using the chi-square distribution, we can use the following steps:\\nHypothesis\\n•\\tNull Hypothesis (H0H_0H0): The population standard deviation (σ\\\\sigmaσ) is 10.5.\\n•\\tAlternative Hypothesis (H1H_1H1): The population standard deviation (σ\\\\sigmaσ) is not 10.5.\\nTest Statistic\\nThe test statistic for the chi-square test on variance is given by: χ2=(n−1)s2σ02\\\\chi^2 = \\x0crac{(n-1)s^2}{\\\\sigma_0^2}χ2=σ02(n−1)s2 where:\\n•\\tnnn is the sample size\\n•\\tsss is the sample standard deviation\\n•\\tσ0\\\\sigma_0σ0 is the hypothesized population standard deviation\\nIn this case:\\n•\\tn=25n = 25n=25\\n•\\ts=9.0s = 9.0s=9.0\\n•\\tσ0=10.5\\\\sigma_0 = 10.5σ0=10.5\\nCalculation\\n1.\\tCalculate the test statistic: χ2=(25−1)×9.0210.52=24×81110.25=1944110.25≈17.63\\\\chi^2 = \\x0crac{(25-1) \\times 9.0^2}{10.5^2} = \\x0crac{24 \\times 81}{110.25} = \\x0crac{1944}{110.25} \\x07pprox 17.63χ2=10.52(25−1)×9.02=110.2524×81=110.251944≈17.63\\n2.\\tDetermine the degrees of freedom: df=n−1=25−1=24\\text{df} = n - 1 = 25 - 1 = 24df=n−1=25−1=24\\n3.\\tDetermine the critical values for the chi-square distribution at the chosen significance level (α\\x07lphaα). For this example, let's use α=0.05\\x07lpha = 0.05α=0.05 for a two-tailed test. We need the chi-square values that correspond to the lower and upper tails of the distribution with 24 degrees of freedom.\\no\\tThe critical value for the lower tail (χα/2,df2\\\\chi^2_{\\x07lpha/2, df}χα/2,df2) at α/2=0.025\\x07lpha/2 = 0.025α/2=0.025 with 24 degrees of freedom can be found using chi-square tables or statistical software.\\no\\tThe critical value for the upper tail (χ1−α/2,df2\\\\chi^2_{1-\\x07lpha/2, df}χ1−α/2,df2) at 1−α/2=0.9751-\\x07lpha/2 = 0.9751−α/2=0.975 with 24 degrees of freedom can also be found using chi-square tables or statistical software.\\nUsing a chi-square table, we find:\\n•\\tχ0.025,242≈13.848\\\\chi^2_{0.025, 24} \\x07pprox 13.848χ0.025,242≈13.848\\n•\\tχ0.975,242≈36.415\\\\chi^2_{0.975, 24} \\x07pprox 36.415χ0.975,242≈36.415\\nDecision Rule\\n•\\tIf the calculated chi-square statistic falls between the critical values, we fail to reject the null hypothesis.\\n•\\tIf the calculated chi-square statistic is less than the lower critical value or greater than the upper critical value, we reject the null hypothesis.\\nConclusion\\nSince 13.848<17.63<36.41513.848 < 17.63 < 36.41513.848<17.63<36.415, the calculated chi-square statistic falls within the range of the critical values.\\nTherefore, we fail to reject the null hypothesis. This means there is not enough evidence to conclude that the population standard deviation is different from 10.5 at the 5% significance level.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To test the hypothesis that the population standard deviation is 10.5 using the chi-square distribution, we can use the following steps:\n",
    "Hypothesis\n",
    "•\tNull Hypothesis (H0H_0H0): The population standard deviation (σ\\sigmaσ) is 10.5.\n",
    "•\tAlternative Hypothesis (H1H_1H1): The population standard deviation (σ\\sigmaσ) is not 10.5.\n",
    "Test Statistic\n",
    "The test statistic for the chi-square test on variance is given by: χ2=(n−1)s2σ02\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2}χ2=σ02(n−1)s2 where:\n",
    "•\tnnn is the sample size\n",
    "•\tsss is the sample standard deviation\n",
    "•\tσ0\\sigma_0σ0 is the hypothesized population standard deviation\n",
    "In this case:\n",
    "•\tn=25n = 25n=25\n",
    "•\ts=9.0s = 9.0s=9.0\n",
    "•\tσ0=10.5\\sigma_0 = 10.5σ0=10.5\n",
    "Calculation\n",
    "1.\tCalculate the test statistic: χ2=(25−1)×9.0210.52=24×81110.25=1944110.25≈17.63\\chi^2 = \\frac{(25-1) \\times 9.0^2}{10.5^2} = \\frac{24 \\times 81}{110.25} = \\frac{1944}{110.25} \\approx 17.63χ2=10.52(25−1)×9.02=110.2524×81=110.251944≈17.63\n",
    "2.\tDetermine the degrees of freedom: df=n−1=25−1=24\\text{df} = n - 1 = 25 - 1 = 24df=n−1=25−1=24\n",
    "3.\tDetermine the critical values for the chi-square distribution at the chosen significance level (α\\alphaα). For this example, let's use α=0.05\\alpha = 0.05α=0.05 for a two-tailed test. We need the chi-square values that correspond to the lower and upper tails of the distribution with 24 degrees of freedom.\n",
    "o\tThe critical value for the lower tail (χα/2,df2\\chi^2_{\\alpha/2, df}χα/2,df2) at α/2=0.025\\alpha/2 = 0.025α/2=0.025 with 24 degrees of freedom can be found using chi-square tables or statistical software.\n",
    "o\tThe critical value for the upper tail (χ1−α/2,df2\\chi^2_{1-\\alpha/2, df}χ1−α/2,df2) at 1−α/2=0.9751-\\alpha/2 = 0.9751−α/2=0.975 with 24 degrees of freedom can also be found using chi-square tables or statistical software.\n",
    "Using a chi-square table, we find:\n",
    "•\tχ0.025,242≈13.848\\chi^2_{0.025, 24} \\approx 13.848χ0.025,242≈13.848\n",
    "•\tχ0.975,242≈36.415\\chi^2_{0.975, 24} \\approx 36.415χ0.975,242≈36.415\n",
    "Decision Rule\n",
    "•\tIf the calculated chi-square statistic falls between the critical values, we fail to reject the null hypothesis.\n",
    "•\tIf the calculated chi-square statistic is less than the lower critical value or greater than the upper critical value, we reject the null hypothesis.\n",
    "Conclusion\n",
    "Since 13.848<17.63<36.41513.848 < 17.63 < 36.41513.848<17.63<36.415, the calculated chi-square statistic falls within the range of the critical values.\n",
    "Therefore, we fail to reject the null hypothesis. This means there is not enough evidence to conclude that the population standard deviation is different from 10.5 at the 5% significance level.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad455ce1-817b-4aa7-b61f-97eff958a3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\1589493433.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"To test the hypothesis that the distribution of grades is uniform using the chi-square (χ2\\chi^2χ2) test, we will follow these steps:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To test the hypothesis that the distribution of grades is uniform using the chi-square (χ2\\\\chi^2χ2) test, we will follow these steps:\\nHypotheses\\n•\\tNull Hypothesis (H0H_0H0): The grades are uniformly distributed.\\n•\\tAlternative Hypothesis (H1H_1H1): The grades are not uniformly distributed.\\nObserved Frequencies\\nThe observed frequencies (O) of grades are:\\n•\\tGrade A: 15\\n•\\tGrade B: 17\\n•\\tGrade C: 30\\n•\\tGrade D: 22\\n•\\tGrade E: 16\\nExpected Frequencies\\nIf the grades are uniformly distributed, each grade should have the same frequency. Since there are 100 students and 5 grades, the expected frequency (E) for each grade is: E=1005=20E = \\x0crac{100}{5} = 20E=5100=20\\nChi-Square Test Statistic\\nThe chi-square test statistic is calculated using the formula: χ2=∑(Oi−Ei)2Ei\\\\chi^2 = \\\\sum \\x0crac{(O_i - E_i)^2}{E_i}χ2=∑Ei(Oi−Ei)2 where OiO_iOi is the observed frequency and EiE_iEi is the expected frequency for each grade.\\nCalculation\\nGradeObserved Frequency(Oi)Expected Frequency(Ei)(Oi−Ei)2EiA1520(15−20)220=2520=1.25B1720(17−20)220=920=0.45C3020(30−20)220=10020=5D2220(22−20)220=420=0.2E1620(16−20)220=1620=0.8\\x08egin{array}{c|c|c|c} \\text{Grade} & \\text{Observed Frequency} (O_i) & \\text{Expected Frequency} (E_i) & \\x0crac{(O_i - E_i)^2}{E_i} \\\\ \\\\hline A & 15 & 20 & \\x0crac{(15 - 20)^2}{20} = \\x0crac{25}{20} = 1.25 \\\\ B & 17 & 20 & \\x0crac{(17 - 20)^2}{20} = \\x0crac{9}{20} = 0.45 \\\\ C & 30 & 20 & \\x0crac{(30 - 20)^2}{20} = \\x0crac{100}{20} = 5 \\\\ D & 22 & 20 & \\x0crac{(22 - 20)^2}{20} = \\x0crac{4}{20} = 0.2 \\\\ E & 16 & 20 & \\x0crac{(16 - 20)^2}{20} = \\x0crac{16}{20} = 0.8 \\\\ \\\\end{array}GradeABCDEObserved Frequency(Oi)1517302216Expected Frequency(Ei)2020202020Ei(Oi−Ei)220(15−20)2=2025=1.2520(17−20)2=209=0.4520(30−20)2=20100=520(22−20)2=204=0.220(16−20)2=2016=0.8\\nSumming these values: χ2=1.25+0.45+5+0.2+0.8=7.7\\\\chi^2 = 1.25 + 0.45 + 5 + 0.2 + 0.8 = 7.7χ2=1.25+0.45+5+0.2+0.8=7.7\\nDegrees of Freedom\\nThe degrees of freedom (df) for the chi-square test is calculated as: df=number of categories−1=5−1=4\\text{df} = \\text{number of categories} - 1 = 5 - 1 = 4df=number of categories−1=5−1=4\\nCritical Value\\nWe will use a significance level (α\\x07lphaα) of 0.05. Using the chi-square table, the critical value for 4 degrees of freedom at α=0.05\\x07lpha = 0.05α=0.05 is approximately 9.488.\\nDecision\\n•\\tIf χ2\\\\chi^2χ2 calculated is less than the critical value, we fail to reject the null hypothesis.\\n•\\tIf χ2\\\\chi^2χ2 calculated is greater than the critical value, we reject the null hypothesis.\\nIn this case: χ2=7.7\\\\chi^2 = 7.7χ2=7.7 Critical value=9.488\\text{Critical value} = 9.488Critical value=9.488\\nSince 7.7 < 9.488, we fail to reject the null hypothesis.\\nConclusion\\nThere is not enough evidence to reject the hypothesis that the distribution of grades is uniform. Therefore, we conclude that the grades are uniformly distributed at the 5% significance level.\\n4o'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To test the hypothesis that the distribution of grades is uniform using the chi-square (χ2\\chi^2χ2) test, we will follow these steps:\n",
    "Hypotheses\n",
    "•\tNull Hypothesis (H0H_0H0): The grades are uniformly distributed.\n",
    "•\tAlternative Hypothesis (H1H_1H1): The grades are not uniformly distributed.\n",
    "Observed Frequencies\n",
    "The observed frequencies (O) of grades are:\n",
    "•\tGrade A: 15\n",
    "•\tGrade B: 17\n",
    "•\tGrade C: 30\n",
    "•\tGrade D: 22\n",
    "•\tGrade E: 16\n",
    "Expected Frequencies\n",
    "If the grades are uniformly distributed, each grade should have the same frequency. Since there are 100 students and 5 grades, the expected frequency (E) for each grade is: E=1005=20E = \\frac{100}{5} = 20E=5100=20\n",
    "Chi-Square Test Statistic\n",
    "The chi-square test statistic is calculated using the formula: χ2=∑(Oi−Ei)2Ei\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}χ2=∑Ei(Oi−Ei)2 where OiO_iOi is the observed frequency and EiE_iEi is the expected frequency for each grade.\n",
    "Calculation\n",
    "GradeObserved Frequency(Oi)Expected Frequency(Ei)(Oi−Ei)2EiA1520(15−20)220=2520=1.25B1720(17−20)220=920=0.45C3020(30−20)220=10020=5D2220(22−20)220=420=0.2E1620(16−20)220=1620=0.8\\begin{array}{c|c|c|c} \\text{Grade} & \\text{Observed Frequency} (O_i) & \\text{Expected Frequency} (E_i) & \\frac{(O_i - E_i)^2}{E_i} \\\\ \\hline A & 15 & 20 & \\frac{(15 - 20)^2}{20} = \\frac{25}{20} = 1.25 \\\\ B & 17 & 20 & \\frac{(17 - 20)^2}{20} = \\frac{9}{20} = 0.45 \\\\ C & 30 & 20 & \\frac{(30 - 20)^2}{20} = \\frac{100}{20} = 5 \\\\ D & 22 & 20 & \\frac{(22 - 20)^2}{20} = \\frac{4}{20} = 0.2 \\\\ E & 16 & 20 & \\frac{(16 - 20)^2}{20} = \\frac{16}{20} = 0.8 \\\\ \\end{array}GradeABCDEObserved Frequency(Oi)1517302216Expected Frequency(Ei)2020202020Ei(Oi−Ei)220(15−20)2=2025=1.2520(17−20)2=209=0.4520(30−20)2=20100=520(22−20)2=204=0.220(16−20)2=2016=0.8\n",
    "Summing these values: χ2=1.25+0.45+5+0.2+0.8=7.7\\chi^2 = 1.25 + 0.45 + 5 + 0.2 + 0.8 = 7.7χ2=1.25+0.45+5+0.2+0.8=7.7\n",
    "Degrees of Freedom\n",
    "The degrees of freedom (df) for the chi-square test is calculated as: df=number of categories−1=5−1=4\\text{df} = \\text{number of categories} - 1 = 5 - 1 = 4df=number of categories−1=5−1=4\n",
    "Critical Value\n",
    "We will use a significance level (α\\alphaα) of 0.05. Using the chi-square table, the critical value for 4 degrees of freedom at α=0.05\\alpha = 0.05α=0.05 is approximately 9.488.\n",
    "Decision\n",
    "•\tIf χ2\\chi^2χ2 calculated is less than the critical value, we fail to reject the null hypothesis.\n",
    "•\tIf χ2\\chi^2χ2 calculated is greater than the critical value, we reject the null hypothesis.\n",
    "In this case: χ2=7.7\\chi^2 = 7.7χ2=7.7 Critical value=9.488\\text{Critical value} = 9.488Critical value=9.488\n",
    "Since 7.7 < 9.488, we fail to reject the null hypothesis.\n",
    "Conclusion\n",
    "There is not enough evidence to reject the hypothesis that the distribution of grades is uniform. Therefore, we conclude that the grades are uniformly distributed at the 5% significance level.\n",
    "4o\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4f3fe595-526b-4d69-be39-c512a38980f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_13192\\2842406643.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo study the performance of three detergents (A, B, and C) at three different water temperatures (Cold, Warm, and Hot) using an ANOVA (Analysis of Variance) test, we need to determine if there are significant differences between the means of the whiteness readings for the different detergents and temperatures. Here, we are dealing with a two-way ANOVA without replication because we have one observation for each combination of detergent and temperature.\\nData\\nLet's summarize the data from the table:\\nWater Temp\\tDetergent A\\tDetergent B\\tDetergent C\\nCold\\t57\\t55\\t67\\nWarm\\t49\\t52\\t68\\nHot\\t54\\t46\\t58\\nSteps to Perform Two-Way ANOVA\\n1.\\tCalculate the Grand Mean (Xˉ\\x08ar{X}Xˉ):\\nXˉ=∑XijN\\x08ar{X} = \\x0crac{\\\\sum X_{ij}}{N}Xˉ=N∑Xij\\nwhere NNN is the total number of observations.\\nTotal of all observations:\\n57+55+67+49+52+68+54+46+58=50657 + 55 + 67 + 49 + 52 + 68 + 54 + 46 + 58 = 50657+55+67+49+52+68+54+46+58=506\\nGrand Mean (Xˉ\\x08ar{X}Xˉ):\\nXˉ=5069≈56.22\\x08ar{X} = \\x0crac{506}{9} \\x07pprox 56.22Xˉ=9506≈56.22\\n2.\\tCalculate the Sum of Squares for Total (SST):\\nSST=∑(Xij−Xˉ)2SST = \\\\sum (X_{ij} - \\x08ar{X})^2SST=∑(Xij−Xˉ)2\\nCalculating each term:\\n(57−56.22)2=0.6084(55−56.22)2=1.4884(67−56.22)2=115.4884(49−56.22)2=52.9284(52−56.22)2=17.7884(68−56.22)2=138.2884(54−56.22)2=4.9284(46−56.22)2=104.0484(58−56.22)2=3.1684\\x08egin{align*} (57 - 56.22)^2 &= 0.6084 \\\\ (55 - 56.22)^2 &= 1.4884 \\\\ (67 - 56.22)^2 &= 115.4884 \\\\ (49 - 56.22)^2 &= 52.9284 \\\\ (52 - 56.22)^2 &= 17.7884 \\\\ (68 - 56.22)^2 &= 138.2884 \\\\ (54 - 56.22)^2 &= 4.9284 \\\\ (46 - 56.22)^2 &= 104.0484 \\\\ (58 - 56.22)^2 &= 3.1684 \\\\ \\\\end{align*}(57−56.22)2(55−56.22)2(67−56.22)2(49−56.22)2(52−56.22)2(68−56.22)2(54−56.22)2(46−56.22)2(58−56.22)2=0.6084=1.4884=115.4884=52.9284=17.7884=138.2884=4.9284=104.0484=3.1684\\nSumming these values:\\nSST=438.724SST = 438.724SST=438.724\\n3.\\tCalculate the Sum of Squares for Factor A (SSA) (Detergent effect):\\nSSA=∑(sum of each row)2number of observations per row−(∑Xij)2NSSA = \\\\sum \\x0crac{(\\text{sum of each row})^2}{\\text{number of observations per row}} - \\x0crac{(\\\\sum X_{ij})^2}{N}SSA=∑number of observations per row(sum of each row)2−N(∑Xij)2\\nSum for each detergent:\\nSum for Detergent A=57+49+54=160Sum for Detergent B=55+52+46=153Sum for Detergent C=67+68+58=193\\x08egin{align*} \\text{Sum for Detergent A} &= 57 + 49 + 54 = 160 \\\\ \\text{Sum for Detergent B} &= 55 + 52 + 46 = 153 \\\\ \\text{Sum for Detergent C} &= 67 + 68 + 58 = 193 \\\\ \\\\end{align*}Sum for Detergent ASum for Detergent BSum for Detergent C=57+49+54=160=55+52+46=153=67+68+58=193\\n\\n1.\\tCalculate SSA:\\nSSA=(16023+15323+19323)−(506)29SSA = \\\\left(\\x0crac{160^2}{3} + \\x0crac{153^2}{3} + \\x0crac{193^2}{3}\\right) - \\x0crac{(506)^2}{9}SSA=(31602+31532+31932)−9(506)2 SSA=(8533.33+7809+12441.67)−2560369SSA = (8533.33 + 7809 + 12441.67) - \\x0crac{256036}{9}SSA=(8533.33+7809+12441.67)−9256036 SSA=28784−28448.44=335.56SSA = 28784 - 28448.44 = 335.56SSA=28784−28448.44=335.56\\n2.\\tCalculate the Sum of Squares for Factor B (SSB) (Temperature effect):\\nSum for each temperature:\\nSum for Cold Water=57+55+67=179Sum for Warm Water=49+52+68=169Sum for Hot Water=54+46+58=158\\x08egin{align*} \\text{Sum for Cold Water} &= 57 + 55 + 67 = 179 \\\\ \\text{Sum for Warm Water} &= 49 + 52 + 68 = 169 \\\\ \\text{Sum for Hot Water} &= 54 + 46 + 58 = 158 \\\\ \\\\end{align*}Sum for Cold WaterSum for Warm WaterSum for Hot Water=57+55+67=179=49+52+68=169=54+46+58=158\\nCalculating each term:\\n17923=10681.6716923=9521.6715823=8326.67\\x08egin{align*} \\x0crac{179^2}{3} &= 10681.67 \\\\ \\x0crac{169^2}{3} &= 9521.67 \\\\ \\x0crac{158^2}{3} &= 8326.67 \\\\ \\\\end{align*}317923169231582=10681.67=9521.67=8326.67 SSB=(17923+16923+15823)−(506)29SSB = \\\\left(\\x0crac{179^2}{3} + \\x0crac{169^2}{3} + \\x0crac{158^2}{3}\\right) - \\x0crac{(506)^2}{9}SSB=(31792+31692+31582)−9(506)2 SSB=(10681.67+9521.67+8326.67)−28448.44SSB = (10681.67 + 9521.67 + 8326.67) - 28448.44SSB=(10681.67+9521.67+8326.67)−28448.44 SSB=28530.01−28448.44=81.57SSB = 28530.01 - 28448.44 = 81.57SSB=28530.01−28448.44=81.57\\n3.\\tCalculate the Sum of Squares for Error (SSE):\\nSSE=SST−SSA−SSBSSE = SST - SSA - SSBSSE=SST−SSA−SSB SSE=438.724−335.56−81.57=21.594SSE = 438.724 - 335.56 - 81.57 = 21.594SSE=438.724−335.56−81.57=21.594\\n4.\\tCalculate the Degrees of Freedom:\\no\\tDegrees of freedom for Factor A (dfA): k−1k - 1k−1, where kkk is the number of levels of Factor A (detergents).\\ndfA=3−1=2dfA = 3 - 1 = 2dfA=3−1=2\\no\\tDegrees of freedom for Factor B (dfB): m−1m - 1m−1, where mmm is the number of levels of Factor B (temperatures).\\ndfB=3−1=2dfB = 3 - 1 = 2dfB=3−1=2\\no\\tDegrees of freedom for Error (dfE): (k−1)(m−1)(k-1)(m-1)(k−1)(m−1)\\ndfE=(3−1)(3−1)=4dfE = (3 - 1)(3 - 1) = 4dfE=(3−1)(3−1)=4\\n5.\\tCalculate the Mean Squares:\\no\\tMean Square for Factor A (MSA):\\nMSA=SSAdfA=335.562=167.78MSA = \\x0crac{SSA}{dfA} = \\x0crac{335.56}{2} = 167.78MSA=dfASSA=2335.56=167.78\\no\\tMean Square for Factor B (MSB):\\nMSB=SSBdfB=81.572=40.785MSB = \\x0crac{SSB}{dfB} = \\x0crac{81.57}{2} = 40.785MSB=dfBSSB=281.57=40.785\\no\\tMean Square for Error (MSE):\\nMSE=SSEdfE=21.5944=5.3985MSE = \\x0crac{SSE}{dfE} = \\x0crac{21.594}{4} = 5.3985MSE=dfESSE=421.594=5.3985\\n6.\\tCalculate the F-statistic:\\no\\tF-statistic for Factor A:\\nFA=MSAMSE=167.785.3985≈31.08F_A = \\x0crac{MSA}{MSE} = \\x0crac{167.78}{5.3985} \\x07pprox 31.08FA=MSEMSA=5.3985167.78≈31.08\\no\\tF-statistic for Factor B:\\nFB=MSBMSE=40.7855.3985≈7.56F_B = \\x0crac{MSB}{MSE} = \\x0crac{40.785}{5.3985} \\x07pprox 7.56FB=MSEMSB=5.398540.785≈7.56\\n7.\\tDetermine the Critical Value and Compare: Using the F-distribution table, we find the critical values at a significance level of 0.05 for df1=2df_1 = 2df1=2 and df2=4df_2 = 4df2=4:\\no\\tCritical value for Factor A: FcritA(0.05,2,4)≈6.94F_{critA}(0.05, 2, 4) \\x07pprox 6.94FcritA(0.05,2,4)≈6.94\\no\\tCritical value for Factor B: FcritB(0.05,2,4)≈6.94F_{critB}(0.05, 2, 4) \\x07pprox 6.94FcritB(0.05,2,4)≈6.94\\nCompare the calculated F-values with the critical values:\\no\\tFor Factor A: FA=31.08>6.94F_A = 31.08 > 6.94FA=31.08>6.94\\no\\tFor Factor B: FB=7.56>6.94F_B = 7.56 > 6.94FB=7.56>6.94\\nConclusion\\nSince both F-values are greater than the critical values, we reject the null hypothesis for both detergents and temperatures. This indicates that there are significant differences in the performance of the detergents and the effect of temperature on the performance.\\n4o\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To study the performance of three detergents (A, B, and C) at three different water temperatures (Cold, Warm, and Hot) using an ANOVA (Analysis of Variance) test, we need to determine if there are significant differences between the means of the whiteness readings for the different detergents and temperatures. Here, we are dealing with a two-way ANOVA without replication because we have one observation for each combination of detergent and temperature.\n",
    "Data\n",
    "Let's summarize the data from the table:\n",
    "Water Temp\tDetergent A\tDetergent B\tDetergent C\n",
    "Cold\t57\t55\t67\n",
    "Warm\t49\t52\t68\n",
    "Hot\t54\t46\t58\n",
    "Steps to Perform Two-Way ANOVA\n",
    "1.\tCalculate the Grand Mean (Xˉ\\bar{X}Xˉ):\n",
    "Xˉ=∑XijN\\bar{X} = \\frac{\\sum X_{ij}}{N}Xˉ=N∑Xij\n",
    "where NNN is the total number of observations.\n",
    "Total of all observations:\n",
    "57+55+67+49+52+68+54+46+58=50657 + 55 + 67 + 49 + 52 + 68 + 54 + 46 + 58 = 50657+55+67+49+52+68+54+46+58=506\n",
    "Grand Mean (Xˉ\\bar{X}Xˉ):\n",
    "Xˉ=5069≈56.22\\bar{X} = \\frac{506}{9} \\approx 56.22Xˉ=9506≈56.22\n",
    "2.\tCalculate the Sum of Squares for Total (SST):\n",
    "SST=∑(Xij−Xˉ)2SST = \\sum (X_{ij} - \\bar{X})^2SST=∑(Xij−Xˉ)2\n",
    "Calculating each term:\n",
    "(57−56.22)2=0.6084(55−56.22)2=1.4884(67−56.22)2=115.4884(49−56.22)2=52.9284(52−56.22)2=17.7884(68−56.22)2=138.2884(54−56.22)2=4.9284(46−56.22)2=104.0484(58−56.22)2=3.1684\\begin{align*} (57 - 56.22)^2 &= 0.6084 \\\\ (55 - 56.22)^2 &= 1.4884 \\\\ (67 - 56.22)^2 &= 115.4884 \\\\ (49 - 56.22)^2 &= 52.9284 \\\\ (52 - 56.22)^2 &= 17.7884 \\\\ (68 - 56.22)^2 &= 138.2884 \\\\ (54 - 56.22)^2 &= 4.9284 \\\\ (46 - 56.22)^2 &= 104.0484 \\\\ (58 - 56.22)^2 &= 3.1684 \\\\ \\end{align*}(57−56.22)2(55−56.22)2(67−56.22)2(49−56.22)2(52−56.22)2(68−56.22)2(54−56.22)2(46−56.22)2(58−56.22)2=0.6084=1.4884=115.4884=52.9284=17.7884=138.2884=4.9284=104.0484=3.1684\n",
    "Summing these values:\n",
    "SST=438.724SST = 438.724SST=438.724\n",
    "3.\tCalculate the Sum of Squares for Factor A (SSA) (Detergent effect):\n",
    "SSA=∑(sum of each row)2number of observations per row−(∑Xij)2NSSA = \\sum \\frac{(\\text{sum of each row})^2}{\\text{number of observations per row}} - \\frac{(\\sum X_{ij})^2}{N}SSA=∑number of observations per row(sum of each row)2−N(∑Xij)2\n",
    "Sum for each detergent:\n",
    "Sum for Detergent A=57+49+54=160Sum for Detergent B=55+52+46=153Sum for Detergent C=67+68+58=193\\begin{align*} \\text{Sum for Detergent A} &= 57 + 49 + 54 = 160 \\\\ \\text{Sum for Detergent B} &= 55 + 52 + 46 = 153 \\\\ \\text{Sum for Detergent C} &= 67 + 68 + 58 = 193 \\\\ \\end{align*}Sum for Detergent ASum for Detergent BSum for Detergent C=57+49+54=160=55+52+46=153=67+68+58=193\n",
    "\n",
    "1.\tCalculate SSA:\n",
    "SSA=(16023+15323+19323)−(506)29SSA = \\left(\\frac{160^2}{3} + \\frac{153^2}{3} + \\frac{193^2}{3}\\right) - \\frac{(506)^2}{9}SSA=(31602+31532+31932)−9(506)2 SSA=(8533.33+7809+12441.67)−2560369SSA = (8533.33 + 7809 + 12441.67) - \\frac{256036}{9}SSA=(8533.33+7809+12441.67)−9256036 SSA=28784−28448.44=335.56SSA = 28784 - 28448.44 = 335.56SSA=28784−28448.44=335.56\n",
    "2.\tCalculate the Sum of Squares for Factor B (SSB) (Temperature effect):\n",
    "Sum for each temperature:\n",
    "Sum for Cold Water=57+55+67=179Sum for Warm Water=49+52+68=169Sum for Hot Water=54+46+58=158\\begin{align*} \\text{Sum for Cold Water} &= 57 + 55 + 67 = 179 \\\\ \\text{Sum for Warm Water} &= 49 + 52 + 68 = 169 \\\\ \\text{Sum for Hot Water} &= 54 + 46 + 58 = 158 \\\\ \\end{align*}Sum for Cold WaterSum for Warm WaterSum for Hot Water=57+55+67=179=49+52+68=169=54+46+58=158\n",
    "Calculating each term:\n",
    "17923=10681.6716923=9521.6715823=8326.67\\begin{align*} \\frac{179^2}{3} &= 10681.67 \\\\ \\frac{169^2}{3} &= 9521.67 \\\\ \\frac{158^2}{3} &= 8326.67 \\\\ \\end{align*}317923169231582=10681.67=9521.67=8326.67 SSB=(17923+16923+15823)−(506)29SSB = \\left(\\frac{179^2}{3} + \\frac{169^2}{3} + \\frac{158^2}{3}\\right) - \\frac{(506)^2}{9}SSB=(31792+31692+31582)−9(506)2 SSB=(10681.67+9521.67+8326.67)−28448.44SSB = (10681.67 + 9521.67 + 8326.67) - 28448.44SSB=(10681.67+9521.67+8326.67)−28448.44 SSB=28530.01−28448.44=81.57SSB = 28530.01 - 28448.44 = 81.57SSB=28530.01−28448.44=81.57\n",
    "3.\tCalculate the Sum of Squares for Error (SSE):\n",
    "SSE=SST−SSA−SSBSSE = SST - SSA - SSBSSE=SST−SSA−SSB SSE=438.724−335.56−81.57=21.594SSE = 438.724 - 335.56 - 81.57 = 21.594SSE=438.724−335.56−81.57=21.594\n",
    "4.\tCalculate the Degrees of Freedom:\n",
    "o\tDegrees of freedom for Factor A (dfA): k−1k - 1k−1, where kkk is the number of levels of Factor A (detergents).\n",
    "dfA=3−1=2dfA = 3 - 1 = 2dfA=3−1=2\n",
    "o\tDegrees of freedom for Factor B (dfB): m−1m - 1m−1, where mmm is the number of levels of Factor B (temperatures).\n",
    "dfB=3−1=2dfB = 3 - 1 = 2dfB=3−1=2\n",
    "o\tDegrees of freedom for Error (dfE): (k−1)(m−1)(k-1)(m-1)(k−1)(m−1)\n",
    "dfE=(3−1)(3−1)=4dfE = (3 - 1)(3 - 1) = 4dfE=(3−1)(3−1)=4\n",
    "5.\tCalculate the Mean Squares:\n",
    "o\tMean Square for Factor A (MSA):\n",
    "MSA=SSAdfA=335.562=167.78MSA = \\frac{SSA}{dfA} = \\frac{335.56}{2} = 167.78MSA=dfASSA=2335.56=167.78\n",
    "o\tMean Square for Factor B (MSB):\n",
    "MSB=SSBdfB=81.572=40.785MSB = \\frac{SSB}{dfB} = \\frac{81.57}{2} = 40.785MSB=dfBSSB=281.57=40.785\n",
    "o\tMean Square for Error (MSE):\n",
    "MSE=SSEdfE=21.5944=5.3985MSE = \\frac{SSE}{dfE} = \\frac{21.594}{4} = 5.3985MSE=dfESSE=421.594=5.3985\n",
    "6.\tCalculate the F-statistic:\n",
    "o\tF-statistic for Factor A:\n",
    "FA=MSAMSE=167.785.3985≈31.08F_A = \\frac{MSA}{MSE} = \\frac{167.78}{5.3985} \\approx 31.08FA=MSEMSA=5.3985167.78≈31.08\n",
    "o\tF-statistic for Factor B:\n",
    "FB=MSBMSE=40.7855.3985≈7.56F_B = \\frac{MSB}{MSE} = \\frac{40.785}{5.3985} \\approx 7.56FB=MSEMSB=5.398540.785≈7.56\n",
    "7.\tDetermine the Critical Value and Compare: Using the F-distribution table, we find the critical values at a significance level of 0.05 for df1=2df_1 = 2df1=2 and df2=4df_2 = 4df2=4:\n",
    "o\tCritical value for Factor A: FcritA(0.05,2,4)≈6.94F_{critA}(0.05, 2, 4) \\approx 6.94FcritA(0.05,2,4)≈6.94\n",
    "o\tCritical value for Factor B: FcritB(0.05,2,4)≈6.94F_{critB}(0.05, 2, 4) \\approx 6.94FcritB(0.05,2,4)≈6.94\n",
    "Compare the calculated F-values with the critical values:\n",
    "o\tFor Factor A: FA=31.08>6.94F_A = 31.08 > 6.94FA=31.08>6.94\n",
    "o\tFor Factor B: FB=7.56>6.94F_B = 7.56 > 6.94FB=7.56>6.94\n",
    "Conclusion\n",
    "Since both F-values are greater than the critical values, we reject the null hypothesis for both detergents and temperatures. This indicates that there are significant differences in the performance of the detergents and the effect of temperature on the performance.\n",
    "4o\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc3ddc-cb78-48be-8c61-0695cf84cd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c265b14b-92eb-43e7-809e-49c1fa531048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#39\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838f472-ca0c-425f-b0e4-2415cfa630a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44341f11-c028-4bbe-8b29-e06546bd9694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3757382-47fb-4e7c-9041-22db8ddca6aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2436036356.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[82], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    <!doctype html>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Form Submission</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Submit Form</h1>\n",
    "    <form method=\"post\" action=\"/submit\">\n",
    "      <label for=\"name\">Name:</label>\n",
    "      <input type=\"text\" id=\"name\" name=\"name\" required>\n",
    "      <button type=\"submit\">Submit</button>\n",
    "    </form>\n",
    "  </body>\n",
    "</html>\n",
    "from flask import Flask, request, render_template\n",
    "\n",
    "app = Flask(_name_)\n",
    "\n",
    "@app.route('/form', methods=['GET'])\n",
    "def form():\n",
    "    return render_template('form.html')\n",
    "\n",
    "@app.route('/submit', methods=['POST'])\n",
    "def submit():\n",
    "    name = request.form['name']\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\n",
    "Explanation:\n",
    "@app.route('/form', methods=['GET']): Displays the form.\n",
    "@app.route('/submit', methods=['POST']): Handles form submission.\n",
    "request.form['name']: Retrieves form data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c87dff0f-f3af-4ca0-9270-d4a40eb55a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15287b6d-7428-4bd6-97b6-5c91650e5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Implementing User Authentication\\nTo implement user authentication in Flask, you can use the Flask-Login extension. Here’s a basic setup:\\n\\nInstall Flask-Login:\\n\\npip install flask-login\\nFlask App Example:\\nfrom flask import Flask, redirect, url_for, session, request\\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\\n\\napp = Flask(_name_)\\napp.secret_key = \\'supersecretkey\\'\\nlogin_manager = LoginManager()\\nlogin_manager.init_app(app)\\n\\n# Dummy user store\\nusers = {\\'user\\': {\\'password\\': \\'pass\\'}}\\n\\nclass User(UserMixin):\\n    def _init_(self, username):\\n        self.id = username\\n\\n@login_manager.user_loader\\ndef load_user(username):\\n    return User(username) if username in users else None\\n\\n@app.route(\\'/login\\', methods=[\\'GET\\', \\'POST\\'])\\ndef login():\\n    if request.method == \\'POST\\':\\n        username = request.form[\\'username\\']\\n        password = request.form[\\'password\\']\\n        if users.get(username, {}).get(\\'password\\') == password:\\n            user = User(username)\\n            login_user(user)\\n            return redirect(url_for(\\'profile\\'))\\n    return \\'\\'\\'\\n        <form method=\"post\">\\n            Username: <input type=\"text\" name=\"username\"><br>\\n            Password: <input type=\"password\" name=\"password\"><br>\\n            <input type=\"submit\" value=\"Login\">\\n        </form>\\n    \\'\\'\\'\\n\\n@app.route(\\'/profile\\')\\n@login_required\\ndef profile():\\n    return f\"Hello, {current_user.id}!\"\\n\\n@app.route(\\'/logout\\')\\n@login_required\\ndef logout():\\n    logout_user()\\n    return redirect(url_for(\\'login\\'))\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)\\nExplanation:\\nFlask-Login handles session management and user authentication.\\nUserMixin: A class provided by Flask-Login with default methods for user management.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Implementing User Authentication\n",
    "To implement user authentication in Flask, you can use the Flask-Login extension. Here’s a basic setup:\n",
    "\n",
    "Install Flask-Login:\n",
    "\n",
    "pip install flask-login\n",
    "Flask App Example:\n",
    "from flask import Flask, redirect, url_for, session, request\n",
    "from flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.secret_key = 'supersecretkey'\n",
    "login_manager = LoginManager()\n",
    "login_manager.init_app(app)\n",
    "\n",
    "# Dummy user store\n",
    "users = {'user': {'password': 'pass'}}\n",
    "\n",
    "class User(UserMixin):\n",
    "    def _init_(self, username):\n",
    "        self.id = username\n",
    "\n",
    "@login_manager.user_loader\n",
    "def load_user(username):\n",
    "    return User(username) if username in users else None\n",
    "\n",
    "@app.route('/login', methods=['GET', 'POST'])\n",
    "def login():\n",
    "    if request.method == 'POST':\n",
    "        username = request.form['username']\n",
    "        password = request.form['password']\n",
    "        if users.get(username, {}).get('password') == password:\n",
    "            user = User(username)\n",
    "            login_user(user)\n",
    "            return redirect(url_for('profile'))\n",
    "    return '''\n",
    "        <form method=\"post\">\n",
    "            Username: <input type=\"text\" name=\"username\"><br>\n",
    "            Password: <input type=\"password\" name=\"password\"><br>\n",
    "            <input type=\"submit\" value=\"Login\">\n",
    "        </form>\n",
    "    '''\n",
    "\n",
    "@app.route('/profile')\n",
    "@login_required\n",
    "def profile():\n",
    "    return f\"Hello, {current_user.id}!\"\n",
    "\n",
    "@app.route('/logout')\n",
    "@login_required\n",
    "def logout():\n",
    "    logout_user()\n",
    "    return redirect(url_for('login'))\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\n",
    "Explanation:\n",
    "Flask-Login handles session management and user authentication.\n",
    "UserMixin: A class provided by Flask-Login with default methods for user management.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a53f6eb-2a4f-43f0-a85c-3b7f6f2a3852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from flask import Flask, request, jsonify\\nfrom flask_sqlalchemy import SQLAlchemy\\n\\napp = Flask(_name_)\\napp.config[\\'SQLALCHEMY_DATABASE_URI\\'] = \\'sqlite:///example.db\\'\\ndb = SQLAlchemy(app)\\n\\nclass User(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    username = db.Column(db.String(80), unique=True, nullable=False)\\n\\n@app.before_first_request\\ndef create_tables():\\n    db.create_all()\\n\\n@app.route(\\'/add_user\\', methods=[\\'POST\\'])\\ndef add_user():\\n    username = request.form[\\'username\\']\\n    new_user = User(username=username)\\n    db.session.add(new_user)\\n    db.session.commit()\\n    return f\"User {username} added!\"\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from flask import Flask, request, jsonify\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///example.db'\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "class User(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    username = db.Column(db.String(80), unique=True, nullable=False)\n",
    "\n",
    "@app.before_first_request\n",
    "def create_tables():\n",
    "    db.create_all()\n",
    "\n",
    "@app.route('/add_user', methods=['POST'])\n",
    "def add_user():\n",
    "    username = request.form['username']\n",
    "    new_user = User(username=username)\n",
    "    db.session.add(new_user)\n",
    "    db.session.commit()\n",
    "    return f\"User {username} added!\"\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b0dd9a07-588d-458a-8672-0936ffd5a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/api/data')\n",
    "def data():\n",
    "    return jsonify({'message': 'Hello, World!', 'status': 'success'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ee621f8-bc30-45c7-bcd1-9a95d2ee3d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from flask import Flask, render_template, redirect, url_for\\nfrom flask_wtf import FlaskForm\\nfrom wtforms import StringField, SubmitField\\nfrom wtforms.validators import DataRequired\\n\\napp = Flask(_name_)\\napp.secret_key = \\'supersecretkey\\'\\n\\nclass MyForm(FlaskForm):\\n    name = StringField(\\'Name\\', validators=[DataRequired()])\\n    submit = SubmitField(\\'Submit\\')\\n\\n@app.route(\\'/form\\', methods=[\\'GET\\', \\'POST\\'])\\ndef form():\\n    form = MyForm()\\n    if form.validate_on_submit():\\n        return redirect(url_for(\\'success\\', name=form.name.data))\\n    return render_template(\\'form.html\\', form=form)\\n\\n@app.route(\\'/success/<name>\\')\\ndef success(name):\\n    return f\"Hello, {name}!\"\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)\\n<!doctype html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <title>Form Submission</title>\\n  </head>\\n  <body>\\n    <h1>Submit Form</h1>\\n    <form method=\"post\">\\n      {{ form.hidden_tag() }}\\n      <label for=\"name\">{{ form.name.label }}</label>\\n      {{ form.name() }}<br>\\n      {{ form.submit() }}\\n    </form>\\n  </body>\\n</html>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from flask import Flask, render_template, redirect, url_for\n",
    "from flask_wtf import FlaskForm\n",
    "from wtforms import StringField, SubmitField\n",
    "from wtforms.validators import DataRequired\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.secret_key = 'supersecretkey'\n",
    "\n",
    "class MyForm(FlaskForm):\n",
    "    name = StringField('Name', validators=[DataRequired()])\n",
    "    submit = SubmitField('Submit')\n",
    "\n",
    "@app.route('/form', methods=['GET', 'POST'])\n",
    "def form():\n",
    "    form = MyForm()\n",
    "    if form.validate_on_submit():\n",
    "        return redirect(url_for('success', name=form.name.data))\n",
    "    return render_template('form.html', form=form)\n",
    "\n",
    "@app.route('/success/<name>')\n",
    "def success(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Form Submission</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Submit Form</h1>\n",
    "    <form method=\"post\">\n",
    "      {{ form.hidden_tag() }}\n",
    "      <label for=\"name\">{{ form.name.label }}</label>\n",
    "      {{ form.name() }}<br>\n",
    "      {{ form.submit() }}\n",
    "    </form>\n",
    "  </body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "053eb3e9-a2ab-4d84-b272-af3c98c8dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <title>Upload File</title>\\n  </head>\\n  <body>\\n    <h1>Upload File</h1>\\n    <form method=\"post\" enctype=\"multipart/form-data\" action=\"/upload\">\\n      <input type=\"file\" name=\"file\" required>\\n      <input type=\"submit\" value=\"Upload\">\\n    </form>\\n  </body>\\n</html>\\nfrom flask import Flask, request, redirect, url_for, render_template\\nimport os\\n\\napp = Flask(_name_)\\napp.config[\\'UPLOAD_FOLDER\\'] = \\'uploads\\'\\napp.config[\\'MAX_CONTENT_LENGTH\\'] = 16 * 1024 * 1024  # Max size of 16MB\\n\\n# Ensure upload folder exists\\nos.makedirs(app.config[\\'UPLOAD_FOLDER\\'], exist_ok=True)\\n\\n@app.route(\\'/upload\\', methods=[\\'GET\\', \\'POST\\'])\\ndef upload_file():\\n    if request.method == \\'POST\\':\\n        if \\'file\\' not in request.files:\\n            return \\'No file part\\'\\n        file = request.files[\\'file\\']\\n        if file.filename == \\'\\':\\n            return \\'No selected file\\'\\n        if file:\\n            file_path = os.path.join(app.config[\\'UPLOAD_FOLDER\\'], file.filename)\\n            file.save(file_path)\\n            return f\"File uploaded successfully: {file.filename}\"\\n    return render_template(\\'upload.html\\')\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Upload File</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Upload File</h1>\n",
    "    <form method=\"post\" enctype=\"multipart/form-data\" action=\"/upload\">\n",
    "      <input type=\"file\" name=\"file\" required>\n",
    "      <input type=\"submit\" value=\"Upload\">\n",
    "    </form>\n",
    "  </body>\n",
    "</html>\n",
    "from flask import Flask, request, redirect, url_for, render_template\n",
    "import os\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
    "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Max size of 16MB\n",
    "\n",
    "# Ensure upload folder exists\n",
    "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files:\n",
    "            return 'No file part'\n",
    "        file = request.files['file']\n",
    "        if file.filename == '':\n",
    "            return 'No selected file'\n",
    "        if file:\n",
    "            file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "            file.save(file_path)\n",
    "            return f\"File uploaded successfully: {file.filename}\"\n",
    "    return render_template('upload.html')\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22dc9406-98ba-4116-9e3c-b2ab885a5d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"myapp/\\n  ├── app.py\\n  ├── auth/\\n  │   ├── _init_.py\\n  │   ├── routes.py\\n  └── templates/\\n      └── login.html\\nfrom flask import Blueprint, render_template\\n\\nauth = Blueprint('auth', _name_)\\n\\n@auth.route('/login')\\ndef login():\\n    return render_template('login.html')\\nfrom flask import Blueprint\\n\\ndef create_auth_blueprint():\\n    from .routes import auth\\n    return auth\\nfrom flask import Flask\\nfrom auth import create_auth_blueprint\\n\\napp = Flask(_name_)\\napp.register_blueprint(create_auth_blueprint(), url_prefix='/auth')\\n\\n@app.route('/')\\ndef home():\\n    return 'Welcome to the Home Page!'\\n\\nif _name_ == '_main_':\\n    app.run(debug=True)\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"myapp/\n",
    "  ├── app.py\n",
    "  ├── auth/\n",
    "  │   ├── _init_.py\n",
    "  │   ├── routes.py\n",
    "  └── templates/\n",
    "      └── login.html\n",
    "from flask import Blueprint, render_template\n",
    "\n",
    "auth = Blueprint('auth', _name_)\n",
    "\n",
    "@auth.route('/login')\n",
    "def login():\n",
    "    return render_template('login.html')\n",
    "from flask import Blueprint\n",
    "\n",
    "def create_auth_blueprint():\n",
    "    from .routes import auth\n",
    "    return auth\n",
    "from flask import Flask\n",
    "from auth import create_auth_blueprint\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.register_blueprint(create_auth_blueprint(), url_prefix='/auth')\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return 'Welcome to the Home Page!'\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8eee34ee-a74b-48a3-9d49-97afd394c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Configure Nginx:\\nCreate a configuration file for Nginx (e.g., /etc/nginx/sites-available/myapp):\\n\\nserver {\\n    listen 80;\\n    server_name your_domain_or_IP;\\n\\n    location / {\\n        proxy_pass http://127.0.0.1:8000;\\n        proxy_set_header Host $host;\\n        proxy_set_header X-Real-IP $remote_addr;\\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n        proxy_set_header X-Forwarded-Proto $scheme;\\n    }\\n}\\n4. Enable Nginx Configuration:\\nsudo ln -s /etc/nginx/sites-available/myapp /etc/nginx/sites-enabled\\nsudo nginx -t\\nsudo systemctl restart nginx\\nExplanation:\\nGunicorn: A WSGI server for serving the Flask application.\\nNginx: Acts as a reverse proxy and handles incoming web traffic.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Configure Nginx:\n",
    "Create a configuration file for Nginx (e.g., /etc/nginx/sites-available/myapp):\n",
    "\n",
    "server {\n",
    "    listen 80;\n",
    "    server_name your_domain_or_IP;\n",
    "\n",
    "    location / {\n",
    "        proxy_pass http://127.0.0.1:8000;\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "        proxy_set_header X-Forwarded-Proto $scheme;\n",
    "    }\n",
    "}\n",
    "4. Enable Nginx Configuration:\n",
    "sudo ln -s /etc/nginx/sites-available/myapp /etc/nginx/sites-enabled\n",
    "sudo nginx -t\n",
    "sudo systemctl restart nginx\n",
    "Explanation:\n",
    "Gunicorn: A WSGI server for serving the Flask application.\n",
    "Nginx: Acts as a reverse proxy and handles incoming web traffic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a86f46e8-6009-47fb-a4c0-cf9405e19089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from flask import Flask, render_template, request, redirect, url_for, session\\nfrom flask_wtf import FlaskForm\\nfrom wtforms import StringField, PasswordField, SubmitField\\nfrom wtforms.validators import DataRequired\\nfrom pymongo import MongoClient\\nimport bcrypt\\n\\napp = Flask(_name_)\\napp.secret_key = \\'supersecretkey\\'\\n\\n# MongoDB setup\\nclient = MongoClient(\\'mongodb://localhost:27017/\\')\\ndb = client[\\'mydatabase\\']\\nusers_collection = db[\\'users\\']\\n\\n# Forms\\nclass SignupForm(FlaskForm):\\n    username = StringField(\\'Username\\', validators=[DataRequired()])\\n    password = PasswordField(\\'Password\\', validators=[DataRequired()])\\n    submit = SubmitField(\\'Sign Up\\')\\n\\nclass SigninForm(FlaskForm):\\n    username = StringField(\\'Username\\', validators=[DataRequired()])\\n    password = PasswordField(\\'Password\\', validators=[DataRequired()])\\n    submit = SubmitField(\\'Sign In\\')\\n\\n@app.route(\\'/signup\\', methods=[\\'GET\\', \\'POST\\'])\\ndef signup():\\n    form = SignupForm()\\n    if form.validate_on_submit():\\n        username = form.username.data\\n        password = form.password.data\\n        hashed_password = bcrypt.hashpw(password.encode(\\'utf-8\\'), bcrypt.gensalt())\\n        users_collection.insert_one({\\'username\\': username, \\'password\\': hashed_password})\\n        return redirect(url_for(\\'signin\\'))\\n    return render_template(\\'signup.html\\', form=form)\\n\\n@app.route(\\'/signin\\', methods=[\\'GET\\', \\'POST\\'])\\ndef signin():\\n    form = SigninForm()\\n    if form.validate_on_submit():\\n        username = form.username.data\\n        password = form.password.data\\n        user = users_collection.find_one({\\'username\\': username})\\n        if user and bcrypt.checkpw(password.encode(\\'utf-8\\'), user[\\'password\\']):\\n            session[\\'username\\'] = username\\n            return redirect(url_for(\\'welcome\\'))\\n        return \\'Invalid username or password\\'\\n    return render_template(\\'signin.html\\', form=form)\\n\\n@app.route(\\'/welcome\\')\\ndef welcome():\\n    if \\'username\\' in session:\\n        return f\"Hello, {session[\\'username\\']}!\"\\n    return redirect(url_for(\\'signin\\'))\\n\\nif _name_ == \\'_main_\\':\\n    app.run(debug=True)\\n<!doctype html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <title>Sign Up</title>\\n  </head>\\n  <body>\\n    <h1>Sign Up</h1>\\n    <form method=\"post\">\\n      {{ form.hidden_tag() }}\\n      <label for=\"username\">{{ form.username.label }}</label>\\n      {{ form.username() }}<br>\\n      <label for=\"password\">{{ form.password.label }}</label>\\n      {{ form.password() }}<br>\\n      {{ form.submit() }}\\n    </form>\\n  </body>\\n</html>\\n<!doctype html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <title>Sign In</title>\\n  </head>\\n  <body>\\n    <h1>Sign In</h1>\\n    <form method=\"post\">\\n      {{ form.hidden_tag() }}\\n      <label for=\"username\">{{ form.username.label }}</label>\\n      {{ form.username() }}<br>\\n      <label for=\"password\">{{ form.password.label }}</label>\\n      {{ form.password() }}<br>\\n      {{ form.submit() }}\\n    </form>\\n  </body>\\n</html>\\nExplanation:\\nMongoDB Setup: Connect to MongoDB and define a collection for users.\\nForms: Create forms for sign-up and sign-in using Flask-WTF.\\nPassword Handling: Use bcrypt for securely hashing and verifying passwords.\\nSessions: Use Flask sessions to manage user login state.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from flask import Flask, render_template, request, redirect, url_for, session\n",
    "from flask_wtf import FlaskForm\n",
    "from wtforms import StringField, PasswordField, SubmitField\n",
    "from wtforms.validators import DataRequired\n",
    "from pymongo import MongoClient\n",
    "import bcrypt\n",
    "\n",
    "app = Flask(_name_)\n",
    "app.secret_key = 'supersecretkey'\n",
    "\n",
    "# MongoDB setup\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "users_collection = db['users']\n",
    "\n",
    "# Forms\n",
    "class SignupForm(FlaskForm):\n",
    "    username = StringField('Username', validators=[DataRequired()])\n",
    "    password = PasswordField('Password', validators=[DataRequired()])\n",
    "    submit = SubmitField('Sign Up')\n",
    "\n",
    "class SigninForm(FlaskForm):\n",
    "    username = StringField('Username', validators=[DataRequired()])\n",
    "    password = PasswordField('Password', validators=[DataRequired()])\n",
    "    submit = SubmitField('Sign In')\n",
    "\n",
    "@app.route('/signup', methods=['GET', 'POST'])\n",
    "def signup():\n",
    "    form = SignupForm()\n",
    "    if form.validate_on_submit():\n",
    "        username = form.username.data\n",
    "        password = form.password.data\n",
    "        hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
    "        users_collection.insert_one({'username': username, 'password': hashed_password})\n",
    "        return redirect(url_for('signin'))\n",
    "    return render_template('signup.html', form=form)\n",
    "\n",
    "@app.route('/signin', methods=['GET', 'POST'])\n",
    "def signin():\n",
    "    form = SigninForm()\n",
    "    if form.validate_on_submit():\n",
    "        username = form.username.data\n",
    "        password = form.password.data\n",
    "        user = users_collection.find_one({'username': username})\n",
    "        if user and bcrypt.checkpw(password.encode('utf-8'), user['password']):\n",
    "            session['username'] = username\n",
    "            return redirect(url_for('welcome'))\n",
    "        return 'Invalid username or password'\n",
    "    return render_template('signin.html', form=form)\n",
    "\n",
    "@app.route('/welcome')\n",
    "def welcome():\n",
    "    if 'username' in session:\n",
    "        return f\"Hello, {session['username']}!\"\n",
    "    return redirect(url_for('signin'))\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Sign Up</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Sign Up</h1>\n",
    "    <form method=\"post\">\n",
    "      {{ form.hidden_tag() }}\n",
    "      <label for=\"username\">{{ form.username.label }}</label>\n",
    "      {{ form.username() }}<br>\n",
    "      <label for=\"password\">{{ form.password.label }}</label>\n",
    "      {{ form.password() }}<br>\n",
    "      {{ form.submit() }}\n",
    "    </form>\n",
    "  </body>\n",
    "</html>\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Sign In</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Sign In</h1>\n",
    "    <form method=\"post\">\n",
    "      {{ form.hidden_tag() }}\n",
    "      <label for=\"username\">{{ form.username.label }}</label>\n",
    "      {{ form.username() }}<br>\n",
    "      <label for=\"password\">{{ form.password.label }}</label>\n",
    "      {{ form.password() }}<br>\n",
    "      {{ form.submit() }}\n",
    "    </form>\n",
    "  </body>\n",
    "</html>\n",
    "Explanation:\n",
    "MongoDB Setup: Connect to MongoDB and define a collection for users.\n",
    "Forms: Create forms for sign-up and sign-in using Flask-WTF.\n",
    "Password Handling: Use bcrypt for securely hashing and verifying passwords.\n",
    "Sessions: Use Flask sessions to manage user login state.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e291a06-3014-4d20-89f8-34511af0156c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Difference Between Series and DataFrames\\nPandas Series:\\n\\nDefinition: A one-dimensional labeled array capable of holding any data type.\\nUsage: Ideal for representing a single column of data.\\nExample:\\nimport pandas as pd\\n\\ndata = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])\\nprint(data)\\nOutput:\\ncss\\nCopy code\\nA    10\\nB    20\\nC    30\\nD    40\\ndtype: int64\\nPandas DataFrame:\\n\\nDefinition: A two-dimensional labeled data structure with columns of potentially different types.\\nUsage: Ideal for representing tabular data with multiple columns.\\nExample:\\nimport pandas as pd\\n\\ndata = pd.DataFrame({\\n    'A': [10, 20, 30, 40],\\n    'B': [50, 60, 70, 80]\\n}, index=['Row1', 'Row2', 'Row3', 'Row4'])\\nprint(data)\\nOutput:\\ncss\\n\\n       A   B\\nRow1  10  50\\nRow2  20  60\\nRow3  30  70\\nRow4  40  80\\n2. Create a MySQL Database and Table, and Read Using Pandas\\nCreate Database and Table:\\n\\nCreate the database:\\n\\nsql\\n\\nCREATE DATABASE Travel_Planner;\\nCreate the table:\\n\\n\\nUSE Travel_Planner;\\n\\nCREATE TABLE bookings (\\n    user_id INT,\\n    flight_id INT,\\n    hotel_id INT,\\n    activity_id INT,\\n    booking_date DATE\\n);\\nInsert dummy values:\\n\\nINSERT INTO bookings (user_id, flight_id, hotel_id, activity_id, booking_date) VALUES\\n(1, 101, 201, 301, '2024-07-01'),\\n(2, 102, 202, 302, '2024-07-02'),\\n(3, 103, 203, 303, '2024-07-03');\\nRead Table Content Using Pandas:\\n\\nInstall necessary packages:\\n\\npip install pandas sqlalchemy mysql-connector-python\\nPython code to read the table:\\n\\npimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\n# Create an engine\\nengine = create_engine('mysql+mysqlconnector://username:password@localhost/Travel_Planner')\\n\\n# Read table into DataFrame\\ndf = pd.read_sql('bookings', con=engine)\\n\\nprint(df)\\nExplanation:\\ncreate_engine: Creates a connection to the MySQL database.\\npd.read_sql: Reads the SQL table into a Pandas DataFrame.\\n3. Difference Between loc and iloc\\nloc:\\n\\nDefinition: Label-based indexing. Accesses a group of rows and columns by labels or a boolean array.\\nUsage: Useful when you want to access data by row/column labels.\\nExample:\\ndf.loc[0, 'A']  # Accesses the value at the first row, column 'A'\\niloc:\\n\\nDefinition: Integer-location based indexing. Accesses a group of rows and columns by integer positions.\\nUsage: Useful when you want to access data by row/column positions.\\nExample:\\npython\\nCopy code\\ndf.iloc[0, 0]  # Accesses the value at the first row, first column\\n4. Difference Between Supervised and Unsupervised Learning\\nSupervised Learning:\\n\\nDefinition: Learning from labeled data where the model is trained on input-output pairs.\\nUsage: Classification, regression.\\nExample: Predicting house prices based on historical data.\\nUnsupervised Learning:\\n\\nDefinition: Learning from unlabeled data to find hidden patterns or intrinsic structures.\\nUsage: Clustering, dimensionality reduction.\\nExample: Grouping customers into segments based on purchasing behavior.\\n5. Bias-Variance Tradeoff\\nBias:\\n\\nDefinition: Error due to overly simplistic models that fail to capture the underlying patterns in the data.\\nEffect: High bias leads to underfitting.\\nVariance:\\n\\nDefinition: Error due to models that are too complex and sensitive to the fluctuations in the training data.\\nEffect: High variance leads to overfitting.\\nTradeoff:\\n\\nExplanation: Increasing model complexity (lower bias) can lead to higher variance, while reducing model complexity (lower variance) can increase bias. The goal is to find a balance where both bias and variance are minimized.\\n6. Precision and Recall\\nPrecision:\\n\\nDefinition: The proportion of true positive results among the positive predictions made.\\nFormula: Precision = TP / (TP + FP)\\nRecall:\\n\\nDefinition: The proportion of true positive results among all the actual positives.\\nFormula: Recall = TP / (TP + FN)\\nDifference from Accuracy:\\n\\nAccuracy: The proportion of all correct predictions (both positive and negative) among all predictions.\\nFormula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\\n7. Overfitting and How to Prevent It\\nOverfitting:\\n\\nDefinition: When a model learns the details and noise in the training data to the extent that it performs poorly on new, unseen data.\\nCause: Model complexity is too high relative to the amount of training data.\\nPrevention:\\n\\nRegularization: Techniques like L1/L2 regularization to penalize large coefficients.\\nCross-Validation: Splitting data into training and validation sets to evaluate model performance.\\nPruning: Reducing the complexity of the model (e.g., reducing the number of layers in a neural network).\\nEarly Stopping: Stopping training when performance on the validation set starts to degrade.\\nThis comprehensive guide covers essential aspects of data manipulation, machine learning concepts, and practical tasks using Flask and Pandas. If you need further details or code examples, feel free to ask!\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"1. Difference Between Series and DataFrames\n",
    "Pandas Series:\n",
    "\n",
    "Definition: A one-dimensional labeled array capable of holding any data type.\n",
    "Usage: Ideal for representing a single column of data.\n",
    "Example:\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])\n",
    "print(data)\n",
    "Output:\n",
    "css\n",
    "Copy code\n",
    "A    10\n",
    "B    20\n",
    "C    30\n",
    "D    40\n",
    "dtype: int64\n",
    "Pandas DataFrame:\n",
    "\n",
    "Definition: A two-dimensional labeled data structure with columns of potentially different types.\n",
    "Usage: Ideal for representing tabular data with multiple columns.\n",
    "Example:\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'A': [10, 20, 30, 40],\n",
    "    'B': [50, 60, 70, 80]\n",
    "}, index=['Row1', 'Row2', 'Row3', 'Row4'])\n",
    "print(data)\n",
    "Output:\n",
    "css\n",
    "\n",
    "       A   B\n",
    "Row1  10  50\n",
    "Row2  20  60\n",
    "Row3  30  70\n",
    "Row4  40  80\n",
    "2. Create a MySQL Database and Table, and Read Using Pandas\n",
    "Create Database and Table:\n",
    "\n",
    "Create the database:\n",
    "\n",
    "sql\n",
    "\n",
    "CREATE DATABASE Travel_Planner;\n",
    "Create the table:\n",
    "\n",
    "\n",
    "USE Travel_Planner;\n",
    "\n",
    "CREATE TABLE bookings (\n",
    "    user_id INT,\n",
    "    flight_id INT,\n",
    "    hotel_id INT,\n",
    "    activity_id INT,\n",
    "    booking_date DATE\n",
    ");\n",
    "Insert dummy values:\n",
    "\n",
    "INSERT INTO bookings (user_id, flight_id, hotel_id, activity_id, booking_date) VALUES\n",
    "(1, 101, 201, 301, '2024-07-01'),\n",
    "(2, 102, 202, 302, '2024-07-02'),\n",
    "(3, 103, 203, 303, '2024-07-03');\n",
    "Read Table Content Using Pandas:\n",
    "\n",
    "Install necessary packages:\n",
    "\n",
    "pip install pandas sqlalchemy mysql-connector-python\n",
    "Python code to read the table:\n",
    "\n",
    "pimport pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine\n",
    "engine = create_engine('mysql+mysqlconnector://username:password@localhost/Travel_Planner')\n",
    "\n",
    "# Read table into DataFrame\n",
    "df = pd.read_sql('bookings', con=engine)\n",
    "\n",
    "print(df)\n",
    "Explanation:\n",
    "create_engine: Creates a connection to the MySQL database.\n",
    "pd.read_sql: Reads the SQL table into a Pandas DataFrame.\n",
    "3. Difference Between loc and iloc\n",
    "loc:\n",
    "\n",
    "Definition: Label-based indexing. Accesses a group of rows and columns by labels or a boolean array.\n",
    "Usage: Useful when you want to access data by row/column labels.\n",
    "Example:\n",
    "df.loc[0, 'A']  # Accesses the value at the first row, column 'A'\n",
    "iloc:\n",
    "\n",
    "Definition: Integer-location based indexing. Accesses a group of rows and columns by integer positions.\n",
    "Usage: Useful when you want to access data by row/column positions.\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "df.iloc[0, 0]  # Accesses the value at the first row, first column\n",
    "4. Difference Between Supervised and Unsupervised Learning\n",
    "Supervised Learning:\n",
    "\n",
    "Definition: Learning from labeled data where the model is trained on input-output pairs.\n",
    "Usage: Classification, regression.\n",
    "Example: Predicting house prices based on historical data.\n",
    "Unsupervised Learning:\n",
    "\n",
    "Definition: Learning from unlabeled data to find hidden patterns or intrinsic structures.\n",
    "Usage: Clustering, dimensionality reduction.\n",
    "Example: Grouping customers into segments based on purchasing behavior.\n",
    "5. Bias-Variance Tradeoff\n",
    "Bias:\n",
    "\n",
    "Definition: Error due to overly simplistic models that fail to capture the underlying patterns in the data.\n",
    "Effect: High bias leads to underfitting.\n",
    "Variance:\n",
    "\n",
    "Definition: Error due to models that are too complex and sensitive to the fluctuations in the training data.\n",
    "Effect: High variance leads to overfitting.\n",
    "Tradeoff:\n",
    "\n",
    "Explanation: Increasing model complexity (lower bias) can lead to higher variance, while reducing model complexity (lower variance) can increase bias. The goal is to find a balance where both bias and variance are minimized.\n",
    "6. Precision and Recall\n",
    "Precision:\n",
    "\n",
    "Definition: The proportion of true positive results among the positive predictions made.\n",
    "Formula: Precision = TP / (TP + FP)\n",
    "Recall:\n",
    "\n",
    "Definition: The proportion of true positive results among all the actual positives.\n",
    "Formula: Recall = TP / (TP + FN)\n",
    "Difference from Accuracy:\n",
    "\n",
    "Accuracy: The proportion of all correct predictions (both positive and negative) among all predictions.\n",
    "Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "7. Overfitting and How to Prevent It\n",
    "Overfitting:\n",
    "\n",
    "Definition: When a model learns the details and noise in the training data to the extent that it performs poorly on new, unseen data.\n",
    "Cause: Model complexity is too high relative to the amount of training data.\n",
    "Prevention:\n",
    "\n",
    "Regularization: Techniques like L1/L2 regularization to penalize large coefficients.\n",
    "Cross-Validation: Splitting data into training and validation sets to evaluate model performance.\n",
    "Pruning: Reducing the complexity of the model (e.g., reducing the number of layers in a neural network).\n",
    "Early Stopping: Stopping training when performance on the validation set starts to degrade.\n",
    "This comprehensive guide covers essential aspects of data manipulation, machine learning concepts, and practical tasks using Flask and Pandas. If you need further details or code examples, feel free to ask!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43313d57-63e6-42f7-a536-f40b91104957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Concept of Cross-Validation\\nCross-validation is a technique used to evaluate the performance of a machine learning model by partitioning the original sample into a training set to train the model, and a validation set to test the model. The primary goal is to ensure that the model generalizes well to an independent dataset, reducing the risk of overfitting.\\nTypes of Cross-Validation:\\n1.\\tK-Fold Cross-Validation: The dataset is divided into k subsets (folds). The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold used exactly once as the test set.\\n2.\\tLeave-One-Out Cross-Validation (LOOCV): A special case of k-fold cross-validation where k is equal to the number of data points. Each data point is used once as the validation set while the rest serve as the training set.\\n3.\\tStratified K-Fold Cross-Validation: Similar to k-fold, but ensures that each fold has the same proportion of classes as the original dataset, which is especially useful for imbalanced datasets.\\n4.\\tHold-Out Method: The dataset is split into two parts: a training set and a test set. The model is trained on the training set and validated on the test set.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Concept of Cross-Validation\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model by partitioning the original sample into a training set to train the model, and a validation set to test the model. The primary goal is to ensure that the model generalizes well to an independent dataset, reducing the risk of overfitting.\n",
    "Types of Cross-Validation:\n",
    "1.\tK-Fold Cross-Validation: The dataset is divided into k subsets (folds). The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold used exactly once as the test set.\n",
    "2.\tLeave-One-Out Cross-Validation (LOOCV): A special case of k-fold cross-validation where k is equal to the number of data points. Each data point is used once as the validation set while the rest serve as the training set.\n",
    "3.\tStratified K-Fold Cross-Validation: Similar to k-fold, but ensures that each fold has the same proportion of classes as the original dataset, which is especially useful for imbalanced datasets.\n",
    "4.\tHold-Out Method: The dataset is split into two parts: a training set and a test set. The model is trained on the training set and validated on the test set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be983d25-2ad3-4b04-a1d0-fb51f1a4025b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Difference between Classification and Regression Problems\\nClassification:\\n•\\tGoal: Predict a discrete label or category for an input.\\n•\\tOutput: Discrete values (e.g., 0 or 1, cat or dog).\\n•\\tExamples: Spam detection, image classification, sentiment analysis.\\nRegression:\\n•\\tGoal: Predict a continuous quantity.\\n•\\tOutput: Continuous values (e.g., a real number).\\n•\\tExamples: House price prediction, stock price forecasting, temperature prediction.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Difference between Classification and Regression Problems\n",
    "Classification:\n",
    "•\tGoal: Predict a discrete label or category for an input.\n",
    "•\tOutput: Discrete values (e.g., 0 or 1, cat or dog).\n",
    "•\tExamples: Spam detection, image classification, sentiment analysis.\n",
    "Regression:\n",
    "•\tGoal: Predict a continuous quantity.\n",
    "•\tOutput: Continuous values (e.g., a real number).\n",
    "•\tExamples: House price prediction, stock price forecasting, temperature prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28f4eff-683a-49db-a3af-aecdfeace834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcept of Ensemble Learning\\nEnsemble learning is a technique that combines multiple models to improve the overall performance of a machine learning system. The idea is that by aggregating the predictions of several models, the ensemble can achieve better accuracy and robustness than any single model.\\nTypes of Ensemble Methods:\\n1.\\tBagging (Bootstrap Aggregating): Multiple models are trained on different subsets of the training data (created through bootstrapping), and their predictions are averaged (for regression) or voted (for classification). Example: Random Forest.\\n2.\\tBoosting: Models are trained sequentially, each new model focusing on correcting the errors of the previous ones. Examples: AdaBoost, Gradient Boosting, XGBoost.\\n3.\\tStacking: Combines the predictions of multiple base models (trained on the entire dataset) using a meta-model, which learns how to best combine these predictions.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Concept of Ensemble Learning\n",
    "Ensemble learning is a technique that combines multiple models to improve the overall performance of a machine learning system. The idea is that by aggregating the predictions of several models, the ensemble can achieve better accuracy and robustness than any single model.\n",
    "Types of Ensemble Methods:\n",
    "1.\tBagging (Bootstrap Aggregating): Multiple models are trained on different subsets of the training data (created through bootstrapping), and their predictions are averaged (for regression) or voted (for classification). Example: Random Forest.\n",
    "2.\tBoosting: Models are trained sequentially, each new model focusing on correcting the errors of the previous ones. Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
    "3.\tStacking: Combines the predictions of multiple base models (trained on the entire dataset) using a meta-model, which learns how to best combine these predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45c96b0-7874-4363-ae94-cae3c47cdfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient Descent\\nGradient Descent is an optimization algorithm used to minimize the cost function in machine learning and deep learning models.\\nHow it Works:\\n1.\\tInitialization: Start with random initial values of the parameters.\\n2.\\tCalculate Gradient: Compute the gradient of the cost function with respect to each parameter.\\n3.\\tUpdate Parameters: Adjust the parameters in the opposite direction of the gradient to reduce the cost function.\\n4.\\tRepeat: Repeat the process until the cost function converges to a minimum or a pre-defined number of iterations is reached.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning and deep learning models.\n",
    "How it Works:\n",
    "1.\tInitialization: Start with random initial values of the parameters.\n",
    "2.\tCalculate Gradient: Compute the gradient of the cost function with respect to each parameter.\n",
    "3.\tUpdate Parameters: Adjust the parameters in the opposite direction of the gradient to reduce the cost function.\n",
    "4.\tRepeat: Repeat the process until the cost function converges to a minimum or a pre-defined number of iterations is reached.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83bbc310-2525-401b-a33b-d5ff494fc2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Difference between Batch Gradient Descent and Stochastic Gradient Descent\\nBatch Gradient Descent:\\n•\\tProcess: Computes the gradient of the cost function with respect to all the training data. This means updating the parameters after processing the entire dataset.\\n•\\tAdvantages: Provides a stable convergence.\\n•\\tDisadvantages: Can be slow and computationally expensive for large datasets.\\nStochastic Gradient Descent (SGD):\\n•\\tProcess: Computes the gradient of the cost function with respect to a single training example at each iteration. Parameters are updated for each training example.\\n•\\tAdvantages: Faster and can handle large datasets since it updates parameters more frequently.\\n•\\tDisadvantages: Can produce noisy updates, leading to a more unstable convergence.\\nMini-Batch Gradient Descent:\\n•\\tA compromise between batch and stochastic gradient descent. It splits the training dataset into small batches and updates the parameters for each batch.\\n•\\tAdvantages: Reduces the variance of parameter updates leading to more stable convergence while being more efficient than batch gradient descent.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Difference between Batch Gradient Descent and Stochastic Gradient Descent\n",
    "Batch Gradient Descent:\n",
    "•\tProcess: Computes the gradient of the cost function with respect to all the training data. This means updating the parameters after processing the entire dataset.\n",
    "•\tAdvantages: Provides a stable convergence.\n",
    "•\tDisadvantages: Can be slow and computationally expensive for large datasets.\n",
    "Stochastic Gradient Descent (SGD):\n",
    "•\tProcess: Computes the gradient of the cost function with respect to a single training example at each iteration. Parameters are updated for each training example.\n",
    "•\tAdvantages: Faster and can handle large datasets since it updates parameters more frequently.\n",
    "•\tDisadvantages: Can produce noisy updates, leading to a more unstable convergence.\n",
    "Mini-Batch Gradient Descent:\n",
    "•\tA compromise between batch and stochastic gradient descent. It splits the training dataset into small batches and updates the parameters for each batch.\n",
    "•\tAdvantages: Reduces the variance of parameter updates leading to more stable convergence while being more efficient than batch gradient descent.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3646112-e3d3-4e12-8562-1197f10f68f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurse of Dimensionality in Machine Learning\\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (with a large number of features or dimensions). As the number of dimensions increases, the volume of the space increases exponentially, making the available data sparse. This sparsity is problematic for any method that requires statistical significance, because the amount of data needed to support the analysis often grows exponentially with the dimensionality. Some key issues include:\\n1.\\tIncreased Computational Cost: The complexity of algorithms often grows exponentially with the number of dimensions.\\n2.\\tOverfitting: High-dimensional spaces allow models to fit noise in the training data rather than the actual patterns.\\n3.\\tDistance Metrics: In high dimensions, the distance between any two points tends to become similar, reducing the effectiveness of distance-based algorithms like k-nearest neighbors (KNN).\\n4.\\tFeature Selection: Finding relevant features becomes harder because the significance of each feature diminishes as the number of dimensions increases.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Curse of Dimensionality in Machine Learning\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (with a large number of features or dimensions). As the number of dimensions increases, the volume of the space increases exponentially, making the available data sparse. This sparsity is problematic for any method that requires statistical significance, because the amount of data needed to support the analysis often grows exponentially with the dimensionality. Some key issues include:\n",
    "1.\tIncreased Computational Cost: The complexity of algorithms often grows exponentially with the number of dimensions.\n",
    "2.\tOverfitting: High-dimensional spaces allow models to fit noise in the training data rather than the actual patterns.\n",
    "3.\tDistance Metrics: In high dimensions, the distance between any two points tends to become similar, reducing the effectiveness of distance-based algorithms like k-nearest neighbors (KNN).\n",
    "4.\tFeature Selection: Finding relevant features becomes harder because the significance of each feature diminishes as the number of dimensions increases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56b258e-0c20-40ce-9c72-7ab8f6dccb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\2036880547.py:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDifference between L1 and L2 Regularization\\nL1 Regularization (Lasso Regression):\\n•\\tPenalty: Adds the sum of the absolute values of the coefficients to the loss function.\\n•\\tObjective Function: Loss+λ∑∣wi∣\\text{Loss} + \\\\lambda \\\\sum |w_i|Loss+λ∑∣wi∣\\n•\\tEffect: Encourages sparsity, meaning it can drive some coefficients to zero, effectively performing feature selection.\\n•\\tUsage: Useful when we expect only a few features to be relevant.\\nL2 Regularization (Ridge Regression):\\n•\\tPenalty: Adds the sum of the squares of the coefficients to the loss function.\\n•\\tObjective Function: Loss+λ∑wi2\\text{Loss} + \\\\lambda \\\\sum w_i^2Loss+λ∑wi2\\n•\\tEffect: Shrinks the coefficients, but typically does not zero them out. It helps to distribute the weight more evenly.\\n•\\tUsage: Useful when we expect many features to be relevant, but want to prevent overfitting by shrinking the coefficients.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Difference between L1 and L2 Regularization\n",
    "L1 Regularization (Lasso Regression):\n",
    "•\tPenalty: Adds the sum of the absolute values of the coefficients to the loss function.\n",
    "•\tObjective Function: Loss+λ∑∣wi∣\\text{Loss} + \\lambda \\sum |w_i|Loss+λ∑∣wi∣\n",
    "•\tEffect: Encourages sparsity, meaning it can drive some coefficients to zero, effectively performing feature selection.\n",
    "•\tUsage: Useful when we expect only a few features to be relevant.\n",
    "L2 Regularization (Ridge Regression):\n",
    "•\tPenalty: Adds the sum of the squares of the coefficients to the loss function.\n",
    "•\tObjective Function: Loss+λ∑wi2\\text{Loss} + \\lambda \\sum w_i^2Loss+λ∑wi2\n",
    "•\tEffect: Shrinks the coefficients, but typically does not zero them out. It helps to distribute the weight more evenly.\n",
    "•\tUsage: Useful when we expect many features to be relevant, but want to prevent overfitting by shrinking the coefficients.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45d5124-3549-47cf-892a-4dc60bb6a530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\159545711.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nConfusion Matrix\\nA confusion matrix is a table used to evaluate the performance of a classification model. It compares the actual target values with those predicted by the model. Each row of the matrix represents the instances in an actual class, while each column represents the instances in a predicted class.\\n\\tPredicted Positive\\tPredicted Negative\\nActual Positive\\tTrue Positive (TP)\\tFalse Negative (FN)\\nActual Negative\\tFalse Positive (FP)\\tTrue Negative (TN)\\nMetrics Derived from Confusion Matrix:\\n•\\tAccuracy: TP+TNTP+TN+FP+FN\\x0crac{TP + TN}{TP + TN + FP + FN}TP+TN+FP+FNTP+TN\\n•\\tPrecision: TPTP+FP\\x0crac{TP}{TP + FP}TP+FPTP\\n•\\tRecall (Sensitivity): TPTP+FN\\x0crac{TP}{TP + FN}TP+FNTP\\n•\\tF1 Score: 2⋅Precision⋅RecallPrecision+Recall2 \\\\cdot \\x0crac{\\text{Precision} \\\\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}2⋅Precision+RecallPrecision⋅Recall\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It compares the actual target values with those predicted by the model. Each row of the matrix represents the instances in an actual class, while each column represents the instances in a predicted class.\n",
    "\tPredicted Positive\tPredicted Negative\n",
    "Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "Metrics Derived from Confusion Matrix:\n",
    "•\tAccuracy: TP+TNTP+TN+FP+FN\\frac{TP + TN}{TP + TN + FP + FN}TP+TN+FP+FNTP+TN\n",
    "•\tPrecision: TPTP+FP\\frac{TP}{TP + FP}TP+FPTP\n",
    "•\tRecall (Sensitivity): TPTP+FN\\frac{TP}{TP + FN}TP+FNTP\n",
    "•\tF1 Score: 2⋅Precision⋅RecallPrecision+Recall2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}2⋅Precision+RecallPrecision⋅Recall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23a2792-ebf0-49d6-8bc2-be2e107f4f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAUC-ROC Curve\\nAUC-ROC Curve: The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier's performance across different threshold values. The Area Under the Curve (AUC) measures the entire two-dimensional area underneath the ROC curve.\\nROC Curve: Plots True Positive Rate (Recall) against False Positive Rate (FPR) at various threshold settings.\\n•\\tTrue Positive Rate (TPR) or Recall: TPTP+FN\\x0crac{TP}{TP + FN}TP+FNTP\\n•\\tFalse Positive Rate (FPR): FPFP+TN\\x0crac{FP}{FP + TN}FP+TNFP\\nAUC Value:\\n•\\tRanges from 0 to 1.\\n•\\tA model with an AUC of 0.5 performs no better than random guessing.\\n•\\tAn AUC closer to 1 indicates a better performing model.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "AUC-ROC Curve\n",
    "AUC-ROC Curve: The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier's performance across different threshold values. The Area Under the Curve (AUC) measures the entire two-dimensional area underneath the ROC curve.\n",
    "ROC Curve: Plots True Positive Rate (Recall) against False Positive Rate (FPR) at various threshold settings.\n",
    "•\tTrue Positive Rate (TPR) or Recall: TPTP+FN\\frac{TP}{TP + FN}TP+FNTP\n",
    "•\tFalse Positive Rate (FPR): FPFP+TN\\frac{FP}{FP + TN}FP+TNFP\n",
    "AUC Value:\n",
    "•\tRanges from 0 to 1.\n",
    "•\tA model with an AUC of 0.5 performs no better than random guessing.\n",
    "•\tAn AUC closer to 1 indicates a better performing model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e72a2c-81ae-49a1-a5ea-708dec7f5a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nk-Nearest Neighbors Algorithm (KNN)\\nThe k-nearest neighbors (KNN) algorithm is a simple, instance-based learning method used for classification and regression. It assumes that similar instances exist in close proximity to each other.\\nHow it Works:\\n1.\\tChoose k: Select the number of neighbors (k).\\n2.\\tCalculate Distance: Compute the distance between the new data point and all training points (commonly using Euclidean distance).\\n3.\\tFind Neighbors: Identify the k nearest neighbors to the new data point.\\n4.\\tVote (Classification) or Average (Regression):\\no\\tClassification: Assign the class that is most frequent among the k nearest neighbors.\\no\\tRegression: Assign the average value of the k nearest neighbors.\\nAdvantages:\\n•\\tSimple and easy to implement.\\n•\\tNon-parametric and lazy learning (no training phase).\\nDisadvantages:\\n•\\tComputationally intensive, especially with large datasets.\\n•\\tSensitive to the choice of k and the distance metric.\\n•\\tCan be affected by the curse of dimensionality.\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "k-Nearest Neighbors Algorithm (KNN)\n",
    "The k-nearest neighbors (KNN) algorithm is a simple, instance-based learning method used for classification and regression. It assumes that similar instances exist in close proximity to each other.\n",
    "How it Works:\n",
    "1.\tChoose k: Select the number of neighbors (k).\n",
    "2.\tCalculate Distance: Compute the distance between the new data point and all training points (commonly using Euclidean distance).\n",
    "3.\tFind Neighbors: Identify the k nearest neighbors to the new data point.\n",
    "4.\tVote (Classification) or Average (Regression):\n",
    "o\tClassification: Assign the class that is most frequent among the k nearest neighbors.\n",
    "o\tRegression: Assign the average value of the k nearest neighbors.\n",
    "Advantages:\n",
    "•\tSimple and easy to implement.\n",
    "•\tNon-parametric and lazy learning (no training phase).\n",
    "Disadvantages:\n",
    "•\tComputationally intensive, especially with large datasets.\n",
    "•\tSensitive to the choice of k and the distance metric.\n",
    "•\tCan be affected by the curse of dimensionality.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8f3342-0d34-4997-8605-1600f65cd147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBasic Concept of a Support Vector Machine (SVM)\\nA Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It aims to find the optimal hyperplane that maximally separates data points of different classes in a feature space. The key idea is to choose a hyperplane that has the largest minimum distance (margin) to the nearest data points of any class, which ensures the best generalization ability on unseen data.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic Concept of a Support Vector Machine (SVM)\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It aims to find the optimal hyperplane that maximally separates data points of different classes in a feature space. The key idea is to choose a hyperplane that has the largest minimum distance (margin) to the nearest data points of any class, which ensures the best generalization ability on unseen data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38dbd8f2-e338-4be5-b83f-3cc406de845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKernel Trick in SVM\\nThe kernel trick is a technique used to transform data into a higher-dimensional space to make it easier to separate using a linear hyperplane. Instead of explicitly mapping data points to a higher dimension, the kernel function computes the dot product of the data points in this higher-dimensional space directly. This approach is computationally efficient because it avoids the need for explicitly computing the coordinates of the data in the high-dimensional space.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kernel Trick in SVM\n",
    "The kernel trick is a technique used to transform data into a higher-dimensional space to make it easier to separate using a linear hyperplane. Instead of explicitly mapping data points to a higher dimension, the kernel function computes the dot product of the data points in this higher-dimensional space directly. This approach is computationally efficient because it avoids the need for explicitly computing the coordinates of the data in the high-dimensional space.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263e3e99-8ae3-4594-b47b-6b51955d6856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\2273052329.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCommon Kernel Functions\\n1.\\tLinear Kernel\\no\\tFormula: K(xi,xj)=xi⋅xjK(x_i, x_j) = x_i \\\\cdot x_jK(xi,xj)=xi⋅xj\\no\\tUse Case: Suitable for linearly separable data. It works well when the number of features is large compared to the number of samples.\\n2.\\tPolynomial Kernel\\no\\tFormula: K(xi,xj)=(xi⋅xj+c)dK(x_i, x_j) = (x_i \\\\cdot x_j + c)^dK(xi,xj)=(xi⋅xj+c)d\\no\\tUse Case: Useful when the data shows a polynomial relationship. The degree ddd controls the flexibility of the boundary, and ccc is a constant that can trade off the influence of higher-order versus lower-order terms.\\n3.\\tRadial Basis Function (RBF) Kernel (Gaussian Kernel)\\no\\tFormula: K(xi,xj)=exp\\u2061(−γ∥xi−xj∥2)K(x_i, x_j) = \\\\exp(-\\\\gamma \\\\|x_i - x_j\\\\|^2)K(xi,xj)=exp(−γ∥xi−xj∥2)\\no\\tUse Case: Effective for non-linear data. It maps the data into an infinite-dimensional space and can handle complex data patterns. The parameter γ\\\\gammaγ defines the influence of a single training example; a low γ\\\\gammaγ means a larger influence radius.\\n4.\\tSigmoid Kernel\\no\\tFormula: K(xi,xj)=tanh\\u2061(αxi⋅xj+c)K(x_i, x_j) = \\tanh(\\x07lpha x_i \\\\cdot x_j + c)K(xi,xj)=tanh(αxi⋅xj+c)\\no\\tUse Case: Similar to a two-layer neural network. It is used in situations where the dataset shows behavior similar to a neural network.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Common Kernel Functions\n",
    "1.\tLinear Kernel\n",
    "o\tFormula: K(xi,xj)=xi⋅xjK(x_i, x_j) = x_i \\cdot x_jK(xi,xj)=xi⋅xj\n",
    "o\tUse Case: Suitable for linearly separable data. It works well when the number of features is large compared to the number of samples.\n",
    "2.\tPolynomial Kernel\n",
    "o\tFormula: K(xi,xj)=(xi⋅xj+c)dK(x_i, x_j) = (x_i \\cdot x_j + c)^dK(xi,xj)=(xi⋅xj+c)d\n",
    "o\tUse Case: Useful when the data shows a polynomial relationship. The degree ddd controls the flexibility of the boundary, and ccc is a constant that can trade off the influence of higher-order versus lower-order terms.\n",
    "3.\tRadial Basis Function (RBF) Kernel (Gaussian Kernel)\n",
    "o\tFormula: K(xi,xj)=exp⁡(−γ∥xi−xj∥2)K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)K(xi,xj)=exp(−γ∥xi−xj∥2)\n",
    "o\tUse Case: Effective for non-linear data. It maps the data into an infinite-dimensional space and can handle complex data patterns. The parameter γ\\gammaγ defines the influence of a single training example; a low γ\\gammaγ means a larger influence radius.\n",
    "4.\tSigmoid Kernel\n",
    "o\tFormula: K(xi,xj)=tanh⁡(αxi⋅xj+c)K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)K(xi,xj)=tanh(αxi⋅xj+c)\n",
    "o\tUse Case: Similar to a two-layer neural network. It is used in situations where the dataset shows behavior similar to a neural network.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8c55ff-ffcf-4dfb-bc9e-4a6c916891ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\q'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\q'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\3212613650.py:1: SyntaxWarning: invalid escape sequence '\\q'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHyperplane in SVM\\nIn SVM, the hyperplane is a decision boundary that separates different classes. For a two-dimensional feature space, the hyperplane is a line, while in three dimensions, it is a plane. In higher dimensions, it is referred to as a hyperplane. The optimal hyperplane is the one that maximizes the margin between the two classes.\\nDetermining the Hyperplane\\nThe process of finding the optimal hyperplane involves solving a quadratic optimization problem with the following objective:\\nMinimize12∥w∥2\\text{Minimize} \\\\quad \\x0crac{1}{2} \\\\|w\\\\|^2Minimize21∥w∥2\\nsubject to:\\nyi(w⋅xi+b)≥1,∀iy_i (w \\\\cdot x_i + b) \\\\geq 1, \\\\quad \\x0corall iyi(w⋅xi+b)≥1,∀i\\nwhere:\\n•\\twww is the weight vector perpendicular to the hyperplane,\\n•\\tbbb is the bias term that offsets the hyperplane,\\n•\\tyiy_iyi is the class label of the iii-th sample,\\n•\\txix_ixi is the feature vector of the iii-th sample.\\nThe constraint ensures that all data points are correctly classified with a margin of at least 1. The support vectors are the data points that lie closest to the hyperplane and influence its position and orientation.\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hyperplane in SVM\n",
    "In SVM, the hyperplane is a decision boundary that separates different classes. For a two-dimensional feature space, the hyperplane is a line, while in three dimensions, it is a plane. In higher dimensions, it is referred to as a hyperplane. The optimal hyperplane is the one that maximizes the margin between the two classes.\n",
    "Determining the Hyperplane\n",
    "The process of finding the optimal hyperplane involves solving a quadratic optimization problem with the following objective:\n",
    "Minimize12∥w∥2\\text{Minimize} \\quad \\frac{1}{2} \\|w\\|^2Minimize21∥w∥2\n",
    "subject to:\n",
    "yi(w⋅xi+b)≥1,∀iy_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall iyi(w⋅xi+b)≥1,∀i\n",
    "where:\n",
    "•\twww is the weight vector perpendicular to the hyperplane,\n",
    "•\tbbb is the bias term that offsets the hyperplane,\n",
    "•\tyiy_iyi is the class label of the iii-th sample,\n",
    "•\txix_ixi is the feature vector of the iii-th sample.\n",
    "The constraint ensures that all data points are correctly classified with a margin of at least 1. The support vectors are the data points that lie closest to the hyperplane and influence its position and orientation.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7108b4f5-ab18-4240-a6f3-8fba58952e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\1658713349.py:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPros and Cons of Using a Support Vector Machine (SVM)\\nPros:\\n1.\\tEffective in high-dimensional spaces: SVM performs well when the number of features is large, as it finds the optimal hyperplane in a higher-dimensional feature space.\\n2.\\tMemory efficient: SVM uses a subset of the training points (support vectors) in the decision function, making it memory efficient.\\n3.\\tVersatile: SVM can be adapted to different kinds of data through the use of various kernel functions (linear, polynomial, RBF, sigmoid), making it versatile for various problems.\\n4.\\tRobust to overfitting: With the correct choice of kernel and parameters, SVM can be robust to overfitting, especially in high-dimensional spaces.\\nCons:\\n1.\\tNot suitable for large datasets: Training an SVM can be computationally expensive and time-consuming for large datasets, as the complexity of the algorithm increases with the number of data points.\\n2.\\tLess effective on noisier datasets: SVM may struggle with datasets where classes overlap significantly and are noisy, as it tries to find a clear margin between classes.\\n3.\\tChoosing the right kernel and parameters: The performance of SVM heavily relies on the choice of kernel and parameters (such as CCC and γ\\\\gammaγ). This requires careful tuning through cross-validation, which can be computationally intensive.\\n4.\\tInterpretability: The resulting model from an SVM can be less interpretable compared to other models like decision trees, especially when using non-linear kernels.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pros and Cons of Using a Support Vector Machine (SVM)\n",
    "Pros:\n",
    "1.\tEffective in high-dimensional spaces: SVM performs well when the number of features is large, as it finds the optimal hyperplane in a higher-dimensional feature space.\n",
    "2.\tMemory efficient: SVM uses a subset of the training points (support vectors) in the decision function, making it memory efficient.\n",
    "3.\tVersatile: SVM can be adapted to different kinds of data through the use of various kernel functions (linear, polynomial, RBF, sigmoid), making it versatile for various problems.\n",
    "4.\tRobust to overfitting: With the correct choice of kernel and parameters, SVM can be robust to overfitting, especially in high-dimensional spaces.\n",
    "Cons:\n",
    "1.\tNot suitable for large datasets: Training an SVM can be computationally expensive and time-consuming for large datasets, as the complexity of the algorithm increases with the number of data points.\n",
    "2.\tLess effective on noisier datasets: SVM may struggle with datasets where classes overlap significantly and are noisy, as it tries to find a clear margin between classes.\n",
    "3.\tChoosing the right kernel and parameters: The performance of SVM heavily relies on the choice of kernel and parameters (such as CCC and γ\\gammaγ). This requires careful tuning through cross-validation, which can be computationally intensive.\n",
    "4.\tInterpretability: The resulting model from an SVM can be less interpretable compared to other models like decision trees, especially when using non-linear kernels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7be6ea1-c2b9-4c94-8eeb-cc71691ce381",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2393-2394: truncated \\xXX escape (2746688118.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2393-2394: truncated \\xXX escape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The difference between hard margin and soft margin SVM lies in how they handle the separation of classes and the tolerance for misclassified data points.\n",
    "Hard Margin SVM\n",
    "In a hard margin SVM, the algorithm aims to find a hyperplane that perfectly separates the data into two classes with no misclassifications. It requires the data to be linearly separable, meaning there must exist a hyperplane that can separate the data points of different classes without any error.\n",
    "Characteristics of Hard Margin SVM:\n",
    "1.\tStrict Separation: All data points must be correctly classified.\n",
    "2.\tMaximized Margin: The distance between the hyperplane and the nearest data points (support vectors) is maximized.\n",
    "3.\tNo Tolerance for Misclassification: It cannot handle outliers or noise in the data, as all points must lie outside the margin boundaries.\n",
    "Optimization Problem:\n",
    "The hard margin SVM solves the following optimization problem:\n",
    "Minimize12∥w∥2\\text{Minimize} \\quad \\frac{1}{2} \\|w\\|^2Minimize21∥w∥2\n",
    "subject to:\n",
    "yi(w⋅xi+b)≥1,∀iy_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall iyi(w⋅xi+b)≥1,∀i\n",
    "where www is the weight vector, bbb is the bias, yiy_iyi is the class label, and xix_ixi is the feature vector of the iii-th sample.\n",
    "Soft Margin SVM\n",
    "Soft margin SVM introduces the concept of allowing some misclassifications and errors to handle datasets that are not perfectly linearly separable. It introduces slack variables to permit some data points to lie within the margin boundaries or on the wrong side of the hyperplane.\n",
    "Characteristics of Soft Margin SVM:\n",
    "1.\tTolerance for Misclassification: It allows some data points to be misclassified or to lie within the margin boundaries.\n",
    "2.\tBalanced Objective: The goal is to find a balance between maximizing the margin and minimizing the classification error.\n",
    "3.\tRegularization Parameter CCC: The parameter CCC controls the trade-off between achieving a large margin and minimizing the classification error. A higher CCC value results in a smaller margin with fewer misclassifications, while a lower CCC value results in a larger margin with more tolerance for errors.\n",
    "Optimization Problem:\n",
    "The soft margin SVM solves the following optimization problem:\n",
    "Minimize12∥w∥2+C∑i=1nξi\\text{Minimize} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_iMinimize21∥w∥2+C∑i=1nξi\n",
    "subject to:\n",
    "yi(w⋅xi+b)≥1−ξi,∀iy_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\forall iyi(w⋅xi+b)≥1−ξi,∀i ξi≥0,∀i\\xi_i \\geq 0, \\quad \\forall iξi≥0,∀i\n",
    "where ξi\\xi_iξi are the slack variables that measure the degree of misclassification of the iii-th sample, and CCC is the regularization parameter.\n",
    "Key Differences:\n",
    "1.\tLinear Separability:\n",
    "o\tHard Margin SVM: Assumes that the data is perfectly linearly separable.\n",
    "o\tSoft Margin SVM: Does not assume perfect linear separability and can handle overlapping classes and noise.\n",
    "2.\tMisclassification Tolerance:\n",
    "o\tHard Margin SVM: No tolerance for misclassification or data points within the margin.\n",
    "o\tSoft Margin SVM: Allows for some misclassifications and violations of the margin constraints.\n",
    "3.\tFlexibility:\n",
    "o\tHard Margin SVM: Less flexible, as it cannot handle datasets with overlapping classes or noise.\n",
    "o\tSoft Margin SVM: More flexible, as it introduces slack variables and a regularization parameter to handle noisy data and overlapping classes.\n",
    "4.\tOptimization Objective:\n",
    "o\tHard Margin SVM: Focuses solely on maximizing the margin.\n",
    "o\tSoft Margin SVM: Balances maximizing the margin and minimizing the classification error through the use of slack variables and the regularization parameter CCC.\n",
    "When to Use:\n",
    "•\tHard Margin SVM: Use when you have a clean, linearly separable dataset with no overlap or noise.\n",
    "•\tSoft Margin SVM: Use when your dataset has noise, overlapping classes, or is not perfectly linearly separable.\n",
    "In practice, soft margin SVM is more commonly used due to its ability to handle real-world data imperfections.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beafa243-dd99-4455-a2b3-f59391754019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcess of Constructing a Decision Tree\\nConstructing a decision tree involves a series of steps aimed at creating a model that predicts the value of a target variable based on several input variables. Here is a detailed description of the process:\\n1.\\tSelect the Best Attribute: At each node of the tree, the algorithm selects the attribute that best separates the data into different classes. This selection is based on a measure of impurity or information gain, such as Gini impurity, entropy, or gain ratio.\\n2.\\tSplitting the Dataset: Once the best attribute is selected, the dataset is split into subsets based on the values of this attribute. Each subset corresponds to a branch of the node.\\n3.\\tCreate Sub-nodes: For each subset created in the previous step, a sub-node is created. This process is recursive, meaning that each sub-node is treated as a new node, and the best attribute is selected for further splitting.\\n4.\\tStopping Criteria: The recursive splitting continues until a stopping criterion is met. Common stopping criteria include:\\no\\tAll data points in a node belong to the same class.\\no\\tNo remaining attributes to split the data.\\no\\tThe tree has reached a maximum specified depth.\\no\\tThe number of data points in a node is below a minimum threshold.\\n5.\\tAssigning Class Labels: Once the stopping criteria are met, each leaf node is assigned a class label. This is typically done by taking a majority vote of the class labels of the data points in that node.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process of Constructing a Decision Tree\n",
    "Constructing a decision tree involves a series of steps aimed at creating a model that predicts the value of a target variable based on several input variables. Here is a detailed description of the process:\n",
    "1.\tSelect the Best Attribute: At each node of the tree, the algorithm selects the attribute that best separates the data into different classes. This selection is based on a measure of impurity or information gain, such as Gini impurity, entropy, or gain ratio.\n",
    "2.\tSplitting the Dataset: Once the best attribute is selected, the dataset is split into subsets based on the values of this attribute. Each subset corresponds to a branch of the node.\n",
    "3.\tCreate Sub-nodes: For each subset created in the previous step, a sub-node is created. This process is recursive, meaning that each sub-node is treated as a new node, and the best attribute is selected for further splitting.\n",
    "4.\tStopping Criteria: The recursive splitting continues until a stopping criterion is met. Common stopping criteria include:\n",
    "o\tAll data points in a node belong to the same class.\n",
    "o\tNo remaining attributes to split the data.\n",
    "o\tThe tree has reached a maximum specified depth.\n",
    "o\tThe number of data points in a node is below a minimum threshold.\n",
    "5.\tAssigning Class Labels: Once the stopping criteria are met, each leaf node is assigned a class label. This is typically done by taking a majority vote of the class labels of the data points in that node.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ae0f0dc-6ca1-46fd-9b25-e52f9f958386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWorking Principle of a Decision Tree\\nA decision tree works by recursively splitting the dataset into smaller subsets based on the values of the attributes, aiming to increase the homogeneity of the target variable within each subset. Here’s a step-by-step description of its working principle:\\n1.\\tRoot Node: The process starts at the root node, which contains the entire dataset. The algorithm evaluates all the attributes to find the one that provides the best split. The best split is the one that maximizes some criterion, such as information gain or Gini impurity reduction.\\n2.\\tSplitting Criteria:\\no\\tEntropy and Information Gain: Entropy measures the impurity or disorder of a dataset. Information gain calculates the reduction in entropy achieved by splitting the data based on an attribute.\\no\\tGini Impurity: Gini impurity measures the likelihood of incorrect classification of a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\\n3.\\tCreating Branches: The selected attribute is used to split the dataset into subsets, creating branches from the root node. Each branch represents a possible value of the selected attribute.\\n4.\\tRecursive Splitting: The splitting process is repeated recursively for each branch, considering the subset of data corresponding to that branch. The best attribute for each subset is determined, and the dataset is further split.\\n5.\\tLeaf Nodes: The recursion continues until it meets a stopping criterion. At this point, leaf nodes are created. Each leaf node represents a class label, determined by the majority class of the data points in that node.\\n6.\\tPrediction: To classify a new data point, the decision tree starts at the root node and traverses down the tree, following the branches based on the values of the attributes of the data point, until it reaches a leaf node. The class label of the leaf node is assigned to the data point.\\nExample of Decision Tree Construction\\nLet\\'s consider a simple example with a dataset that includes the attributes \"Weather\" (Sunny, Overcast, Rainy), \"Temperature\" (Hot, Mild, Cool), and \"Play\" (Yes, No).\\n1.\\tRoot Node Selection: Evaluate the attributes \"Weather\" and \"Temperature\" to determine which attribute provides the best split based on information gain or Gini impurity.\\no\\tSuppose \"Weather\" provides the best split.\\n2.\\tFirst Split: Split the dataset into three subsets based on \"Weather\" (Sunny, Overcast, Rainy).\\no\\tCreate branches for each value of \"Weather\".\\n3.\\tRecursive Splitting: For each subset, evaluate the remaining attributes and perform further splits.\\no\\tIf \"Weather\" is Sunny, evaluate \"Temperature\" for the next best split.\\no\\tContinue this process for each branch.\\n4.\\tStopping Criteria: Stop splitting when all data points in a subset belong to the same class, no more attributes are available for splitting, or other stopping criteria are met.\\n5.\\tLeaf Nodes: Assign class labels to the leaf nodes based on the majority class of the data points in each subset.\\nAdvantages and Disadvantages of Decision Trees\\nAdvantages:\\n1.\\tEasy to Understand: Decision trees are intuitive and easy to interpret, even for non-experts.\\n2.\\tHandles Both Numerical and Categorical Data: Decision trees can handle a variety of data types.\\n3.\\tNon-parametric: No assumptions about the distribution of data.\\n4.\\tHandles Non-linear Relationships: Capable of capturing complex patterns in data.\\nDisadvantages:\\n1.\\tOverfitting: Decision trees can easily overfit, especially with noisy data or when they become too deep.\\n2.\\tUnstable: Small changes in the data can result in a completely different tree structure.\\n3.\\tBias in Attribute Selection: Attributes with more levels can dominate the splitting criterion.\\nIn summary, decision trees are powerful and versatile tools for classification and regression tasks. They build models by recursively splitting the data based on attributes, aiming to maximize homogeneity within subsets, and they predict the class of new data points by traversing the tree structure.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Working Principle of a Decision Tree\n",
    "A decision tree works by recursively splitting the dataset into smaller subsets based on the values of the attributes, aiming to increase the homogeneity of the target variable within each subset. Here’s a step-by-step description of its working principle:\n",
    "1.\tRoot Node: The process starts at the root node, which contains the entire dataset. The algorithm evaluates all the attributes to find the one that provides the best split. The best split is the one that maximizes some criterion, such as information gain or Gini impurity reduction.\n",
    "2.\tSplitting Criteria:\n",
    "o\tEntropy and Information Gain: Entropy measures the impurity or disorder of a dataset. Information gain calculates the reduction in entropy achieved by splitting the data based on an attribute.\n",
    "o\tGini Impurity: Gini impurity measures the likelihood of incorrect classification of a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
    "3.\tCreating Branches: The selected attribute is used to split the dataset into subsets, creating branches from the root node. Each branch represents a possible value of the selected attribute.\n",
    "4.\tRecursive Splitting: The splitting process is repeated recursively for each branch, considering the subset of data corresponding to that branch. The best attribute for each subset is determined, and the dataset is further split.\n",
    "5.\tLeaf Nodes: The recursion continues until it meets a stopping criterion. At this point, leaf nodes are created. Each leaf node represents a class label, determined by the majority class of the data points in that node.\n",
    "6.\tPrediction: To classify a new data point, the decision tree starts at the root node and traverses down the tree, following the branches based on the values of the attributes of the data point, until it reaches a leaf node. The class label of the leaf node is assigned to the data point.\n",
    "Example of Decision Tree Construction\n",
    "Let's consider a simple example with a dataset that includes the attributes \"Weather\" (Sunny, Overcast, Rainy), \"Temperature\" (Hot, Mild, Cool), and \"Play\" (Yes, No).\n",
    "1.\tRoot Node Selection: Evaluate the attributes \"Weather\" and \"Temperature\" to determine which attribute provides the best split based on information gain or Gini impurity.\n",
    "o\tSuppose \"Weather\" provides the best split.\n",
    "2.\tFirst Split: Split the dataset into three subsets based on \"Weather\" (Sunny, Overcast, Rainy).\n",
    "o\tCreate branches for each value of \"Weather\".\n",
    "3.\tRecursive Splitting: For each subset, evaluate the remaining attributes and perform further splits.\n",
    "o\tIf \"Weather\" is Sunny, evaluate \"Temperature\" for the next best split.\n",
    "o\tContinue this process for each branch.\n",
    "4.\tStopping Criteria: Stop splitting when all data points in a subset belong to the same class, no more attributes are available for splitting, or other stopping criteria are met.\n",
    "5.\tLeaf Nodes: Assign class labels to the leaf nodes based on the majority class of the data points in each subset.\n",
    "Advantages and Disadvantages of Decision Trees\n",
    "Advantages:\n",
    "1.\tEasy to Understand: Decision trees are intuitive and easy to interpret, even for non-experts.\n",
    "2.\tHandles Both Numerical and Categorical Data: Decision trees can handle a variety of data types.\n",
    "3.\tNon-parametric: No assumptions about the distribution of data.\n",
    "4.\tHandles Non-linear Relationships: Capable of capturing complex patterns in data.\n",
    "Disadvantages:\n",
    "1.\tOverfitting: Decision trees can easily overfit, especially with noisy data or when they become too deep.\n",
    "2.\tUnstable: Small changes in the data can result in a completely different tree structure.\n",
    "3.\tBias in Attribute Selection: Attributes with more levels can dominate the splitting criterion.\n",
    "In summary, decision trees are powerful and versatile tools for classification and regression tasks. They build models by recursively splitting the data based on attributes, aiming to maximize homogeneity within subsets, and they predict the class of new data points by traversing the tree structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ae0e2ca-d1c9-4a71-8ed3-29669c4d87d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\1768496881.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nInformation Gain and Its Use in Decision Trees\\nInformation Gain\\nInformation gain is a metric used to measure the effectiveness of an attribute in classifying a dataset. It is based on the concept of entropy from information theory. Entropy quantifies the amount of uncertainty or impurity in a dataset. Information gain measures the reduction in entropy after a dataset is split based on an attribute. The attribute with the highest information gain is chosen for splitting the dataset at each node in the decision tree.\\nCalculating Entropy\\nEntropy (HHH) is calculated as follows:\\nH(S)=−∑i=1npilog\\u20612(pi)H(S) = - \\\\sum_{i=1}^{n} p_i \\\\log_2(p_i)H(S)=−∑i=1npilog2(pi)\\nwhere:\\n•\\tSSS is the dataset,\\n•\\tpip_ipi is the proportion of instances belonging to class iii,\\n•\\tnnn is the number of classes.\\nCalculating Information Gain\\nInformation gain (IGIGIG) is the difference between the entropy of the dataset before and after splitting on an attribute:\\nIG(T,A)=H(T)−∑v∈Values(A)∣Tv∣∣T∣H(Tv)IG(T, A) = H(T) - \\\\sum_{v \\\\in \\text{Values}(A)} \\x0crac{|T_v|}{|T|} H(T_v)IG(T,A)=H(T)−∑v∈Values(A)∣T∣∣Tv∣H(Tv)\\nwhere:\\n•\\tTTT is the original dataset,\\n•\\tAAA is the attribute being considered for splitting,\\n•\\tTvT_vTv is the subset of TTT for which attribute AAA has value vvv,\\n•\\tH(T)H(T)H(T) is the entropy of the dataset TTT,\\n•\\tH(Tv)H(T_v)H(Tv) is the entropy of the subset TvT_vTv,\\n•\\t∣Tv∣|T_v|∣Tv∣ and ∣T∣|T|∣T∣ are the sizes of the subsets and the original dataset, respectively.\\nUse in Decision Trees\\nAt each node of the decision tree, the attribute with the highest information gain is chosen to split the dataset. This process is repeated recursively, ensuring that the most informative attributes are used first to partition the data, leading to a tree that accurately classifies instances.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Information Gain and Its Use in Decision Trees\n",
    "Information Gain\n",
    "Information gain is a metric used to measure the effectiveness of an attribute in classifying a dataset. It is based on the concept of entropy from information theory. Entropy quantifies the amount of uncertainty or impurity in a dataset. Information gain measures the reduction in entropy after a dataset is split based on an attribute. The attribute with the highest information gain is chosen for splitting the dataset at each node in the decision tree.\n",
    "Calculating Entropy\n",
    "Entropy (HHH) is calculated as follows:\n",
    "H(S)=−∑i=1npilog⁡2(pi)H(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)H(S)=−∑i=1npilog2(pi)\n",
    "where:\n",
    "•\tSSS is the dataset,\n",
    "•\tpip_ipi is the proportion of instances belonging to class iii,\n",
    "•\tnnn is the number of classes.\n",
    "Calculating Information Gain\n",
    "Information gain (IGIGIG) is the difference between the entropy of the dataset before and after splitting on an attribute:\n",
    "IG(T,A)=H(T)−∑v∈Values(A)∣Tv∣∣T∣H(Tv)IG(T, A) = H(T) - \\sum_{v \\in \\text{Values}(A)} \\frac{|T_v|}{|T|} H(T_v)IG(T,A)=H(T)−∑v∈Values(A)∣T∣∣Tv∣H(Tv)\n",
    "where:\n",
    "•\tTTT is the original dataset,\n",
    "•\tAAA is the attribute being considered for splitting,\n",
    "•\tTvT_vTv is the subset of TTT for which attribute AAA has value vvv,\n",
    "•\tH(T)H(T)H(T) is the entropy of the dataset TTT,\n",
    "•\tH(Tv)H(T_v)H(Tv) is the entropy of the subset TvT_vTv,\n",
    "•\t∣Tv∣|T_v|∣Tv∣ and ∣T∣|T|∣T∣ are the sizes of the subsets and the original dataset, respectively.\n",
    "Use in Decision Trees\n",
    "At each node of the decision tree, the attribute with the highest information gain is chosen to split the dataset. This process is repeated recursively, ensuring that the most informative attributes are used first to partition the data, leading to a tree that accurately classifies instances.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c9a55f-b319-491f-9ebe-0dbb5ca57717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\67777335.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGini Impurity\\nGini impurity is another measure of impurity or disorder used in decision trees, particularly in the CART (Classification and Regression Trees) algorithm. It measures the likelihood of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\\nCalculating Gini Impurity\\nGini impurity (GGG) is calculated as follows:\\nG(S)=1−∑i=1npi2G(S) = 1 - \\\\sum_{i=1}^{n} p_i^2G(S)=1−∑i=1npi2\\nwhere:\\n•\\tSSS is the dataset,\\n•\\tpip_ipi is the proportion of instances belonging to class iii,\\n•\\tnnn is the number of classes.\\nUse in Decision Trees\\nAt each node of the decision tree, the attribute that results in the lowest Gini impurity for the split is chosen. The goal is to create subsets that are as pure as possible, minimizing the chance of misclassification. This process is repeated recursively, leading to a tree structure that effectively classifies instances based on the attributes.\\nAdvantages and Disadvantages of Decision Trees\\nAdvantages\\n1.\\tEasy to Understand and Interpret: Decision trees are intuitive and can be visualized, making them easy to explain to non-experts.\\n2.\\tHandles Both Numerical and Categorical Data: Decision trees can be applied to various types of data, including continuous and categorical variables.\\n3.\\tNon-parametric: They do not require any assumptions about the underlying distribution of the data.\\n4.\\tHandles Non-linear Relationships: Decision trees can capture complex, non-linear relationships between features and the target variable.\\n5.\\tFeature Importance: They provide a measure of feature importance, which helps in understanding the impact of different features on the target variable.\\nDisadvantages\\n1.\\tOverfitting: Decision trees can easily overfit the training data, especially if they are allowed to grow too deep without pruning.\\n2.\\tUnstable: Small changes in the data can result in a completely different tree structure, making them sensitive to noise in the data.\\n3.\\tBias in Attribute Selection: Attributes with more levels or unique values can dominate the splitting criterion, leading to biased splits.\\n4.\\tLack of Smooth Predictions: For regression tasks, decision trees can produce piecewise constant approximations, which may not be smooth and can lead to poor generalization.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gini Impurity\n",
    "Gini impurity is another measure of impurity or disorder used in decision trees, particularly in the CART (Classification and Regression Trees) algorithm. It measures the likelihood of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
    "Calculating Gini Impurity\n",
    "Gini impurity (GGG) is calculated as follows:\n",
    "G(S)=1−∑i=1npi2G(S) = 1 - \\sum_{i=1}^{n} p_i^2G(S)=1−∑i=1npi2\n",
    "where:\n",
    "•\tSSS is the dataset,\n",
    "•\tpip_ipi is the proportion of instances belonging to class iii,\n",
    "•\tnnn is the number of classes.\n",
    "Use in Decision Trees\n",
    "At each node of the decision tree, the attribute that results in the lowest Gini impurity for the split is chosen. The goal is to create subsets that are as pure as possible, minimizing the chance of misclassification. This process is repeated recursively, leading to a tree structure that effectively classifies instances based on the attributes.\n",
    "Advantages and Disadvantages of Decision Trees\n",
    "Advantages\n",
    "1.\tEasy to Understand and Interpret: Decision trees are intuitive and can be visualized, making them easy to explain to non-experts.\n",
    "2.\tHandles Both Numerical and Categorical Data: Decision trees can be applied to various types of data, including continuous and categorical variables.\n",
    "3.\tNon-parametric: They do not require any assumptions about the underlying distribution of the data.\n",
    "4.\tHandles Non-linear Relationships: Decision trees can capture complex, non-linear relationships between features and the target variable.\n",
    "5.\tFeature Importance: They provide a measure of feature importance, which helps in understanding the impact of different features on the target variable.\n",
    "Disadvantages\n",
    "1.\tOverfitting: Decision trees can easily overfit the training data, especially if they are allowed to grow too deep without pruning.\n",
    "2.\tUnstable: Small changes in the data can result in a completely different tree structure, making them sensitive to noise in the data.\n",
    "3.\tBias in Attribute Selection: Attributes with more levels or unique values can dominate the splitting criterion, leading to biased splits.\n",
    "4.\tLack of Smooth Predictions: For regression tasks, decision trees can produce piecewise constant approximations, which may not be smooth and can lead to poor generalization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a0b1f1-1038-4785-8268-5498c0065b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRandom forests address some of the main limitations of decision trees, particularly their tendency to overfit and their instability. Here are the key ways in which random forests improve upon decision trees:\\n1.\\tEnsemble Learning: Random forests combine multiple decision trees to create an ensemble model. Each tree is trained on a different subset of the data, and the final prediction is made by aggregating the predictions of all the individual trees (e.g., by majority vote for classification or averaging for regression). This reduces overfitting and improves generalization.\\n2.\\tReduction in Variance: By averaging the predictions of many trees, random forests reduce the variance of the model. This makes the model more robust and less sensitive to noise in the training data.\\n3.\\tFeature Randomness: Random forests introduce randomness not only in the data sampling (bootstrapping) but also in the feature selection. When building each tree, only a random subset of features is considered for splitting at each node. This decorrelates the trees and further reduces overfitting.\\n\\n•  Bootstrapping: For each tree in the forest, a bootstrap sample (a random sample with replacement) of the training data is created. This means some data points may be repeated in the sample, while others may be left out.\\n•  Tree Construction: A decision tree is built on each bootstrap sample. However, instead of considering all features for each split, only a random subset of features is considered. This ensures that the trees are not identical and reduces correlation among them.\\n•  Aggregation: Once all the trees are built, the random forest makes predictions by aggregating the predictions of the individual trees. For classification, this is typically done by majority vote. For regression, the predictions are averaged.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random forests address some of the main limitations of decision trees, particularly their tendency to overfit and their instability. Here are the key ways in which random forests improve upon decision trees:\n",
    "1.\tEnsemble Learning: Random forests combine multiple decision trees to create an ensemble model. Each tree is trained on a different subset of the data, and the final prediction is made by aggregating the predictions of all the individual trees (e.g., by majority vote for classification or averaging for regression). This reduces overfitting and improves generalization.\n",
    "2.\tReduction in Variance: By averaging the predictions of many trees, random forests reduce the variance of the model. This makes the model more robust and less sensitive to noise in the training data.\n",
    "3.\tFeature Randomness: Random forests introduce randomness not only in the data sampling (bootstrapping) but also in the feature selection. When building each tree, only a random subset of features is considered for splitting at each node. This decorrelates the trees and further reduces overfitting.\n",
    "\n",
    "•  Bootstrapping: For each tree in the forest, a bootstrap sample (a random sample with replacement) of the training data is created. This means some data points may be repeated in the sample, while others may be left out.\n",
    "•  Tree Construction: A decision tree is built on each bootstrap sample. However, instead of considering all features for each split, only a random subset of features is considered. This ensures that the trees are not identical and reduces correlation among them.\n",
    "•  Aggregation: Once all the trees are built, the random forest makes predictions by aggregating the predictions of the individual trees. For classification, this is typically done by majority vote. For regression, the predictions are averaged.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a37ac81c-e9b8-4efe-8d77-8936f23560f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBootstrapping is a technique used in random forests to create multiple training datasets from the original dataset by sampling with replacement. Each tree in the forest is trained on a different bootstrap sample. Bootstrapping helps to ensure that the trees are different from each other, promoting diversity in the ensemble. The idea is that by training each tree on a slightly different dataset, the ensemble of trees can generalize better and be more robust to overfitting.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bootstrapping is a technique used in random forests to create multiple training datasets from the original dataset by sampling with replacement. Each tree in the forest is trained on a different bootstrap sample. Bootstrapping helps to ensure that the trees are different from each other, promoting diversity in the ensemble. The idea is that by training each tree on a slightly different dataset, the ensemble of trees can generalize better and be more robust to overfitting.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2263ee28-d6a3-443e-825a-033787111fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFeature importance in random forests provides a measure of how important each feature is for the prediction task. This is typically calculated using the following methods:\\n1.\\tMean Decrease in Impurity (MDI): This measures the total reduction in impurity (e.g., Gini impurity or entropy) brought by a feature across all trees in the forest. Features that result in larger reductions are considered more important.\\n2.\\tMean Decrease in Accuracy (MDA): This involves randomly permuting the values of each feature and measuring the impact on the model's accuracy. Features that, when permuted, lead to a significant drop in accuracy are considered important.\\n\\n\\n\\nKey Hyperparameters of a Random Forest and How They Affect the Model\\n1.\\tNumber of Trees (n_estimators): The number of trees in the forest. A larger number of trees usually leads to better performance and stability but increases computational cost.\\no\\tEffect: More trees generally improve performance but with diminishing returns beyond a certain point. Too many trees can lead to long training times.\\n2.\\tMaximum Depth (max_depth): The maximum depth of each tree. This controls how deep the trees can grow.\\no\\tEffect: Deeper trees can capture more complex patterns but may overfit. Limiting depth can prevent overfitting and reduce training time.\\n3.\\tMinimum Samples Split (min_samples_split): The minimum number of samples required to split an internal node.\\no\\tEffect: Higher values prevent the tree from splitting too much, leading to simpler, more general trees.\\n4.\\tMinimum Samples Leaf (min_samples_leaf): The minimum number of samples required to be at a leaf node.\\no\\tEffect: Ensures that leaf nodes have enough samples. Higher values can smooth the model and reduce overfitting.\\n5.\\tMaximum Features (max_features): The number of features to consider when looking for the best split.\\no\\tEffect: Smaller values reduce the correlation between trees, enhancing diversity and reducing overfitting. Too small values may lead to underfitting.\\n6.\\tBootstrap (bootstrap): Whether to use bootstrapping when building trees.\\no\\tEffect: Enabling bootstrapping usually improves the performance and robustness of the model.\\n7.\\tCriterion (criterion): The function to measure the quality of a split (e.g., Gini impurity or entropy for classification).\\no\\tEffect: Different criteria can affect the splits and the structure of the trees. Gini is computationally less expensive, while entropy might lead to different splits.\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature importance in random forests provides a measure of how important each feature is for the prediction task. This is typically calculated using the following methods:\n",
    "1.\tMean Decrease in Impurity (MDI): This measures the total reduction in impurity (e.g., Gini impurity or entropy) brought by a feature across all trees in the forest. Features that result in larger reductions are considered more important.\n",
    "2.\tMean Decrease in Accuracy (MDA): This involves randomly permuting the values of each feature and measuring the impact on the model's accuracy. Features that, when permuted, lead to a significant drop in accuracy are considered important.\n",
    "\n",
    "\n",
    "\n",
    "Key Hyperparameters of a Random Forest and How They Affect the Model\n",
    "1.\tNumber of Trees (n_estimators): The number of trees in the forest. A larger number of trees usually leads to better performance and stability but increases computational cost.\n",
    "o\tEffect: More trees generally improve performance but with diminishing returns beyond a certain point. Too many trees can lead to long training times.\n",
    "2.\tMaximum Depth (max_depth): The maximum depth of each tree. This controls how deep the trees can grow.\n",
    "o\tEffect: Deeper trees can capture more complex patterns but may overfit. Limiting depth can prevent overfitting and reduce training time.\n",
    "3.\tMinimum Samples Split (min_samples_split): The minimum number of samples required to split an internal node.\n",
    "o\tEffect: Higher values prevent the tree from splitting too much, leading to simpler, more general trees.\n",
    "4.\tMinimum Samples Leaf (min_samples_leaf): The minimum number of samples required to be at a leaf node.\n",
    "o\tEffect: Ensures that leaf nodes have enough samples. Higher values can smooth the model and reduce overfitting.\n",
    "5.\tMaximum Features (max_features): The number of features to consider when looking for the best split.\n",
    "o\tEffect: Smaller values reduce the correlation between trees, enhancing diversity and reducing overfitting. Too small values may lead to underfitting.\n",
    "6.\tBootstrap (bootstrap): Whether to use bootstrapping when building trees.\n",
    "o\tEffect: Enabling bootstrapping usually improves the performance and robustness of the model.\n",
    "7.\tCriterion (criterion): The function to measure the quality of a split (e.g., Gini impurity or entropy for classification).\n",
    "o\tEffect: Different criteria can affect the splits and the structure of the trees. Gini is computationally less expensive, while entropy might lead to different splits.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4bb2b28-4afe-4331-85f2-51024f06cca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\409296049.py:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLogistic Regression Model and Its Assumptions\\nLogistic Regression Model\\nLogistic regression is a statistical model used for binary classification tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of a binary outcome (e.g., success/failure, yes/no). The model uses a logistic function to map the output of a linear combination of input features to a value between 0 and 1, representing the probability of the target class.\\nThe logistic regression model can be expressed as:\\nP(y=1∣X)=11+e−(β0+β1x1+β2x2+…+βnxn)P(y=1|X) = \\x0crac{1}{1 + e^{-(\\x08eta_0 + \\x08eta_1 x_1 + \\x08eta_2 x_2 + \\\\ldots + \\x08eta_n x_n)}}P(y=1∣X)=1+e−(β0+β1x1+β2x2+…+βnxn)1\\nwhere:\\n•\\tP(y=1∣X)P(y=1|X)P(y=1∣X) is the probability that the target variable yyy equals 1 given the input features XXX.\\n•\\tβ0\\x08eta_0β0 is the intercept.\\n•\\tβ1,β2,…,βn\\x08eta_1, \\x08eta_2, \\\\ldots, \\x08eta_nβ1,β2,…,βn are the coefficients for the input features x1,x2,…,xnx_1, x_2, \\\\ldots, x_nx1,x2,…,xn.\\nAssumptions of Logistic Regression\\n1.\\tBinary Outcome: The dependent variable is binary.\\n2.\\tIndependence of Observations: The observations are assumed to be independent of each other.\\n3.\\tLinearity of Logits: The logit (log-odds) of the outcome should be a linear combination of the input features.\\n4.\\tNo Multicollinearity: The independent variables should not be highly correlated with each other.\\n5.\\tLarge Sample Size: Logistic regression requires a reasonably large sample size for stable estimates.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Logistic Regression Model and Its Assumptions\n",
    "Logistic Regression Model\n",
    "Logistic regression is a statistical model used for binary classification tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of a binary outcome (e.g., success/failure, yes/no). The model uses a logistic function to map the output of a linear combination of input features to a value between 0 and 1, representing the probability of the target class.\n",
    "The logistic regression model can be expressed as:\n",
    "P(y=1∣X)=11+e−(β0+β1x1+β2x2+…+βnxn)P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)}}P(y=1∣X)=1+e−(β0+β1x1+β2x2+…+βnxn)1\n",
    "where:\n",
    "•\tP(y=1∣X)P(y=1|X)P(y=1∣X) is the probability that the target variable yyy equals 1 given the input features XXX.\n",
    "•\tβ0\\beta_0β0 is the intercept.\n",
    "•\tβ1,β2,…,βn\\beta_1, \\beta_2, \\ldots, \\beta_nβ1,β2,…,βn are the coefficients for the input features x1,x2,…,xnx_1, x_2, \\ldots, x_nx1,x2,…,xn.\n",
    "Assumptions of Logistic Regression\n",
    "1.\tBinary Outcome: The dependent variable is binary.\n",
    "2.\tIndependence of Observations: The observations are assumed to be independent of each other.\n",
    "3.\tLinearity of Logits: The logit (log-odds) of the outcome should be a linear combination of the input features.\n",
    "4.\tNo Multicollinearity: The independent variables should not be highly correlated with each other.\n",
    "5.\tLarge Sample Size: Logistic regression requires a reasonably large sample size for stable estimates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2814c1dd-18a8-40fa-8294-e331b3f986f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow Logistic Regression Handles Binary Classification Problems\\nLogistic regression handles binary classification by modeling the probability of one class (typically labeled as 1) against the other class (typically labeled as 0). It uses the logistic function (also known as the sigmoid function) to transform the linear combination of input features into a probability value between 0 and 1.\\n•\\tDecision Rule: The predicted probability is compared to a threshold (commonly 0.5). If the probability is greater than or equal to the threshold, the model predicts the class as 1; otherwise, it predicts 0.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How Logistic Regression Handles Binary Classification Problems\n",
    "Logistic regression handles binary classification by modeling the probability of one class (typically labeled as 1) against the other class (typically labeled as 0). It uses the logistic function (also known as the sigmoid function) to transform the linear combination of input features into a probability value between 0 and 1.\n",
    "•\tDecision Rule: The predicted probability is compared to a threshold (commonly 0.5). If the probability is greater than or equal to the threshold, the model predicts the class as 1; otherwise, it predicts 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bd4e7df-15ff-4791-b268-8789506b6efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\1147385388.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe Sigmoid Function and Its Use in Logistic Regression\\nSigmoid Function\\nThe sigmoid function is defined as:\\nσ(z)=11+e−z\\\\sigma(z) = \\x0crac{1}{1 + e^{-z}}σ(z)=1+e−z1\\nwhere zzz is the input to the function, which is the linear combination of the input features and their corresponding coefficients.\\nUse in Logistic Regression\\nIn logistic regression, the sigmoid function is applied to the linear combination of input features to map the output to a value between 0 and 1. This value represents the probability of the target class. The sigmoid function ensures that the output can be interpreted as a probability, facilitating binary classification.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Sigmoid Function and Its Use in Logistic Regression\n",
    "Sigmoid Function\n",
    "The sigmoid function is defined as:\n",
    "σ(z)=11+e−z\\sigma(z) = \\frac{1}{1 + e^{-z}}σ(z)=1+e−z1\n",
    "where zzz is the input to the function, which is the linear combination of the input features and their corresponding coefficients.\n",
    "Use in Logistic Regression\n",
    "In logistic regression, the sigmoid function is applied to the linear combination of input features to map the output to a value between 0 and 1. This value represents the probability of the target class. The sigmoid function ensures that the output can be interpreted as a probability, facilitating binary classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e9f2b0f-86dd-47e0-bec4-11c43b7be455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\523565619.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nConcept of the Cost Function in Logistic Regression\\nCost Function\\nThe cost function in logistic regression, also known as the log-loss or binary cross-entropy, measures the difference between the predicted probabilities and the actual class labels. It is defined as:\\nJ(β)=−1m∑i=1m[yilog\\u2061(hβ(xi))+(1−yi)log\\u2061(1−hβ(xi))]J(\\x08eta) = -\\x0crac{1}{m} \\\\sum_{i=1}^{m} \\\\left[ y_i \\\\log(h_\\x08eta(x_i)) + (1 - y_i) \\\\log(1 - h_\\x08eta(x_i)) \\right]J(β)=−m1∑i=1m[yilog(hβ(xi))+(1−yi)log(1−hβ(xi))]\\nwhere:\\n•\\tmmm is the number of observations.\\n•\\tyiy_iyi is the actual class label for the iii-th observation.\\n•\\thβ(xi)h_\\x08eta(x_i)hβ(xi) is the predicted probability for the iii-th observation.\\n•\\tβ\\x08etaβ represents the model parameters.\\nOptimization\\nThe objective is to minimize the cost function to find the best-fitting parameters β\\x08etaβ. This is typically done using optimization algorithms like gradient descent.\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Concept of the Cost Function in Logistic Regression\n",
    "Cost Function\n",
    "The cost function in logistic regression, also known as the log-loss or binary cross-entropy, measures the difference between the predicted probabilities and the actual class labels. It is defined as:\n",
    "J(β)=−1m∑i=1m[yilog⁡(hβ(xi))+(1−yi)log⁡(1−hβ(xi))]J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\beta(x_i)) + (1 - y_i) \\log(1 - h_\\beta(x_i)) \\right]J(β)=−m1∑i=1m[yilog(hβ(xi))+(1−yi)log(1−hβ(xi))]\n",
    "where:\n",
    "•\tmmm is the number of observations.\n",
    "•\tyiy_iyi is the actual class label for the iii-th observation.\n",
    "•\thβ(xi)h_\\beta(x_i)hβ(xi) is the predicted probability for the iii-th observation.\n",
    "•\tβ\\betaβ represents the model parameters.\n",
    "Optimization\n",
    "The objective is to minimize the cost function to find the best-fitting parameters β\\betaβ. This is typically done using optimization algorithms like gradient descent.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14790545-cb08-467a-a1fb-8cb218c5a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\747931135.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExtending Logistic Regression to Handle Multiclass Classification\\nOne-vs-Rest (OvR) or One-vs-All (OvA)\\nOne common approach to extend logistic regression for multiclass classification is the One-vs-Rest method. In this approach:\\n1.\\tBinary Classifiers: Train a separate binary classifier for each class. Each classifier distinguishes between one class and the rest of the classes.\\n2.\\tPrediction: For a new instance, each classifier provides a probability of the instance belonging to its class. The class with the highest probability is chosen as the predicted class.\\nSoftmax Regression (Multinomial Logistic Regression)\\nAnother approach is to use Softmax regression, which generalizes logistic regression to handle multiple classes directly.\\n1.\\tSoftmax Function: The softmax function converts the raw scores (logits) for each class into probabilities:\\nP(y=j∣X)=eβj⋅X∑k=1Keβk⋅XP(y=j|X) = \\x0crac{e^{\\x08eta_j \\\\cdot X}}{\\\\sum_{k=1}^{K} e^{\\x08eta_k \\\\cdot X}}P(y=j∣X)=∑k=1Keβk⋅Xeβj⋅X\\nwhere:\\n•\\tβj\\x08eta_jβj is the coefficient vector for class jjj,\\n•\\tKKK is the total number of classes,\\n•\\tXXX is the input feature vector.\\n2.\\tCross-Entropy Loss: The cost function used is the cross-entropy loss, which generalizes the binary cross-entropy loss for multiple classes.\\nJ(β)=−1m∑i=1m∑j=1Kyijlog\\u2061(P(y=j∣xi))J(\\x08eta) = -\\x0crac{1}{m} \\\\sum_{i=1}^{m} \\\\sum_{j=1}^{K} y_{ij} \\\\log(P(y=j|x_i))J(β)=−m1∑i=1m∑j=1Kyijlog(P(y=j∣xi))\\nwhere yijy_{ij}yij is a binary indicator (0 or 1) if class label jjj is the correct classification for observation iii.\\n3.\\tOptimization: The parameters βj\\x08eta_jβj are optimized using gradient-based methods to minimize the cross-entropy loss.\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extending Logistic Regression to Handle Multiclass Classification\n",
    "One-vs-Rest (OvR) or One-vs-All (OvA)\n",
    "One common approach to extend logistic regression for multiclass classification is the One-vs-Rest method. In this approach:\n",
    "1.\tBinary Classifiers: Train a separate binary classifier for each class. Each classifier distinguishes between one class and the rest of the classes.\n",
    "2.\tPrediction: For a new instance, each classifier provides a probability of the instance belonging to its class. The class with the highest probability is chosen as the predicted class.\n",
    "Softmax Regression (Multinomial Logistic Regression)\n",
    "Another approach is to use Softmax regression, which generalizes logistic regression to handle multiple classes directly.\n",
    "1.\tSoftmax Function: The softmax function converts the raw scores (logits) for each class into probabilities:\n",
    "P(y=j∣X)=eβj⋅X∑k=1Keβk⋅XP(y=j|X) = \\frac{e^{\\beta_j \\cdot X}}{\\sum_{k=1}^{K} e^{\\beta_k \\cdot X}}P(y=j∣X)=∑k=1Keβk⋅Xeβj⋅X\n",
    "where:\n",
    "•\tβj\\beta_jβj is the coefficient vector for class jjj,\n",
    "•\tKKK is the total number of classes,\n",
    "•\tXXX is the input feature vector.\n",
    "2.\tCross-Entropy Loss: The cost function used is the cross-entropy loss, which generalizes the binary cross-entropy loss for multiple classes.\n",
    "J(β)=−1m∑i=1m∑j=1Kyijlog⁡(P(y=j∣xi))J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{K} y_{ij} \\log(P(y=j|x_i))J(β)=−m1∑i=1m∑j=1Kyijlog(P(y=j∣xi))\n",
    "where yijy_{ij}yij is a binary indicator (0 or 1) if class label jjj is the correct classification for observation iii.\n",
    "3.\tOptimization: The parameters βj\\beta_jβj are optimized using gradient-based methods to minimize the cross-entropy loss.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8caece9d-1e54-448a-95a5-0bf531c423ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Afan\\AppData\\Local\\Temp\\ipykernel_2080\\2624845155.py:1: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDifference Between L1 and L2 Regularization in Logistic Regression\\nL1 Regularization (Lasso Regularization)\\nL1 regularization adds the absolute values of the coefficients to the loss function. The regularization term for L1 is:\\nL1 term=λ∑j=1n∣βj∣\\text{L1 term} = \\\\lambda \\\\sum_{j=1}^{n} |\\x08eta_j|L1 term=λ∑j=1n∣βj∣\\nwhere:\\n•\\tλ\\\\lambdaλ is the regularization parameter,\\n•\\tβj\\x08eta_jβj are the model coefficients.\\nCharacteristics:\\n•\\tSparsity: L1 regularization tends to produce sparse models, meaning it drives some coefficients to exactly zero, effectively performing feature selection.\\n•\\tFeature Selection: It is useful when you have a large number of features and want to select a subset of them.\\nL2 Regularization (Ridge Regularization)\\nL2 regularization adds the squared values of the coefficients to the loss function. The regularization term for L2 is:\\nL2 term=λ∑j=1nβj2\\text{L2 term} = \\\\lambda \\\\sum_{j=1}^{n} \\x08eta_j^2L2 term=λ∑j=1nβj2\\nCharacteristics:\\n•\\tNo Sparsity: L2 regularization does not produce sparse models; it tends to shrink coefficients uniformly but not to zero.\\n•\\tHandling Multicollinearity: It is useful for dealing with multicollinearity by distributing the weights among correlated features.\\nComparison:\\n•\\tSparsity: L1 can zero out coefficients, L2 cannot.\\n•\\tFeature Selection: L1 acts as a feature selector, L2 does not.\\n•\\tWeight Shrinkage: L2 tends to distribute weights more evenly among features.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Difference Between L1 and L2 Regularization in Logistic Regression\n",
    "L1 Regularization (Lasso Regularization)\n",
    "L1 regularization adds the absolute values of the coefficients to the loss function. The regularization term for L1 is:\n",
    "L1 term=λ∑j=1n∣βj∣\\text{L1 term} = \\lambda \\sum_{j=1}^{n} |\\beta_j|L1 term=λ∑j=1n∣βj∣\n",
    "where:\n",
    "•\tλ\\lambdaλ is the regularization parameter,\n",
    "•\tβj\\beta_jβj are the model coefficients.\n",
    "Characteristics:\n",
    "•\tSparsity: L1 regularization tends to produce sparse models, meaning it drives some coefficients to exactly zero, effectively performing feature selection.\n",
    "•\tFeature Selection: It is useful when you have a large number of features and want to select a subset of them.\n",
    "L2 Regularization (Ridge Regularization)\n",
    "L2 regularization adds the squared values of the coefficients to the loss function. The regularization term for L2 is:\n",
    "L2 term=λ∑j=1nβj2\\text{L2 term} = \\lambda \\sum_{j=1}^{n} \\beta_j^2L2 term=λ∑j=1nβj2\n",
    "Characteristics:\n",
    "•\tNo Sparsity: L2 regularization does not produce sparse models; it tends to shrink coefficients uniformly but not to zero.\n",
    "•\tHandling Multicollinearity: It is useful for dealing with multicollinearity by distributing the weights among correlated features.\n",
    "Comparison:\n",
    "•\tSparsity: L1 can zero out coefficients, L2 cannot.\n",
    "•\tFeature Selection: L1 acts as a feature selector, L2 does not.\n",
    "•\tWeight Shrinkage: L2 tends to distribute weights more evenly among features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae5bbc3-56d9-42f8-8403-571b335688f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nXGBoost (Extreme Gradient Boosting)\\nXGBoost is an advanced implementation of gradient boosting designed for speed and performance. It is widely used in machine learning competitions and industry applications due to its efficiency and accuracy.\\nKey Features:\\n1.\\tRegularization: XGBoost includes L1 (lasso) and L2 (ridge) regularization to prevent overfitting.\\n2.\\tHandling Missing Values: XGBoost has a built-in mechanism to handle missing data by learning the best imputation strategy.\\n3.\\tParallel Processing: XGBoost can use parallel processing to speed up the training process.\\n4.\\tTree Pruning: XGBoost uses a sophisticated algorithm for tree pruning, which prevents overfitting by reducing the complexity of the model.\\n5.\\tWeighted Quantile Sketch: This allows the algorithm to handle weighted data.\\nDifferences from Other Boosting Algorithms\\n1.\\tEfficiency: XGBoost is optimized for speed and can handle large datasets more efficiently than traditional boosting algorithms.\\n2.\\tRegularization: Traditional gradient boosting does not include regularization terms by default.\\n3.\\tTree Pruning: XGBoost uses a more advanced tree pruning technique compared to standard gradient boosting.\\n4.\\tMissing Values Handling: XGBoost has a unique way of handling missing values during the training process.\\n5.\\tScalability: XGBoost is designed to scale well with large datasets and high-dimensional data.\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "XGBoost (Extreme Gradient Boosting)\n",
    "XGBoost is an advanced implementation of gradient boosting designed for speed and performance. It is widely used in machine learning competitions and industry applications due to its efficiency and accuracy.\n",
    "Key Features:\n",
    "1.\tRegularization: XGBoost includes L1 (lasso) and L2 (ridge) regularization to prevent overfitting.\n",
    "2.\tHandling Missing Values: XGBoost has a built-in mechanism to handle missing data by learning the best imputation strategy.\n",
    "3.\tParallel Processing: XGBoost can use parallel processing to speed up the training process.\n",
    "4.\tTree Pruning: XGBoost uses a sophisticated algorithm for tree pruning, which prevents overfitting by reducing the complexity of the model.\n",
    "5.\tWeighted Quantile Sketch: This allows the algorithm to handle weighted data.\n",
    "Differences from Other Boosting Algorithms\n",
    "1.\tEfficiency: XGBoost is optimized for speed and can handle large datasets more efficiently than traditional boosting algorithms.\n",
    "2.\tRegularization: Traditional gradient boosting does not include regularization terms by default.\n",
    "3.\tTree Pruning: XGBoost uses a more advanced tree pruning technique compared to standard gradient boosting.\n",
    "4.\tMissing Values Handling: XGBoost has a unique way of handling missing values during the training process.\n",
    "5.\tScalability: XGBoost is designed to scale well with large datasets and high-dimensional data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "683952dd-cf09-4035-accb-776040782eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcept of Boosting in the Context of Ensemble Learning\\nBoosting\\nBoosting is an ensemble learning technique that combines the predictions of several base learners (typically weak learners like decision trees) to create a stronger overall model. The main idea is to sequentially train base models, each focusing on correcting the errors of the previous models.\\nProcess:\\n1.\\tInitialization: Start with an initial model, often a simple one.\\n2.\\tSequential Training: Train a sequence of models, where each new model is trained to correct the errors made by the previous models.\\n3.\\tWeighting: Assign higher weights to misclassified instances so that subsequent models focus more on these difficult cases.\\n4.\\tAggregation: Combine the predictions of all models, often using a weighted sum or majority vote, to produce the final prediction.\\nKey Concepts in Boosting\\n1.\\tWeak Learners: Models that perform slightly better than random guessing. Commonly, shallow decision trees are used.\\n2.\\tError Correction: Each subsequent model aims to reduce the errors of the previous models.\\n3.\\tAdaptive Learning: The algorithm adapts by giving more importance to hard-to-predict instances.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Concept of Boosting in the Context of Ensemble Learning\n",
    "Boosting\n",
    "Boosting is an ensemble learning technique that combines the predictions of several base learners (typically weak learners like decision trees) to create a stronger overall model. The main idea is to sequentially train base models, each focusing on correcting the errors of the previous models.\n",
    "Process:\n",
    "1.\tInitialization: Start with an initial model, often a simple one.\n",
    "2.\tSequential Training: Train a sequence of models, where each new model is trained to correct the errors made by the previous models.\n",
    "3.\tWeighting: Assign higher weights to misclassified instances so that subsequent models focus more on these difficult cases.\n",
    "4.\tAggregation: Combine the predictions of all models, often using a weighted sum or majority vote, to produce the final prediction.\n",
    "Key Concepts in Boosting\n",
    "1.\tWeak Learners: Models that perform slightly better than random guessing. Commonly, shallow decision trees are used.\n",
    "2.\tError Correction: Each subsequent model aims to reduce the errors of the previous models.\n",
    "3.\tAdaptive Learning: The algorithm adapts by giving more importance to hard-to-predict instances.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aaad75eb-2efc-4b2b-9f2a-9b33038c35c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XGBoost can handle missing values automatically during training by learning the best direction to go when a missing value is encountered. This is achieved through a process called \"sparsity-aware split finding.\"\\nProcess:\\n1.\\tDefault Direction: For each split, XGBoost determines a default direction (left or right) to send missing values based on which direction results in a lower loss.\\n2.\\tLearning Imputation: During training, the algorithm learns which direction to take for missing values to minimize the loss function.\\n3.\\tHandling During Prediction: When a missing value is encountered during prediction, XGBoost follows the learned default direction for that feature.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"XGBoost can handle missing values automatically during training by learning the best direction to go when a missing value is encountered. This is achieved through a process called \"sparsity-aware split finding.\"\n",
    "Process:\n",
    "1.\tDefault Direction: For each split, XGBoost determines a default direction (left or right) to send missing values based on which direction results in a lower loss.\n",
    "2.\tLearning Imputation: During training, the algorithm learns which direction to take for missing values to minimize the loss function.\n",
    "3.\tHandling During Prediction: When a missing value is encountered during prediction, XGBoost follows the learned default direction for that feature.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75334eb5-e786-4623-b195-461374827fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nKey Hyperparameters in XGBoost and How They Affect Model Performance\\n1.\\tLearning Rate (eta)\\no\\tDescription: Shrinks the contribution of each tree. Lower values make the model more robust to overfitting but require more trees.\\no\\tEffect: Smaller values lead to slower learning but better generalization. Higher values may cause overfitting.\\n2.\\tNumber of Trees (n_estimators)\\no\\tDescription: Number of boosting rounds or trees.\\no\\tEffect: More trees can improve performance but increase computation time and risk of overfitting.\\n3.\\tMaximum Depth (max_depth)\\no\\tDescription: Maximum depth of a tree.\\no\\tEffect: Deeper trees can capture more complex patterns but are more likely to overfit.\\n4.\\tMinimum Child Weight (min_child_weight)\\no\\tDescription: Minimum sum of instance weight needed in a child.\\no\\tEffect: Higher values prevent the model from learning overly specific patterns and reduce overfitting.\\n5.\\tSubsample\\no\\tDescription: Fraction of training data used to grow each tree.\\no\\tEffect: Reducing this value can prevent overfitting but might increase bias.\\n6.\\tColumn Subsample (colsample_bytree, colsample_bylevel, colsample_bynode)\\no\\tDescription: Fraction of features used to train each tree.\\no\\tEffect: Lower values can reduce overfitting but might miss important features.\\n7.\\tRegularization Parameters (alpha and lambda)\\no\\tDescription: L1 and L2 regularization terms, respectively.\\no\\tEffect: Regularization helps prevent overfitting by adding penalties for large coefficients.\\n8.\\tGamma (gamma)\\no\\tDescription: Minimum loss reduction required to make a split.\\no\\tEffect: Higher values make the algorithm more conservative, reducing the likelihood of overfitting.\\n9.\\tScale Pos Weight (scale_pos_weight)\\no\\tDescription: Balances the positive and negative weights in the dataset.\\no\\tEffect: Useful for dealing with class imbalance.\\n10.\\tMax Delta Step (max_delta_step)\\no\\tDescription: Maximum step size for each tree's weight estimation.\\no\\tEffect: Helps stabilize the model and is useful for logistic regression when classes are extremely imbalanced.\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Key Hyperparameters in XGBoost and How They Affect Model Performance\n",
    "1.\tLearning Rate (eta)\n",
    "o\tDescription: Shrinks the contribution of each tree. Lower values make the model more robust to overfitting but require more trees.\n",
    "o\tEffect: Smaller values lead to slower learning but better generalization. Higher values may cause overfitting.\n",
    "2.\tNumber of Trees (n_estimators)\n",
    "o\tDescription: Number of boosting rounds or trees.\n",
    "o\tEffect: More trees can improve performance but increase computation time and risk of overfitting.\n",
    "3.\tMaximum Depth (max_depth)\n",
    "o\tDescription: Maximum depth of a tree.\n",
    "o\tEffect: Deeper trees can capture more complex patterns but are more likely to overfit.\n",
    "4.\tMinimum Child Weight (min_child_weight)\n",
    "o\tDescription: Minimum sum of instance weight needed in a child.\n",
    "o\tEffect: Higher values prevent the model from learning overly specific patterns and reduce overfitting.\n",
    "5.\tSubsample\n",
    "o\tDescription: Fraction of training data used to grow each tree.\n",
    "o\tEffect: Reducing this value can prevent overfitting but might increase bias.\n",
    "6.\tColumn Subsample (colsample_bytree, colsample_bylevel, colsample_bynode)\n",
    "o\tDescription: Fraction of features used to train each tree.\n",
    "o\tEffect: Lower values can reduce overfitting but might miss important features.\n",
    "7.\tRegularization Parameters (alpha and lambda)\n",
    "o\tDescription: L1 and L2 regularization terms, respectively.\n",
    "o\tEffect: Regularization helps prevent overfitting by adding penalties for large coefficients.\n",
    "8.\tGamma (gamma)\n",
    "o\tDescription: Minimum loss reduction required to make a split.\n",
    "o\tEffect: Higher values make the algorithm more conservative, reducing the likelihood of overfitting.\n",
    "9.\tScale Pos Weight (scale_pos_weight)\n",
    "o\tDescription: Balances the positive and negative weights in the dataset.\n",
    "o\tEffect: Useful for dealing with class imbalance.\n",
    "10.\tMax Delta Step (max_delta_step)\n",
    "o\tDescription: Maximum step size for each tree's weight estimation.\n",
    "o\tEffect: Helps stabilize the model and is useful for logistic regression when classes are extremely imbalanced.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3188575b-746c-48f3-a22c-0d2fb8a35ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nProcess of Gradient Boosting in XGBoost\\n1.\\tInitialize the Model: Start with an initial prediction, often the mean of the target variable.\\n2.\\tCompute Residuals: Calculate the difference between the actual target values and the current predictions. These residuals represent the errors the model needs to correct.\\n3.\\tFit a Tree to Residuals: Train a decision tree on the residuals to learn how to correct the errors. The tree is fitted using the negative gradient of the loss function with respect to the model's predictions.\\n4.\\tUpdate Predictions: Add the predictions of the new tree to the existing model's predictions, weighted by the learning rate.\\n5.\\tRepeat: Iteratively add trees, each trained to correct the residuals of the combined model from the previous iteration, until the maximum number of trees is reached or the model converges.\\n6.\\tRegularization and Pruning: Apply regularization techniques to prevent overfitting and prune the trees to remove unnecessary complexity.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process of Gradient Boosting in XGBoost\n",
    "1.\tInitialize the Model: Start with an initial prediction, often the mean of the target variable.\n",
    "2.\tCompute Residuals: Calculate the difference between the actual target values and the current predictions. These residuals represent the errors the model needs to correct.\n",
    "3.\tFit a Tree to Residuals: Train a decision tree on the residuals to learn how to correct the errors. The tree is fitted using the negative gradient of the loss function with respect to the model's predictions.\n",
    "4.\tUpdate Predictions: Add the predictions of the new tree to the existing model's predictions, weighted by the learning rate.\n",
    "5.\tRepeat: Iteratively add trees, each trained to correct the residuals of the combined model from the previous iteration, until the maximum number of trees is reached or the model converges.\n",
    "6.\tRegularization and Pruning: Apply regularization techniques to prevent overfitting and prune the trees to remove unnecessary complexity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35f9d08b-898f-405a-b9bb-2a4c5a512db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdvantages and Disadvantages of Using XGBoost\\nAdvantages\\n1.\\tHigh Performance: XGBoost is known for its high predictive accuracy and speed.\\n2.\\tRegularization: Built-in L1 and L2 regularization helps prevent overfitting.\\n3.\\tHandling Missing Values: Efficient handling of missing values during training.\\n4.\\tParallel Processing: Supports parallel processing, making it faster than many other implementations.\\n5.\\tFeature Importance: Provides insights into feature importance, helping in model interpretation.\\n6.\\tScalability: Can handle large datasets and high-dimensional data efficiently.\\n7.\\tCustom Objectives: Allows custom loss functions and evaluation metrics.\\nDisadvantages\\n1.\\tComplexity: XGBoost can be complex to configure and tune, requiring careful parameter tuning.\\n2.\\tResource Intensive: Can be resource-intensive in terms of memory and computation, especially for very large datasets.\\n3.\\tOverfitting: Despite regularization, XGBoost can still overfit if not properly tuned, especially with a high number of trees.\\n4.\\tInterpretability: While better than some other ensemble methods, the interpretability of the final model can still be challenging compared to simpler models.\\n\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advantages and Disadvantages of Using XGBoost\n",
    "Advantages\n",
    "1.\tHigh Performance: XGBoost is known for its high predictive accuracy and speed.\n",
    "2.\tRegularization: Built-in L1 and L2 regularization helps prevent overfitting.\n",
    "3.\tHandling Missing Values: Efficient handling of missing values during training.\n",
    "4.\tParallel Processing: Supports parallel processing, making it faster than many other implementations.\n",
    "5.\tFeature Importance: Provides insights into feature importance, helping in model interpretation.\n",
    "6.\tScalability: Can handle large datasets and high-dimensional data efficiently.\n",
    "7.\tCustom Objectives: Allows custom loss functions and evaluation metrics.\n",
    "Disadvantages\n",
    "1.\tComplexity: XGBoost can be complex to configure and tune, requiring careful parameter tuning.\n",
    "2.\tResource Intensive: Can be resource-intensive in terms of memory and computation, especially for very large datasets.\n",
    "3.\tOverfitting: Despite regularization, XGBoost can still overfit if not properly tuned, especially with a high number of trees.\n",
    "4.\tInterpretability: While better than some other ensemble methods, the interpretability of the final model can still be challenging compared to simpler models.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74effdcb-2e92-4533-acc8-3d9c1abbd0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
